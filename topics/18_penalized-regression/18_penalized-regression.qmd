---
title: "Penalized Regression"
subtitle: "DATA 5600 Introduction to Regression and Machine Learning for Analytics"
author: Marc Dotson
title-slide-attributes:
  data-background-color: "#0F2439"
format: 
  revealjs:
    theme: ../slides.scss  # Modified slides theme
    code-copy: true        # Show code blocks copy button
    slide-number: c/t      # Numbered slides current/total
    embed-resources: true  # Render to a single HTML file
execute:
  eval: false
  echo: true
jupyter: python3
---

## {background-color="#288DC2"}

### Get Started {.permanent-marker-font}

#### Last Time

- Discussed additional measures of overall classification model fit
- Introduced cross-validation for model selection and hyperparameter tuning

#### Preview

- Introduce penalized regression and shrinkage estimates
- Discuss cross-validation for tuning regularization hyperparameters
- Walk through variable selection techniques as an alternative

## {background-color="#D1CBBD"}

::: {.columns .v-center}
![](../../figures/workflow_reconcile-fit-evaluate.png){fig-align="center"}
:::

# Bias-Variance Tradeoff

## Posterior distributions vs. sampling distributions

Where does the **variation** come from in our parameter estimates?

:::: {.columns}

::: {.fragment .column width="50%"}
**Bayesian**

::: {.fragment}
$$
X = \text{data we observe} \\
\theta \sim \text{a probability distribution}
$$
:::

::: {.incremental}
- Start with **prior distributions** for the parameters
- Use the **likelihood** and the data we observe to update our priors into a **posterior distribution** for the parameters
:::
:::

::: {.fragment .column width="50%"}
**Frequentist**

::: {.fragment}
$$
X \sim \text{a probability distribution} \\
\theta = \text{a value we don't observe}
$$
:::

::: {.incremental}
- Use the **likelihood** and the data we observe to get **point estimates** for the parameters
- Use the theoretical **sampling distribution** of the estimator from many hypothetical datasets to construct **interval estimates** for the parameters
:::
:::

::::

## Posterior distributions vs. sampling distributions {visibility="uncounted"}

Where does the **variation** come from in our parameter estimates?

:::: {.columns}

::: {.column width="50%"}
**Bayesian**

$$
X = \text{data we observe} \\
\theta \sim \color{red}{\text{a posterior distribution}}
$$

- Start with **prior distributions** for the parameters
- Use the **likelihood** and the data we observe to update our priors into a **posterior distribution** for the parameters
:::

::: {.column width="50%"}
**Frequentist**

$$
X \sim \color{red}{\text{a sampling distribution}} \\
\theta = \text{a value we don't observe}
$$

- Use the **likelihood** and the data we observe to get **point estimates** for the parameters
- Use the theoretical **sampling distribution** of the estimator from many hypothetical datasets to construct **interval estimates** for the parameters
:::

::::

## Adding bias to reduce variance

If our point estimate $\hat{\theta} = \theta$ then it is **unbiased** [and we naturally want to have the **lowest variance** in our interval estimates as possible]{.fragment}

::: {.fragment}
$$
\large{\ell(\hat{y}, y) = \sum_{i=1}^n (\hat{y}_i - y_i)^2 = \text{MSE} = \text{Variance} + \text{Bias}^2}
$$
:::

::: {.fragment}
While MLE (including OLS) are unbiased, low variance estimators **when the model assumptions are met**, it is often useful to use a biased estimator if it reduces our variance
:::

## Model complexity and total error

This central machine learning idea is called the **bias-variance tradeoff**

![](../../figures/bias-variance-01.png){fig-align="center"}

## Model complexity and total error {visibility="uncounted"}

This central machine learning idea is called the **bias-variance tradeoff**

![](../../figures/bias-variance-02.png){fig-align="center"}

# When would we want to add bias to reduce variance in our parameter estimates? {background-color="#006242"}

# Regularization

## Overfitting and shrinkage

Point estimates are chosen by **minimizing the loss** on the training data, but that's no guarantee it will have low loss on future data (i.e., **generalize**) thus overfitting remains a problem

::: {.fragment}
$$
\large{\ell(\hat{\theta}, \theta) = \underset{\theta}{\text{argmin}} - \sum_{i=1}^n \log \mathcal{L}(\theta | X_i, Y_i)}
$$
:::

## Overfitting and shrinkage {visibility="uncounted"}

Point estimates are chosen by **minimizing the loss** on the training data, but that's no guarantee it will have low loss on future data (i.e., **generalize**) thus overfitting remains a problem

$$
\large{\ell(\hat{\theta}, \theta) = \underset{\theta}{\text{argmin}} - \sum_{i=1}^n \log \mathcal{L}(\theta | X_i, Y_i) + \lambda C(\theta)}
$$

::: {.fragment}
$\lambda$ is the **regularization** hyperparameter and $C(\theta)$ is the **complexity penalty**
:::

::: {.incremental}
- Adding a complexity penalty **adds bias to reduce variance**
- The complexity penalty **shrinks** parameter estimates toward zero
- These **shrinkage estimates** are less extreme and more **regular**
:::

## Shrinkage methods

**Penalized regression** (a.k.a., **regularization** or **shrinkage methods**) are frequently used to reduce overfitting and improve generalization, especially since model assumptions (e.g., multicollinearity) are not always satisfied

::: {.fragment}
Where have we seen shrinkage before?
:::

::: {.fragment}
![](../../figures/plot_bayesian-updating.png){fig-align="center"}
:::

## 

::: {.v-center}
![](../../figures/meme_regularizing-priors.png){fig-align="center"}
:::

## Choice of prior/complexity penalty

There is a **one-to-one correspondence** between complexity penalties and prior distributions

![](../../figures/regularization_priors.png){fig-align="center"}

## Choice of prior/complexity penalty {visibility="uncounted"}

There is a **one-to-one correspondence** between complexity penalties and prior distributions

$$
\large{\ell(\hat{\theta}, \theta) = \underset{\theta}{\text{argmin}} - \sum_{i=1}^n \log \mathcal{L}(\theta | X_i, Y_i) + \lambda \sum_{k=1}^{p-1} \theta_k^2}
$$

::: {.fragment}
**Ridge regression** uses an $\ell_2$ (i.e., quadratic) complexity penalty which corresponds to a **normal prior**
:::

## Choice of prior/complexity penalty {visibility="uncounted"}

There is a **one-to-one correspondence** between complexity penalties and prior distributions

$$
\large{\ell(\hat{\theta}, \theta) = \underset{\theta}{\text{argmin}} - \sum_{i=1}^n \log \mathcal{L}(\theta | X_i, Y_i) + \lambda \sum_{k=1}^{p-1} | \theta_k |}
$$

::: {.fragment}
**LASSO** uses an $\ell_1$ complexity penalty which corresponds to a **Laplace prior**
:::

## Choice of prior/complexity penalty {visibility="uncounted"}

There is a **one-to-one correspondence** between complexity penalties and prior distributions

$$
\ell(\hat{\theta}, \theta) = \underset{\theta}{\text{argmin}} - \sum_{i=1}^n \log \mathcal{L}(\theta | X_i, Y_i) + \lambda_1 \sum_{k=1}^{p-1} \theta_k^2 + \lambda_2 \sum_{k=1}^{p-1} | \theta_k |
$$

::: {.fragment}
**Elastic net** uses a combined $\ell_1$ and $\ell_2$ complexity penalty which corresponds to a **normal/Laplace mixture prior**
:::

## Engineering, tuning, and bootstrapping

Penalized regression is powerful as it helps us **navigate the bias-variance tradeoff** using **shrinkage** so that we can **deal with overfitting**, improve **generalization**, and **learn regular features**, but complexity comes with a cost

::: {.incremental}
- Predictors must be **standardized** for fitting penalized regression models
- The **regularization hyperparameter(s)** must be tuned via cross-validation
- Confidence intervals for parameter estimates must be obtained via **bootstrapping**
:::

::: {.fragment}
Bayesian models include **shrinkage by default** with all the same benefits and none of these additional costs, though the same computational costs for fitting a Bayesian model apply
:::

# Holding everything else constant, why do we prefer model *parsimony*? {background-color="#006242"}

# Variable Selection

## Dealing with multicollinearity

There are a number of ways to **deal with multicollinearity** and **improve model parsimony**

::: {.incremental}
1. Manually drop variables from the model
2. Use penalized regression (which also deals with overfitting)
3. Apply a **variable selection method**
4. Combine correlated variables via dimensionality reduction
:::

::: {.fragment}
Depending on the prior/complexity penalty, certain penalized regression methods also perform variable selection
:::

## Removing variables from the model

Given three predictors, how do we choose which model is best?

:::: {.columns}

::: {.column width="50%"}
![](../../figures/variable-selection.png){fig-align="center"}
:::

::: {.column width="50%"}
::: {.incremental}
1. Compare **all possible models**
2. Instead of comparing all possible models, **incrementally build a good model**
:::

::: {.fragment}
Variable selection methods are a **simple, algorithmic** way to arrive at model parsimony (i.e., parameters are set to 0) compared with penalized regression (i.e., parameters are shrunk toward 0)
:::
:::

::::

## Best subset method

Comparing **all possible subsets** of predictors (i.e., each subset is its own model) with $p - 1$ predictors means we have $2^{p-1}$ models to compare

::: {.incremental}
- The sample size $n$ must be sufficiently large relative to $p$
- A good rule of thumb is $n$ should be 6-10 times larger than $p$
- Computationally expensive when $p$ is large
- Only feasible if $p$ is relatively small
:::

## Stepwise methods

To avoid comparing all possible subsets of predictors, we can **incrementally build a good model** using stepwise methods

::: {.incremental}
- There is no guarantee the "right" model will be chosen
- These methods are best used as a heuristic to confirm model choices
- Methods include **forward**, **backward**, and **stepwise/sequential replacement**
- Different methods and overall fit metrics will yield different results
- Once you have a final model, you'll need to re-run the model assumption diagnostics
:::

## Stepwise methods: Forward selection

::: {.incremental}
1. Find the predictor that is most correlated with the response
    a. Fit a regression model with that one predictor
    b. Leave the predictor in the model if overall model fit improves
2. Given the previously entered predictor, find the predictor with the highest partial correlation with the response
    a. Add this predictor to the model created in Step 1
    b. Leave the predictor in the model if overall model fit improves
3. Continue this process of adding predictors one at a time until adding predictors no longer improves overall model fit
:::

::: {.fragment}
What problems might this method have? [**Don't use it**]{.fragment}
:::

## Stepwise methods: Backward selection

::: {.incremental}
1. Fit a model with all $p-1$ predictors
2. Fit a second model with one predictor from the model
3. Keep the second model if it improves overall model fit
4. Repeat with the remaining $p-2$ predictors until removing predictors no longer improves overall model fit
:::

::: {.fragment}
Why is backward selection a better option than forward selection?
:::

## Stepwise methods: Sequential replacement

::: {.incremental}
1. Start with an intercept-only model
2. Take a forward step: Add a predictor to the model if the predictor improves overall model fit
3. Take a backward step: Drop a predictor from the model if dropping the preditor improves overall model fit
4. Iterate the forward and backward steps until the model stays the same
:::

::: {.fragment}
How does this *approximate* the best subset method?
:::

## {background-color="#006242"}

### Exercise 18 {.lato-font}

1. Return to your data from the previous exercise (or start with the exercise solution)
2. Reuse the 5-fold cross-validation code to tune the regularization hyperparameter for the `LogisticRegression()` model from `scikit-learn` using the default `l2` complexity penalty (i.e., ridge regression) to maximize overall accuracy
3. To calculate overall accuracy, use the tuned cutoff value from the previous exercise and, again, when splitting the data into training, validation, and testing data, remember to use the `stratify` parameter
4. Using MLE is sufficient, though you are welcome to compare overall accuracy with a Bayesian logistic regression with regularizing standard normal priors
5. Submit your code, output, and explanation as a single PDF on Canvas

## {background-color="#288DC2"}

### Wrap Up {.permanent-marker-font}

#### Summary

- Introduced penalized regression and shrinkage estimates
- Discussed cross-validation for tuning regularization hyperparameters
- Walked through variable selection techniques as an alternative

#### Next Time

- Apply and compare specific regularization techniques

