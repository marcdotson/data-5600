---
title: "Linear Models"
subtitle: "DATA 5600 Introduction to Regression and Machine Learning for Analytics"
author: Marc Dotson
title-slide-attributes:
  data-background-color: "#0F2439"
format: 
  revealjs:
    theme: ../slides.scss  # Modified slides theme
    code-copy: true        # Show code blocks copy button
    slide-number: c/t      # Numbered slides current/total
    embed-resources: true  # Render to a single HTML file
execute:
  eval: false
  echo: true
jupyter: python3
---

## {background-color="#288DC2"}

### Get Started {.permanent-marker-font}

TODO: Add Bernie holding predictors constant meme, change additivity/linearity to linearity throughout

#### Last Time

- Finished building models using probability
- Demonstrated fitting models to recover parameters

#### Preview

- Formally introduce linear regression

## {background-color="#D1CBBD"}

::: {.columns .v-center}
![](../../figures/workflow_build.png){fig-align="center"}
:::

## Simulate data

If we don't have real data or want to test our code, we can simulate data

```{python}
#| eval: true

import numpy as np
import polars as pl
import seaborn as sns
import seaborn.objects as so

# Set randomization seed
rng = np.random.default_rng(42)

# Specify a function to simulate data
def sim_data(n, beta_0, beta_price, beta_discount, beta_online, beta_promo, sigma):
    price = rng.normal(4, 2, size=n)
    discount = rng.normal(2, 1, size=n)
    online = rng.binomial(1, 0.7, size=n)
    promo = rng.binomial(1, 0.2, size=n)
    error = rng.normal(0, sigma, size=n)
    sales = beta_0 + beta_price * price + beta_discount * discount + beta_online * online + beta_promo * promo + error

    # Return the output
    return sales, price, discount, online, promo

# Call the function and convert to a dataframe
data_arr = sim_data(n = 100, beta_0 = 10, beta_price = -2, beta_discount = 3, beta_online = 5, beta_promo = 3, sigma = 5)
data_df = pl.DataFrame(data_arr, schema = ['sales', 'price', 'discount', 'online', 'promo'])
```

## Observations and variables {.scrollable}

```{python}
#| eval: true

data_df
```

# Lines and Means

## 

::: {.v-center}
$$
\Huge{y \sim \text{Normal}(\mu, \sigma^2)}
$$
:::

## 

:::: {.columns .v-center}

::: {.column width="50%"}
$$
\large{y \sim \text{Normal}(\mu, \sigma^2)}
$$
:::

::: {.column width="50%"}
::: {.incremental}
- What is the support?
- What are the parameters?
- What range of values can the parameters be?
- How could this be a useful model?
:::
:::

::::

## Galton Board

![](../../figures/history_galton-board.png){fig-align="center"}

## 

::: {.v-center}
$$
\Huge{y \sim \text{Normal}(\mu, \sigma^2)} \\
\Huge{\mu = \beta_0 + \beta_1 x}
$$
:::

## 

:::: {.columns .v-center}

::: {.column width="50%"}
$$
\large{y \sim \text{Normal}(\mu, \sigma^2)} \\
\large{\mu = \beta_0 + \beta_1 x}
$$
:::

::: {.column width="50%"}
::: {.incremental}
- What are the parameters?
- Why a line?
- Why model $\mu$?
- How could this be a useful model?
:::
:::

::::

## Scatterplot of price and sales {auto-animate=true}

Does it look like a line might be a good approximation of this relationship?

```{python}
#| eval: true
#| output-location: column

(so.Plot(data_df, x = 'price', y = 'sales')
    .add(so.Dot())
)
```

## Scatterplot of price and sales {auto-animate=true}

Is this line a good approximation of the relationship? What is this line?

```{python}
#| eval: true
#| output-location: column

(so.Plot(data_df, x = 'price', y = 'sales')
    .add(so.Dot())
    .add(so.Line(), so.PolyFit(order=0))
)
```

## Scatterplot of price and sales {auto-animate=true}

Is this line a good approximation of the relationship? What is this line?

```{python}
#| eval: true
#| output-location: column

(so.Plot(data_df, x = 'price', y = 'sales')
    .add(so.Dot())
    .add(so.Line(), so.PolyFit(order=1))
)
```

# If we use a line to model the mean of the data generating process, what is a "good" line? {background-color="#006242"}

# Linear Regression

## Keeping it as simple as possible

Modeling the mean $\mu$ of a normal distribution using a linear function of predictors $X$ and other parameters $\beta$ is called **linear regression** or, more generally, a **linear model**

::: {.incremental}
- A normal distribution is the **simplest** distribution (i.e., the **maximum entropy** distribution) to use to model a continuous $y$
- A linear function is the **simplest** way to map predictors $X$ to a continuous $y$
:::

::: {.fragment}
It is always a good idea to **start simple** and add complexity as you need it
:::

## Simple linear regression

A linear model with a *single* predictor is called **simple linear regression**

$$
\large{y_i = \beta_0 + \beta_1 x_i + \epsilon_i} \\
\large{\text{where } \epsilon_i \sim \text{Normal}(0, \sigma^2)}
$$

::: {.incremental}
- $\beta_0$ is the **intercept** or **bias** parameter
- $\beta_1$ is the **slope** parameter
- the **error term** (not a parameter)
- $\sigma^2$ is the **variance** parameter
- $i$ indexes the observations
:::

::: {.fragment}
The linear model is the **deterministic component** and the normal error is the **probabilistic component**
:::

## Simple linear regression {visibility="uncounted"}

A linear model with a *single* predictor is called **simple linear regression**

$$
\large{y_i \sim \text{Normal}(\mu_i, \sigma^2)} \\
\large{\mu_i = \beta_0 + \beta_1 x_i}
$$

- $\beta_0$ is the **intercept** or **bias** parameter
- $\beta_1$ is the **slope** parameter
- ~~the **error term** (not a parameter)~~
- $\sigma^2$ is the **variance** parameter
- $i$ indexes the observations

The linear model is the **deterministic component** and the normal error is the **probabilistic component**

## Simple linear regression parameters

$$
\large{y_i = \beta_0 + \beta_1 x_i + \epsilon_i} \\
\large{\text{where } \epsilon_i \sim \text{Normal}(0, \sigma^2)}
$$

::: {.incremental}
- $\beta_0$ is the average of $y$ when $x = 0$
- $\beta_1$ is the average change in $y$ for every one unit increase in $x$
- the **residuals** are the difference between the observed $y$ and the *average* or *fitted* value of $y$ based on the linear model
:::

## Simple linear regression parameters {visibility="uncounted"}

$$
\large{y_i \sim \text{Normal}(\mu_i, \sigma^2)} \\
\large{\mu_i = \beta_0 + \beta_1 x_i}
$$

- $\beta_0$ is the average of $y$ when $x = 0$
- $\beta_1$ is the average change in $y$ for every one unit increase in $x$
- the **residuals** are the difference between the observed $y$ and the *average* or *fitted* value of $y$ based on the linear model

## Simple linear regression parameters {visibility="uncounted"}

$$
\large{\text{sales}_i = 19.39 - 1.83 \times \text{price}_i + \epsilon_i} \\
\large{\text{where } \epsilon_i \sim \text{Normal}(0, \sigma^2)}
$$

- $\beta_0$ is the average of $y$ when $x = 0$
- $\beta_1$ is the average change in $y$ for every one unit increase in $x$
- the **residuals** are the difference between the observed $y$ and the *average* or *fitted* value of $y$ based on the linear model

::: {.fragment}
What about $\sigma^2$?
:::

## Simple linear regression and correlation

For simple linear regression, the slope $\beta_1$ is related to the correlation (a.k.a., the correlation coefficient) $r$ between $x$ and $y$

```{python}
#| eval: true
#| output-location: column

(so.Plot(data_df, x = 'price', y = 'sales')
    .add(so.Dot())
    .add(so.Line(), so.PolyFit(order=1))
)
```

## Simple linear regression and correlation {visibility="uncounted"}

For simple linear regression, the slope $\beta_1$ is related to the correlation (a.k.a., the correlation coefficient) $r$ between $x$ and $y$

```{python}
#| eval: true
#| output-location: column

(so.Plot(data_df, x = 'discount', y = 'sales')
    .add(so.Dot())
    .add(so.Line(), so.PolyFit(order=1))
)
```

## Multiple linear regression

A linear model with *multiple* predictors is called **multiple linear regression**

$$
\large{y_i = \beta_0 + \beta_1 x_{1,i} + \cdots + \beta_p x_{p,i} + \epsilon_i} \\
\large{\text{where } \epsilon_i \sim \text{Normal}(0, \sigma^2)}
$$

::: {.incremental}
- $\beta_0$ is the **intercept** or **bias** parameter
- $\beta_1$ through $\beta_p$ are the **slope** parameters
- the **error term** (not a parameter)
- $\sigma^2$ is the **variance** parameter
- $i$ indexes the observations
:::

::: {.fragment}
The linear model is the **deterministic component** and the normal error is the **probabilistic component**
:::

## Multiple linear regression {visibility="uncounted"}

A linear model with *multiple* predictors is called **multiple linear regression**

$$
\large{y_i \sim \text{Normal}(\mu_i, \sigma^2)} \\
\large{\mu_i = \beta_0 + \beta_1 x_{1,i} + \cdots + \beta_p x_{p,i}}
$$

- $\beta_0$ is the **intercept** or **bias** parameter
- $\beta_1$ through $\beta_p$ are the **slope** parameters
- ~~the **error term** (not a parameter)~~
- $\sigma^2$ is the **variance** parameter
- $i$ indexes the observations

The linear model is the **deterministic component** and the normal error is the **probabilistic component**

## Multiple linear regression parameters

$$
\large{y_i = \beta_0 + \beta_1 x_{1,i} + \cdots + \beta_p x_{p,i} + \epsilon_i} \\
\large{\text{where } \epsilon_i \sim \text{Normal}(0, \sigma^2)}
$$

::: {.incremental}
- $\beta_0$ is the average of $y$ when $x_1 \cdots x_p = 0$
- $\beta_1$ is the average change in $y$ for every one unit increase in $x_1$, **holding all other predictors constant**
- the **residuals** are the difference between the observed $y$ and the *average* or *fitted* value of $y$ based on the linear model
:::

## Multiple linear regression parameters {visibility="uncounted"}

$$
\large{y_i \sim \text{Normal}(\mu_i, \sigma^2)} \\
\large{\mu_i = \beta_0 + \beta_1 x_{1,i} + \cdots + \beta_p x_{p,i}}
$$

- $\beta_0$ is the average of $y$ when $x_1 \cdots x_p = 0$
- $\beta_1$ is the average change in $y$ for every one unit increase in $x_1$, **holding all other predictors constant**
- the **residuals** are the difference between the observed $y$ and the *average* or *fitted* value of $y$ based on the linear model

## Multiple linear regression parameters {visibility="uncounted"}

$$
\large{\text{sales}_i = 5.39 - 2.65 \times \text{price}_i + \cdots - 0.22 \times \text{promo}_i + \epsilon_i} \\
\large{\text{where } \epsilon_i \sim \text{Normal}(0, \sigma^2)}
$$

- $\beta_0$ is the average of $y$ when $x_1 \cdots x_p = 0$
- $\beta_1$ is the average change in $y$ for every one unit increase in $x_1$, **holding all other predictors constant**
- the **residuals** are the difference between the observed $y$ and the *average* or *fitted* value of $y$ based on the linear model

::: {.fragment}
What about $\sigma^2$?
:::

## Multiple linear regression and correlation

For multiple linear regression, each slope $\beta_1, \cdots, \beta_p$ is no longer connected directly to the correlation between the given $x$ and $y$

```{python}
#| eval: true
#| output-location: column

cont_df = (data_df
    .select(['sales', 'price', 'discount'])
    .to_pandas()
)

sns.pairplot(cont_df)
```

## 

::: {.v-center}
![](../../figures/meme_hypothesis-tests-regression.png){fig-align="center"}
:::

# We want to keep our model as simple as possible, so why would we use multiple linear regression? {background-color="#006242"}

# Assumptions

## Ensuring valid inference

Before interpreting results from linear regression, we need to confirm the model assumptions are met

::: {.incremental}
1. **Validity**: Data is relevant to the objective and no predictors are missing
2. **Representativeness**: Data is representative of the data generating process or population
3. **Additivity and Linearity**: The mapping function from the predictors to the outcome is additive and a linear function of the parameters
4. **Independence**: Observations are independent of each other
5. **Constant Variance**: Homoscedasticity or constant variance of errors
6. **Normality**: Variation or error is normally distributed
7. **Identifiability**: Data allows for parameters to be estimated, including no strong multicollinearity
:::

## 

:::: {.columns .v-center}

::: {.column width="60%"}
$$
y_i = \beta_0 + \beta_1 x_{1,i} + \cdots + \beta_p x_{p,i} + \epsilon_i \\
\text{where } \epsilon_i \sim \text{Normal}(0, \sigma^2)
$$

$$
y_i \sim \text{Normal}(\mu_i, \sigma^2) \\
\mu_i = \beta_0 + \beta_1 x_{1,i} + \cdots + \beta_p x_{p,i}
$$
:::

::: {.column width="40%"}
1. Validity
2. Representativeness
3. Additivity and Linearity
4. Independence
5. Constant Variance
6. Normality
7. Identifiability
:::

::::

## {background-color="#006242"}

### Exercise 05 {.lato-font}

1. Review the materials from the course thus far
2. Identify lingering questions you have about the concepts covered
3. Use an AI tool to ask your questions and reflect on the responses you receive
4. Identify at least two questions you feel have been answered and share your prompts, the responses you received, and your reflections on the responses
5. Submit your prompts, responses, and reflections as a single PDF on Canvas

## {background-color="#288DC2"}

### Wrap Up {.permanent-marker-font}

#### Summary

- Formally introduced linear regression

#### Next Time

- Start working with real data
- Conduct exploratory data analysis
- Begin reconciling the data and the model

