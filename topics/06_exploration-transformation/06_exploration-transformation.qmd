---
title: "Exploration and Transformation"
subtitle: "DATA 5600 Introduction to Regression and Machine Learning for Analytics"
author: Marc Dotson
title-slide-attributes:
  data-background-color: "#0F2439"
format: 
  revealjs:
    theme: ../slides.scss  # Modified slides theme
    code-copy: true        # Show code blocks copy button
    slide-number: c/t      # Numbered slides current/total
    embed-resources: true  # Render to a single HTML file
execute:
  eval: false
  echo: true
jupyter: python3
---

## {background-color="#288DC2"}

### Get Started {.permanent-marker-font}

#### Last Time

- Formally introduced linear regression

#### Preview

- Start working with real data
- Conduct exploratory data analysis
- Begin reconciling the data and the model

## {background-color="#D1CBBD"}

::: {.columns .v-center}
![](../../figures/workflow_explore-reconcile.png){fig-align="center"}
:::

## Assumptions all the way down

We need to use **diagnostics** to confirm the model assumptions are met, and **remedy** any issues, so that our interpretation (i.e., statistical inference) is valid

:::: {.columns}

::: {.column width="50%"}
::: {.fragment}
![](../../figures/models_all-02.png){fig-align="center"}
:::
:::

::: {.column width="50%"}
::: {.incremental}
- Flexible, black-box models aren't interpretable
- Inferences from interpretable models require assumptions
- Causal effects require **additional assumptions** to be met
:::
:::

::::

## Linear regression assumptions

1. **Validity**: Data is relevant to the objective and no predictors are missing
2. **Representativeness**: Data is representative of the data generating process or population
3. **Additivity and Linearity**: The mapping function from the predictors to the outcome is additive and a linear function of the parameters
4. **Independence**: Observations are independent of each other
5. **Constant Variance**: Homoscedasticity or constant variance of errors
6. **Normality**: Variation or error is normally distributed
7. **Identifiability**: Data allows for parameters to be estimated, including no strong multicollinearity

# Exploratory Data Analysis

## Understand the data and its limitations

Remember that it is your job as a data analyst to understand the data so you can communicate about the data and address its limitations

::: {.incremental}
- Visualize data and relationships
- Compute numeric summaries
- Check for missing data, outliers, and errors
- Consider proxy variables
- Ask questions
:::

::: {.fragment}
Conducting EDA **with the model assumptions in mind** will help you identify potential issues and needed remedies early
:::

## Parquet, *pour quoi*?

Apache Parquet is an open source, **column-oriented data file format** designed for efficient data storage and retrieval

```{python}
#| eval: true
#| echo: false

import os
import polars as pl
import seaborn as sns
import seaborn.objects as so

# Import data
# sl_data = pl.read_parquet(os.path.join('..', '..', 'data', 'soft_launch.parquet'))
sl_data = (pl.read_parquet(os.path.join('..', '..', 'data', 'original_df.parquet'))
  .select('customer_id', 'units', 'sales', 'brand', 'promo', 'loyal', 'texture', 'size', 'price')
  .cast({'size': pl.String})
)
```

```{python}
import os
import polars as pl
import seaborn as sns
import seaborn.objects as so

# Import data
# sl_data = pl.read_parquet(os.path.join('data', 'soft_launch.parquet'))
sl_data = (pl.read_parquet(os.path.join('data', 'original_df.parquet'))
  .select('customer_id', 'units', 'sales', 'brand', 'promo', 'loyal', 'texture', 'size', 'price')
  .cast({'size': pl.String})
)
```

## {.scrollable}

Let's look at some already cleaned data to review the **data dictionary** and use Positron's **data explorer**

```{python}
#| eval: true

sl_data
```

## Variable types and relationships

:::: {.columns}

::: {.fragment .column width="33%"}
**Discrete**

"Individually separate and distinct" (i.e., **categorical** or qualitative)

::: {.incremental}
- Counts
- Column/bar plots
- Facets
:::
:::

::: {.fragment .column width="33%"}
**Continuous**

"Forming an unbroken whole; without interruption" (i.e., **numeric** or quantitative)

::: {.incremental}
- Means
- Histograms
- Correlations
- Scatterplots
:::
:::

::: {.fragment .column width="33%"}
**Discrete and Continuous**

Comparing continuous values by discrete levels

::: {.incremental}
- Grouped summaries
- Boxplots
- Density plots
- Facets
:::
:::

::::

## Counts and column plots

I recommend visualization libraries that follow the [grammar of graphics](https://link.springer.com/book/10.1007/0-387-28695-0) where each plot is composed of **data**, a **mapping** from data to visual elements, the **specific graphic** (e.g., Marks and Stats)

```{python}
#| eval: true
#| output-location: column

pb_data = sl_data.filter(pl.col('units') > 0)

texture_count = (pb_data
  .group_by(pl.col('texture'))
  .agg(n = pl.len())
)

(so.Plot(texture_count, x = 'texture', y = 'n')
  .add(so.Bar())
)
```

## Counts and column plots

```{python}
#| eval: true
#| output-location: column

(so.Plot(pb_data, x = 'texture')
  .add(so.Bar(), so.Hist())
)
```

## Counts and column plots

```{python}
#| eval: true
#| output-location: column

texture_count = (pb_data
  .group_by(pl.col(['texture', 'size']))
  .agg(n = pl.len())
)

(so.Plot(texture_count, x = 'texture', y = 'n', color = 'size')
  .add(so.Bar(), so.Dodge())
)
```

## Counts, column plots, and facets

```{python}
#| eval: true
#| output-location: column

texture_count = (pb_data
  .group_by(pl.col(['texture', 'size', 'brand']))
  .agg(n = pl.len())
  .with_columns(
    (pl.col('n') / pl.col('n').sum().over(['texture', 'size'])).alias('prop')
  ).sort('brand')
)

(so.Plot(texture_count, x = 'texture', y = 'prop', color = 'brand')
  .facet('size')
  .add(so.Bar(), so.Stack())
)
```

## Means and histograms

```{python}
#| eval: true
#| output-location: column

pb_data['units'].drop_nans().mean()
```

```{python}
#| eval: true
#| output-location: column

(so.Plot(pb_data, x = 'units')
  .add(so.Bars(), so.Hist(bins = 10))
)
```

## Correlations

```{python}
#| eval: true
#| output-location: column

(pb_data
  .select(['price', 'units'])
  .drop_nans()
  .corr()
)
```

## Scatterplots

```{python}
#| eval: true
#| output-location: column

(so.Plot(pb_data, x = 'price', y = 'units')
    .add(
      so.Dot(alpha = 0.25), 
      so.Jitter(x = 0.25, y = 0.25)
    )
    .add(so.Line(), so.PolyFit(order = 1))
)
```

## Correlation matrices

```{python}
#| eval: true
#| output-location: column

(pb_data
  .select(['price', 'units', 'sales'])
  .drop_nans()
  .corr()
)
```

## Scatterplots matrices

```{python}
#| eval: true
#| output-location: column

sns.pairplot(
  pb_data.select(['price', 'units', 'sales'])
  .to_pandas()
)
```

## Grouped summaries

```{python}
#| eval: true
#| output-location: column

(pb_data
  .group_by(pl.col(['texture', 'size']))
  .agg(
    n = pl.len(), 
    avg_units = pl.col('units').drop_nans().mean(), 
    avg_sales = pl.col('sales').drop_nans().mean()
  )
  .sort(pl.col('avg_units'), descending=True)
)
```

## Boxplots

```{python}
#| eval: true
#| output-location: column

sns.catplot(
  pb_data, x = 'units', y = 'brand', kind = 'box'
)
```

## Density plots

```{python}
#| eval: true
#| output-location: column

(so.Plot(pb_data, x = 'units', color = 'brand')
  .add(so.Area(), so.Hist(bins=10))
)
```

## Density plots and facets

```{python}
#| eval: true
#| output-location: column

(so.Plot(pb_data, x = 'units', color = 'brand')
  .facet(col = 'size', row = 'texture')
  .add(so.Area(), so.Hist(bins=10))
)
```

# Copy some visualization code and apply it to a different variable or set of variables. What do you discover? {background-color="#006242"}

# Assumptions

```{python}
#| eval: true
#| echo: false

import os
import polars as pl
import seaborn.objects as so
import statsmodels.api as sm
import statsmodels.formula.api as smf
from statsmodels.stats.outliers_influence import variance_inflation_factor as vif

# Import data
# sl_data = pl.read_parquet(os.path.join('..', '..', 'data', 'soft_launch.parquet'))
sl_data = pl.read_parquet(os.path.join('..', '..', 'data', 'original_df.parquet'))
```

## Validity

Data is relevant to the objective and no predictors are missing

:::: {.columns}

::: {.fragment .column width="50%"}
**Diagnostics**

::: {.incremental}
- Long-run frequency
- Describes a repeatable event under identical conditions
- Exists in the world
:::
:::

::: {.fragment .column width="50%"}
**Fixes**

::: {.incremental}
- Information
- Describes uncertainty when working with incomplete knowledge
- Exists in the mind
:::
:::

::::

::: {.fragment}
The rules of probability operate the same regardless of the interpretation
:::

## Representativeness

Data is representative of the data generating process or population

## Additivity and Linearity

:::: {.columns .v-center}

::: {.column width="60%"}
$$
y_i = \beta_0 + \beta_1 x_{1,i} + \cdots + \beta_p x_{p,i} + \epsilon_i \\
\text{where } \epsilon_i \sim \text{Normal}(0, \sigma^2)
$$

$$
\mu_i = \beta_0 + \beta_1 x_{1,i} + \cdots + \beta_p x_{p,i} \\
y_i \sim \text{Normal}(\mu_i, \sigma^2)
$$
:::

::: {.column width="40%"}
The mapping function from the predictors to the outcome is additive and a linear function of the parameters
:::

::::

## Independence

:::: {.columns .v-center}

::: {.column width="60%"}
$$
y_i = \beta_0 + \beta_1 x_{1,i} + \cdots + \beta_p x_{p,i} + \epsilon_i \\
\text{where } \epsilon_i \sim \text{Normal}(0, \sigma^2)
$$

$$
\mu_i = \beta_0 + \beta_1 x_{1,i} + \cdots + \beta_p x_{p,i} \\
y_i \sim \text{Normal}(\mu_i, \sigma^2)
$$
:::

::: {.column width="40%"}
Observations are independent of each other
:::

::::

## Constant Variance

:::: {.columns .v-center}

::: {.column width="60%"}
$$
y_i = \beta_0 + \beta_1 x_{1,i} + \cdots + \beta_p x_{p,i} + \epsilon_i \\
\text{where } \epsilon_i \sim \text{Normal}(0, \sigma^2)
$$

$$
\mu_i = \beta_0 + \beta_1 x_{1,i} + \cdots + \beta_p x_{p,i} \\
y_i \sim \text{Normal}(\mu_i, \sigma^2)
$$
:::

::: {.column width="40%"}
Homoscedasticity or constant variance of errors
:::

::::

## Normality

:::: {.columns .v-center}

::: {.column width="60%"}
$$
y_i = \beta_0 + \beta_1 x_{1,i} + \cdots + \beta_p x_{p,i} + \epsilon_i \\
\text{where } \epsilon_i \sim \text{Normal}(0, \sigma^2)
$$

$$
\mu_i = \beta_0 + \beta_1 x_{1,i} + \cdots + \beta_p x_{p,i} \\
y_i \sim \text{Normal}(\mu_i, \sigma^2)
$$
:::

::: {.column width="40%"}
Variation or error is normally distributed
:::

::::

## Identifiability

:::: {.columns .v-center}

::: {.column width="60%"}
$$
y_i = \beta_0 + \beta_1 x_{1,i} + \cdots + \beta_p x_{p,i} + \epsilon_i \\
\text{where } \epsilon_i \sim \text{Normal}(0, \sigma^2)
$$

$$
\mu_i = \beta_0 + \beta_1 x_{1,i} + \cdots + \beta_p x_{p,i} \\
y_i \sim \text{Normal}(\mu_i, \sigma^2)
$$
:::

::: {.column width="40%"}
Data allows for parameters to be estimated, including no strong multicollinearity
:::

::::







## Reconcile the model and the data

:::: {.columns}

::: {.column width="50%"}
With a model prepared and data explored, its time to resolve the differences between the model and the data

- Comparing simulated and real data
- Evaluating model assumptions
- Revising the model
- Feature engineering

But be careful, we are trying to extract information about the data generating process and not **overfit** to the data
:::

::: {.column .gray-box width="42%"}
In the case, we discover that to satisfy a linear regression's assumption of **linearity in the parameters**, we need to transform the `units` outcome and `price` predictor variables

Also, acknowledging that there are a number of **omitted predictors**, we find that including `brand` is critical and needs to be dummy coded to be included in the model
:::

::::








## 

:::: {.columns}

::: {.column .gray-box width="92%"}
Feature engineering (a.k.a., preprocessing data) is data cleaning or wrangling for the benefit of the model, for example:

- Log transformations need an offset
- Dummy coding (a.k.a., indicator coding or one hot encoding) turns a discrete predictor into multiple binary predictors

```{python}
#| code-line-numbers: "|2-4|5-8|9"

# Preprocess data
lm_data = pb_data.to_dummies(
    # Dummy code brand
    columns = ['brand'], drop_first = True
).with_columns(
    # Log units and price
    (pl.col('units') + 1).log().alias('log_units'),
    (pl.col('price') + 1).log().alias('log_price')
).to_pandas()
```
:::

::::














# Linear Regression



# We want to keep our model as simple as possible, so why would we use multiple linear regression? {background-color="#006242"}





## {background-color="#006242"}

### Exercise 06 {.lato-font}

Clean up the soft launch data
Create two interesting visualizations that help you understand the data and its limitations

1. Review the materials from the course thus far
2. Identify lingering questions you have about the concepts covered
3. Use an AI tool to ask your questions and reflect on the responses you receive
4. Identify at least two questions you feel have been answered and share your prompts, the responses you received, and your reflections on the responses
5. Submit your prompts, responses, and reflections as a single PDF on Canvas

## {background-color="#288DC2"}

### Wrap Up {.permanent-marker-font}

#### Summary

- Started working with real data
- Conducted exploratory data analysis
- Began reconciling the data and the model

#### Next Time

- 

