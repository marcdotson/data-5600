---
title: "Exploration and Transformation"
subtitle: "DATA 5600 Introduction to Regression and Machine Learning for Analytics"
author: Marc Dotson
title-slide-attributes:
  data-background-color: "#0F2439"
format: 
  revealjs:
    theme: ../slides.scss  # Modified slides theme
    code-copy: true        # Show code blocks copy button
    slide-number: c/t      # Numbered slides current/total
    embed-resources: true  # Render to a single HTML file
execute:
  eval: false
  echo: true
jupyter: python3
---

## {background-color="#288DC2"}

### Get Started {.permanent-marker-font}

#### Last Time

- Formally introduced linear regression

#### Preview

- Start working with real data
- Conduct exploratory data analysis
- Begin reconciling the data and the model

## {background-color="#D1CBBD"}

::: {.columns .v-center}
![](../../figures/workflow_explore-reconcile.png){fig-align="center"}
:::

## Linear regression assumptions

Before interpreting results from a linear regression, we need to confirm the model assumptions are met

::: {.incremental}
1. **Validity**: Data is relevant to the objective and no predictors are missing
2. **Representativeness**: Data is representative of the data generating process or population
3. **Additivity and Linearity**: The mapping function from the predictors to the outcome is additive and a linear function of the parameters
4. **Independence**: Observations are independent of each other
5. **Constant Variance**: Homoscedasticity or constant variance of errors
6. **Normality**: Variation or error is normally distributed
7. **Identifiability**: Data allows for parameters to be estimated, including no strong multicollinearity
:::

## 

:::: {.columns .v-center}

::: {.column width="60%"}
$$
y_i = \beta_0 + \beta_1 x_{1,i} + \cdots + \beta_p x_{p,i} + \epsilon_i \\
\text{where } \epsilon_i \sim \text{Normal}(0, \sigma^2)
$$

$$
\mu_i = \beta_0 + \beta_1 x_{1,i} + \cdots + \beta_p x_{p,i} \\
y_i \sim \text{Normal}(\mu_i, \sigma^2)
$$
:::

::: {.column width="40%"}
1. Validity
2. Representativeness
3. Additivity and Linearity
4. Independence
5. Constant Variance
6. Normality
7. Identifiability
:::

::::

# Exploratory Data Analysis

## Understand the data and its limitations

Remember that it is **your job** to understand the data so you can communicate and address its limitations

::: {.incremental}
- Visualize data and relationships
- Compute numeric summaries
- Check for missing data
- Consider **proxy** variables
- Ask questions
:::

## Parquet, *pour quoi*?

Apache Parquet is an open source, column-oriented data file format designed for efficient data storage and retrieval

```{python}
#| eval: true
#| echo: false

import os
import polars as pl
import seaborn.objects as so
import statsmodels.formula.api as smf

# Import data
sl_data = pl.read_parquet(os.path.join('..', '..', 'data', 'soft_launch.parquet'))
```

```{python}
import os
import polars as pl
import seaborn.objects as so
import statsmodels.formula.api as smf

# Import data
sl_data = pl.read_parquet(os.path.join('data', 'soft_launch.parquet'))
```

## {.scrollable}

Let's walk through the **data dictionary** and use Positron's **data explorer**

```{python}
#| eval: true

sl_data
```

## Variable types and relationships

:::: {.columns}

::: {.fragment .column width="33%"}
**Discrete**

"Individually separate and distinct" (i.e., categorical)

::: {.incremental}
- Counts
- Column/bar plots
- Facets
:::
:::

::: {.fragment .column width="33%"}
**Continuous**

"Forming an unbroken whole; without interruption" (i.e., numeric)

::: {.incremental}
- Means
- Histograms
- Scatterplots, jitter
:::
:::

::: {.fragment .column width="33%"}
**Discrete and Continuous**

Comparing continuous values by discrete levels

::: {.incremental}
- Grouped summaries
- Boxplots
- Density plots
- Facets
:::
:::

::::

## Counts and column plots

I recommend visualization libraries that follow the [grammar of graphics](https://link.springer.com/book/10.1007/0-387-28695-0) where each plot is composed of **data**, a **mapping** from data to visual elements, the **specific graphic** (e.g., Marks and Stats)

```{python}
#| eval: true
#| output-location: column

pb_data = sl_data.filter(pl.col('units') > 0)

texture_count = (pb_data
  .group_by(pl.col('texture'))
  .agg(n = pl.len())
)

(so.Plot(texture_count, x = 'texture', y = 'n')
  .add(so.Bar())
)
```

## Counts and column plots

```{python}
#| eval: true
#| output-location: column

(so.Plot(pb_data, x = 'texture')
  .add(so.Bar(), so.Hist())
)
```

## Counts and column plots

```{python}
#| eval: true
#| output-location: column

texture_count = (pb_data
  .group_by(pl.col(['texture', 'size']))
  .agg(n = pl.len())
)

(so.Plot(texture_count, x = 'texture', y = 'n', color = 'size')
  .add(so.Bar(), so.Agg(), so.Dodge())
)
```

## Facets

```{python}
texture_count = (sl_data
  .group_by(pl.col(['texture', 'size', 'brand']))
  .agg(n = pl.len())
  .with_columns(
    (pl.col('n') / pl.col('n').sum().over()).alias('prop')
)

(so.Plot(texture_count, x = 'texture', y = 'prop', color = 'brand')
  .facet('size')
  .add(so.Bar(), so.Agg(), so.Dodge())
)
```

## Means and histograms

```{python}
(so.Plot(customer_data, x = 'income', color = 'region')
  .add(so.Bars(), so.Hist())
)
```

## Scatterplots

```{python}
(so.Plot(customer_data, x = 'income', y = 'credit')
  .add(so.Dot())
)

(so.Plot(customer_data, x = 'star_rating', y = 'income')
  .add(so.Dot(pointsize = 10, alpha = 0.5), so.Jitter(0.75))
)
```

# If we use a line to model the mean of the data generating process, what is a "good" line? {background-color="#006242"}

# Assumptions

## Reconcile the model and the data

:::: {.columns}

::: {.column width="50%"}
With a model prepared and data explored, its time to resolve the differences between the model and the data

- Comparing simulated and real data
- Evaluating model assumptions
- Revising the model
- Feature engineering

But be careful, we are trying to extract information about the data generating process and not **overfit** to the data
:::

::: {.column .gray-box width="42%"}
In the case, we discover that to satisfy a linear regression's assumption of **linearity in the parameters**, we need to transform the `units` outcome and `price` predictor variables

Also, acknowledging that there are a number of **omitted predictors**, we find that including `brand` is critical and needs to be dummy coded to be included in the model
:::

::::

## All the way down

Use the predictive/inferential/causal model as a reference to nested assumptions
Assumptions all the way down?

## Ensuring valid inference

Before interpreting results from linear regression, we need to confirm the model assumptions are met

::: {.incremental}
1. **Validity**: Data is relevant to the objective and no predictors are missing
2. **Representativeness**: Data is representative of the data generating process or population
3. **Additivity and Linearity**: The mapping function from the predictors to the outcome is additive and a linear function of the parameters
4. **Independence**: Observations are independent of each other
5. **Constant Variance**: Homoscedasticity or constant variance of errors
6. **Normality**: Variation or error is normally distributed
7. **Identifiability**: Data allows for parameters to be estimated, including no strong multicollinearity
:::






## 

:::: {.columns}

::: {.column .gray-box width="92%"}
Feature engineering (a.k.a., preprocessing data) is data cleaning or wrangling for the benefit of the model, for example:

- Log transformations need an offset
- Dummy coding (a.k.a., indicator coding or one hot encoding) turns a discrete predictor into multiple binary predictors

```{python}
#| code-line-numbers: "|2-4|5-8|9"

# Preprocess data
lm_data = pb_data.to_dummies(
    # Dummy code brand
    columns = ['brand'], drop_first = True
).with_columns(
    # Log units and price
    (pl.col('units') + 1).log().alias('log_units'),
    (pl.col('price') + 1).log().alias('log_price')
).to_pandas()
```
:::

::::














# Linear Regression



# We want to keep our model as simple as possible, so why would we use multiple linear regression? {background-color="#006242"}





## {background-color="#006242"}

### Exercise 06 {.lato-font}

1. Review the materials from the course thus far
2. Identify lingering questions you have about the concepts covered
3. Use an AI tool to ask your questions and reflect on the responses you receive
4. Identify at least two questions you feel have been answered and share your prompts, the responses you received, and your reflections on the responses
5. Submit your prompts, responses, and reflections as a single PDF on Canvas

## {background-color="#288DC2"}

### Wrap Up {.permanent-marker-font}

#### Summary

- Started working with real data
- Conducted exploratory data analysis
- Began reconciling the data and the model

#### Next Time

- 

