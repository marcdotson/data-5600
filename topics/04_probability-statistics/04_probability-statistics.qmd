---
title: "Probability and Statistics"
subtitle: "DATA 5600 Introduction to Regression and Machine Learning for Analytics"
author: Marc Dotson
title-slide-attributes:
  data-background-color: "#0F2439"
format: 
  revealjs:
    theme: ../slides.scss  # Modified slides theme
    code-copy: true        # Show code blocks copy button
    slide-number: c/t      # Numbered slides current/total
    embed-resources: true  # Render to a single HTML file
execute:
  eval: false
  echo: true
jupyter: python3
---

## {background-color="#288DC2"}

### Get Started {.permanent-marker-font}

#### Last Time

- Discussed decision-making under uncertainty
- Started modeling data generating processes

#### Preview

- Finish building models using probability
- Demonstrate fitting models to recover parameters

## {background-color="#D1CBBD"}

::: {.columns .v-center}
![](../../figures/workflow_build.png){fig-align="center"}
:::

# Probability

## What is probability?

Probability is the mathematical language of uncertainty and a unifying framework for machine learning

$$
\LARGE{p(s)}
$$

::: {.fragment}
Consider an **event** $s$ that's part of a **sample space** $S$ of all possible events
:::

::: {.incremental}
1. The probability of an event is $0 \le p(s) \le 1$
2. If $S$ is the set of all possible events then $p(S) = 1$
3. If $s_1$ and $s_2$ are mutually exclusive events then the probability that at least one of them will occur is $p(s_1) + p(s_2)$
:::

## Three types of probability

:::: {.columns}

::: {.fragment .column width="33%"}
**Marginal Probability**

$\Large{p(a)}$

The probability of an event occurring
:::

::: {.fragment .column width="33%"}
**Joint Probability**

$\Large{p(a, b)}$

The probability of two or more events occurring together 
:::

::: {.fragment .column width="33%"}
**Conditional Probability**

$\Large{p(a | b)}$

The probability of an event occurring given that another event has occurred
:::

::::

## Two interpretations of probability

What is the probability of getting heads when flipping a coin?

:::: {.columns}

::: {.fragment .column width="50%"}
**Frequentist**

::: {.incremental}
- Long-run frequency
- Describes a repeatable event under identical conditions
- Exists in the world
:::
:::

::: {.fragment .column width="50%"}
**Bayesian**

::: {.incremental}
- Information
- Describes uncertainty when working with incomplete knowledge
- Exists in the mind
:::
:::

::::

::: {.fragment}
The rules of probability operate the same regardless of the interpretation
:::

## Probability distributions

Probability distributions are functions that produce the probabilities of all possible events in a sample space for a given **random variable**

::: {.incremental}
- The **support** is the sample space, the events or values that have non-zero probability
- The **probability** of some subset of the support is the associated area
- A distribution's **parameters** are the values that define the specific distribution
:::

::: {.fragment}
Probability distributions can be discrete (**probability mass functions** or **PMFs**) or continuous (**probability density functions** or **PDFs**)
:::

::: {.fragment}
We can compute a probability distribution **analytically** (i.e., using the actual formula) or using a **Monte Carlo simulation** (i.e., drawing random samples from the distribution)
:::

## Discrete uniform distribution {auto-animate=true}

![](../../figures/plot_discrete-uniform.png){fig-align="center"}

## Discrete uniform distribution {auto-animate=true visibility="uncounted"}

:::: {.columns}

::: {.column width="40%"}
![](../../figures/plot_discrete-uniform.png){fig-align="center"}
:::

::: {.column width="60%"}
```{python}
#| code-line-numbers: "|1-3|5-7|9-11|13-18|20-23"

import numpy as np
import polars as pl
import seaborn.objects as so

# Set randomization seed and n
rng = np.random.default_rng(42)
n = 10_000_000

# Discrete uniform distribution
disc_unif = np.random.randint(1, 7, size=n)
draws = pl.DataFrame(disc_unif, schema=['disc_unif'])

disc_unif_p = (draws
    .group_by(pl.col('disc_unif'))
    .agg(n = pl.len())
    .with_columns(p = pl.col('n') / pl.col('n').sum())
    .sort('disc_unif')
)

(so.Plot(disc_unif_p, x = 'disc_unif', y = 'p')
    .add(so.Bar())
    .label(x='support', y='p')
)
```
:::

::::

## Continuous uniform distribution {auto-animate=true}

![](../../figures/plot_continuous-uniform.png){fig-align="center"}

## Continuous uniform distribution {auto-animate=true visibility="uncounted"}

:::: {.columns}

::: {.column width="40%"}
![](../../figures/plot_continuous-uniform.png){fig-align="center"}
:::

::: {.column width="60%"}
```{python}
#| code-line-numbers: "|1-6|8-11"

# Continuous uniform distribution
draws = (draws
    .with_columns(
        pl.Series('cont_unif', rng.uniform(1, 6, size=n))
    )
)

(so.Plot(draws, x='cont_unif')
    .add(so.Area(alpha=0.5), so.Hist(stat='density'))
    .label(x='support', y='density')
)
```
:::

::::

## Normal distribution {auto-animate=true}

![](../../figures/plot_normal.png){fig-align="center"}

## Normal distribution {auto-animate=true visibility="uncounted"}

:::: {.columns}

::: {.column width="40%"}
![](../../figures/plot_normal.png){fig-align="center"}
:::

::: {.column width="60%"}
```{python}
#| code-line-numbers: "|1-6|8-11"

# Normal distribution
draws = (draws
    .with_columns(
        pl.Series('norm', rng.normal(10, 5, size=n))
    )
)

(so.Plot(draws, x='norm')
    .add(so.Area(alpha=0.5), so.Hist(stat='density'))
    .label(x='support', y='density')
)
```
:::

::::

## Binomial distribution {auto-animate=true}

![](../../figures/plot_binomial.png){fig-align="center"}

## Binomial distribution {auto-animate=true visibility="uncounted"}

:::: {.columns}

::: {.column width="40%"}
![](../../figures/plot_binomial.png){fig-align="center"}
:::

::: {.column width="60%"}
```{python}
#| code-line-numbers: "|1-6|8-14|16-19"

# Binomial distribution
draws = (draws
    .with_columns(
        pl.Series('binom', rng.binomial(1, 0.20, size=n))
    )
)

binom_p = (draws
    .group_by(pl.col('binom'))
    .agg(n = pl.len())
    .with_columns(p = pl.col('n') / pl.col('n').sum())
    .with_columns(pl.col('binom').cast(pl.Utf8))
    .sort('binom')
)

(so.Plot(binom_p, x = 'binom', y = 'p')
    .add(so.Bar())
    .label(x='support', y='p')
)
```
:::

::::

# Copy the Monte Carlo code for a distribution and try different parameter values. How does the distribution change? {background-color="#006242"}

# Statistics

## Three branches of statistics

:::: {.columns}

::: {.fragment .column width="33%"}
**Descriptive Statistics**

::: {.incremental}
- Visualizing data and relationships
- Computing numeric summaries (i.e., statistics)
- Exploratory data analysis (EDA)
:::
:::

::: {.fragment .column width="33%"}
**Inferential Statistics**

::: {.incremental}
- Synonymous with interpretable supervised learning (i.e., learning a mapping function $f: X \rightarrow Y$)
- Mapping functions (i.e., models) often use known probability distributions
- Estimate parameters describing a data generating process or population
:::
:::

::: {.fragment .column width="33%"}
**Decision Theory**

::: {.incremental}
- Using data to choose from a set of possible actions
:::
:::

::::

## Using probability distributions to model data

If our mapping function uses known probability distributions to model the data generating process or population, its called a **parametric model**

::: {.fragment}
$$
\large{y = \beta_0 + \beta_1 x_1 + \cdots + \beta_p x_p + \epsilon} \\
\large{\text{where } \epsilon \sim \text{Normal}(0, \sigma^2)}
$$
:::

::: {.fragment}
and
:::

::: {.fragment}
$$
\large{y \sim \text{Normal}(\mu, \sigma^2)} \\
\large{\mu = \beta_0 + \beta_1 x_1 + \cdots + \beta_p x_p}
$$
:::

::: {.fragment}
are the same **linear regression** model -- how? What does this model mean?
:::

## Using statistics to estimate parameters

Our parametric model of the data $f(X, Y | \theta)$ is used to determine *likely* values of $\theta$ and is also called a **likelihood function** $\mathcal{L}(\theta | X, Y)$

$$
\LARGE{f(X, Y | \theta) = \mathcal{L}(\theta | X, Y)}
$$

::: {.incremental}
- If you give a likelihood function parameters $\theta$ and $X$, it can produce plausible observations $Y$
- If you give a likelihood function observations $Y$ and $X$, it can determine plausible parameters $\theta$
:::

::: {.fragment}
How we use the likelihood function to estimate parameters depends on what we assume is a random variable (i.e., how we interpret probability)
:::

## 

::: {.v-center}
$$
\Huge{X, Y; \theta}
$$
:::

## 

:::: {.columns .v-center}

::: {.column width="50%"}
::: {.fragment .h-center}
### Frequentist
:::

$$
X, Y \sim \text{some distribution} \\
\theta = \text{some unknown value}
$$
:::

::: {.column width="50%"}
::: {.fragment .h-center}
### Bayesian
:::

$$
X, Y = \text{some known value} \\
\theta \sim \text{some distribution}
$$
:::

::::

## Using the likelihood to estimate parameters {auto-animate=true}

TODO: Demonstrate likelihood being used for inference as the inverse of it being used for simulation

![](../../figures/plot_likelihood.png){fig-align="center"}

## Using the likelihood to estimate parameters {auto-animate=true visibility="uncounted"}

:::: {.columns}

::: {.column width="40%"}
![](../../figures/plot_likelihood.png){fig-align="center"}
:::

::: {.column width="60%"}
::: {.fragment}
**Frequentist**
:::

::: {.incremental}
- Find the single value of $\theta$ that maximizes the likelihood function
- This **maximum likelihood estimate** is a point estimate of the fixed but unknown value $\theta$
:::

::: {.fragment}
**Bayesian**
:::

::: {.incremental}
- Use Bayes' theorem to combine the likelihood function with a **prior distribution** $p(\theta)$
- This produces a **posterior distribution** $p(\theta | X)$ of the random variable $\theta$
- Instead of computing this distribution analytically, we typically draw samples from it using **Markov chain Monte Carlo** (MCMC)
:::
:::

::::

## 

::: {.v-center}
$$
\LARGE{\mathcal{L}(\theta | X, Y)}
$$
:::

## {visibility="uncounted"}

::: {.v-center}
$$
\LARGE{p(X, Y | \theta)}
$$
:::

## {visibility="uncounted"}

::: {.v-center}
$$
\LARGE{p(\theta | X, Y) \propto p(X, Y | \theta) \ p(\theta)}
$$
:::

## 

:::: {.columns .v-center}

::: {.column width="100%"}
![](../../figures/frequentist-panel.png){fig-align="center"}
:::

::::

## 

:::: {.columns .v-center}

::: {.column width="100%"}
![](../../figures/bayesian-triptych.png){fig-align="center"}
:::

::::

## 

::: {.v-center}
![](../../figures/meme_me-and-bayes.png){fig-align="center"}
:::

# Both frequentist and Bayesian statistics use probability to quantify uncertainty, so how are they different? {background-color="#006242"}

# Recovering Parameters

## Simulate data

When we simulate data, we know we're using the correct model (i.e., no model uncertainty) so we can make sure our code is working

```{python}
#| code-line-numbers: "|1-5|7-8|10-16|18-19|21-22"
#| eval: true

import numpy as np
import polars as pl
import statsmodels.formula.api as smf
import bambi as bmb
import arviz as az

# Set randomization seed
rng = np.random.default_rng(42)

# Specify a function to simulate data
def sim_data(n, beta_0, beta_x1, beta_x2, sigma):
    x1 = rng.normal(10, 3, size=n)
    x2 = rng.binomial(1, 0.5, size=n)
    error = rng.normal(0, sigma, size=n)
    y = beta_0 + beta_x1 * x1 + beta_x2 * x2 + error
    return y, x1, x2

# Call the function and save as an array
data_arr = sim_data(n = 100, beta_0 = 2, beta_x1 = -0.3, beta_x2 = 4, sigma = 5)

# Convert to a dataframe
data_df = pl.DataFrame(data_arr, schema = ['y', 'x1', 'x2']).to_pandas()
```

## Fit frequentist and Bayesian models

```{python}
#| code-line-numbers: "|1-2|4-5"
#| eval: true

# Fit a frequentist linear regression
fr_fit = smf.ols('y ~ x1 + x2', data = data_df).fit()

# Fit a Bayesian linear regression
ba_fit = bmb.Model('y ~ x1 + x2', data = data_df).fit() # Use progressbar=False
```

## Did we recover the parameters?

I set `beta_0 = 2`, `beta_x1 = -0.3`, `beta_x2 = 4`, and `sigma = 5`

```{python}
#| eval: true

fr_fit.conf_int()  # Interval estimates
```

```{python}
#| eval: true

az.summary(ba_fit) # Posterior estimates
```

## {background-color="#006242"}

### Exercise 04 {.lato-font}

1. Return to your simulated sales data from the previous exercise (or start with the exercise solution)
2. Demonstrate that you can recover the parameters you used to simulate the data using either a frequentist or Bayesian model
3. Reflect on how the interval estimates of the two approaches differ and how you might interpret each of them
4. Submit your code, output, and reflection as a single PDF on Canvas

## {background-color="#288DC2"}

### Wrap Up {.permanent-marker-font}

#### Summary

- Finished building models using probability
- Demonstrated fitting models to recover parameters

#### Next Time

- Formally introduce linear regression

