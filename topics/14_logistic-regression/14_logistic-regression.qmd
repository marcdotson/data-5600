---
title: "Logistic Regression"
subtitle: "DATA 5600 Introduction to Regression and Machine Learning for Analytics"
author: Marc Dotson
title-slide-attributes:
  data-background-color: "#0F2439"
format: 
  revealjs:
    theme: ../slides.scss  # Modified slides theme
    code-copy: true        # Show code blocks copy button
    slide-number: c/t      # Numbered slides current/total
    embed-resources: true  # Render to a single HTML file
execute:
  eval: false
  echo: true
jupyter: python3
---

## {background-color="#288DC2"}

### Get Started {.permanent-marker-font}

#### Last Time

- Reviewed linear models
- Generalized linear models beyond regression

#### Preview

- Interpret logistic regression coefficients
- Review assumption diagnostics and remedies
- Expand diagnostics and remedies to GLMs

## {background-color="#D1CBBD"}

::: {.columns .v-center}
![](../../figures/workflow_explore-reconcile){fig-align="center"}
:::

# Interpreting Parameter Estimates

## 

:::: {.v-center}
$$
\Large{y_i \sim \text{Binomial}(1, p_i)} \\
\Large{p_i = {\exp\left(\beta_0 + \beta_1 x_{1,i} + \cdots + \beta_p x_{p,i}\right) \over 1 + \exp\left(\beta_0 + \beta_1 x_{1,i} + \cdots + \beta_p x_{p,i}\right)}}
$$
::::

## {visibility="uncounted"}

:::: {.v-center}
$$
\Large{y_i \sim \text{Binomial}(1, p_i)} \\
\Large{\color{grey}{p_i = {\exp\left(\beta_0 + \beta_1 x_{1,i} + \cdots + \beta_p x_{p,i}\right) \over 1 + \exp\left(\beta_0 + \beta_1 x_{1,i} + \cdots + \beta_p x_{p,i}\right)}}}
$$
::::

## {visibility="uncounted"}

:::: {.v-center}
$$
\Large{\color{grey}{y_i \sim \text{Binomial}(1, p_i)}} \\
\Large{p_i = {\exp\left(\beta_0 + \beta_1 x_{1,i} + \cdots + \beta_p x_{p,i}\right) \over 1 + \exp\left(\beta_0 + \beta_1 x_{1,i} + \cdots + \beta_p x_{p,i}\right)}}
$$
::::

## {visibility="uncounted"}

:::: {.v-center}
$$
\Large{\color{grey}{y_i \sim \text{Binomial}(1, p_i)}} \\
\Large{\log\left({p_i \over 1 - p_i}\right) = \beta_0 + \beta_1 x_{1,i} + \cdots + \beta_p x_{p,i}}
$$
::::

## Likelihoods and link functions

Using a **link function** so we can still have a linear model with a non-normal likelihood is a **generalized linear model**

::: {.incremental}
- There are lots of generalized linear models (GLMs), but **logistic regression** is the most common
- Each GLM gets its name from the **likelihood** (PMF or PDF), link function, or **inverse link function**
- Logistic regression is also known as **binomial regression**, **Bernoulli regression**, and the **logit model**
- Using a link function makes interpreting parameter estimates tricky
:::

## Multiple *linear* regression parameters

$$
\large{y_i \sim \text{Normal}(\mu_i, \sigma^2)} \\
\large{\mu_i = \beta_0 + \beta_1 x_{1,i} + \cdots + \beta_p x_{p,i}}
$$

- $\beta_0$ is the average of $y$ when $x_1 \cdots x_p = 0$
- $\beta_1$ is the average change in $y$ for every one unit increase in $x_1$, **holding all other predictors constant**

## Multiple *logistic* regression parameters

$$
\Large{y_i \sim \text{Binomial}(1, p_i)} \\
\Large{\log\left({p_i \over 1 - p_i}\right) = \beta_0 + \beta_1 x_{1,i} + \cdots + \beta_p x_{p,i}}
$$

Each $\beta$ represents the **log-odds** of $y = 1$ (i.e., "success") where $\beta > 0$ increases and $\beta < 0$ descrease the log-odds of $y = 1$

::: {.incremental}
- $\beta_0$ is the **log-odds** of $y = 1$ when $x_1 \cdots x_p = 0$
- $\beta_1$ is the **log-odds** of $y = 1$ for every one unit increase in $x_1$, **holding all other predictors constant** (and **relative to the reference level** if discrete)
:::

## Odds ratios and probabilities

It's not just you, **log-odds** are not intuitive

:::: {.columns}

::: {.fragment .column width="50%"}
**Odds Ratios**

::: {.incremental}
- Exponentiating the coefficients gives us **odds ratios** where $\exp(\beta) > 1$ increases and $\exp(\beta) < 1$ decreases the odds of $y = 1$ by $\exp(\beta)$ *times*
- For every one unit increase in $x_1$, the odds of $y = 1$ are *multiplied* by $\exp(\beta_1)$ or increase by $100 \times (\exp(\beta_1) - 1)\%$, **holding all other predictors constant** (and **relative to the reference level** if discrete)
:::
:::

::: {.fragment .column width="50%"}
**Probabilities**

::: {.incremental}
- Applying the logistic (i.e., **sigmoid**) function to the coefficients gives us **probabilities** where values closer to 1 increase and values closer to 0 decrease the probability of $y = 1$
- For every one unit increase in $x_1$, the probability of $y = 1$ changes by $\text{logistic}(\beta_1)$, **holding all other predictors constant** (and **relative to the reference level** if discrete)
:::
:::

::::

## Logistic regression parameters as *log-odds*

```{python}
#| eval: true
#| echo: false

import numpy as np
import polars as pl
import statsmodels.api as sm
import statsmodels.formula.api as smf

# Set randomization seed
rng = np.random.default_rng(42)

# Specify a function to simulate data
def sim_data(n, beta_0, beta_days, beta_large, beta_trade, beta_online):
    # Simulate days since first contact using a normal distribution
    days = rng.normal(20, 5, size=n)
    # Simulate large company size using a binomial distribution
    large = rng.binomial(1, 0.3, size=n)
    # Simulate trade show contact using a binomial distribution
    trade = rng.binomial(1, 0.2, size=n)
    # Simulate online contact using a binomial distribution
    online = rng.binomial(1, 0.7, size=n)
    
    # Simulate the probability of a lead qualifying
    prob_y = (
      np.exp(beta_0 + beta_days * days + beta_large * large + beta_trade * trade + beta_online * online) / 
      (1 + np.exp(beta_0 + beta_days * days + beta_large * large + beta_trade * trade + beta_online * online))
    )
    # Use prob_y to simulate the qualifed outcome variable
    qualified = rng.binomial(1, prob_y, size=n)

    # Return the output
    return qualified, days, large, trade, online

# Call the function and save as an array
data_arr = sim_data(n = 500, beta_0 = 0.10, beta_days = -0.07, beta_large = 1, beta_trade = 0.75, beta_online = 0.25)

# Convert to a dataframe
data_df = pl.DataFrame(data_arr, schema = ['qualified', 'days', 'large', 'trade', 'online'])

# Fit a frequentist logistic regression
fr_fit = smf.glm(
    'qualified ~ days + large + trade + online', 
    data = data_df.to_pandas(), 
    family = sm.families.Binomial()
).fit()
```

Each $\beta$ represents the **log-odds** of $y = 1$ (i.e., "success") where $\beta > 0$ increases and $\beta < 0$ decreases the log-odds of $y = 1$

```{python}
#| eval: true

# Confidence intervals
fr_fit.conf_int()
```

## Logistic regression parameters as *odds ratios*

Exponentiating the coefficients gives us **odds ratios** where $\exp(\beta) > 1$ increases and $\exp(\beta) < 1$ decreases the odds of $y = 1$ by $\exp(\beta)$ *times*

```{python}
#| eval: true

# Confidence intervals
np.exp(fr_fit.conf_int())
```

## Logistic regression parameters as *odds ratios* {visibility="uncounted"}

Exponentiating the coefficients gives us **odds ratios** where $\exp(\beta) > 1$ increases and $\exp(\beta) < 1$ decreases the odds of $y = 1$ by $\exp(\beta)$ *times*

```{python}
#| eval: true

# Confidence intervals
100 * np.exp(fr_fit.conf_int() - 1)
```

## Logistic regression parameters as *probabilities*

Applying the logistic (i.e., **sigmoid**) function to the coefficients gives us **probabilities** where values closer to 1 increase and values closer to 0 decrease the probability of $y = 1$

```{python}
#| eval: true

from scipy.special import expit

# Confidence intervals
expit(fr_fit.conf_int())
```

## Ensuring valid inference

We need to use **diagnostics** to confirm the model assumptions are met, and **remedy** any issues, so that our interpretation (i.e., statistical inference) is valid

::: {.fragment}
1. **Validity**: Data is relevant to the objective and no predictors are missing
2. **Representativeness**: Data is representative of the data generating process or population
3. **Linearity**: The mapping function from the predictors to the outcome is additive and a linear function of the parameters
4. **Independence**: Observations are independent of each other
5. **Constant Variance**: Homoscedasticity or constant variance of errors
6. **Normality**: Variation or error is normally distributed
7. **Identifiability**: Data allows for parameters to be estimated, including no strong multicollinearity
:::

## Ensuring valid inference {visibility="uncounted"}

We need to use **diagnostics** to confirm the model assumptions are met, and **remedy** any issues, so that our interpretation (i.e., statistical inference) is valid

1. **Validity**: Data is relevant to the objective and no predictors are missing
2. **Representativeness**: Data is representative of the data generating process or population
3. **Linearity**: The mapping function from the predictors to the outcome is additive and a linear function of the parameters
4. **Independence**: Observations are independent of each other
5. ~~**Constant Variance**: Homoscedasticity or constant variance of errors~~
6. ~~**Normality**: Variation or error is normally distributed~~
7. **Identifiability**: Data allows for parameters to be estimated, including no strong multicollinearity

# We want to keep our model as simple as possible, so why would we use multiple logistic regression? {background-color="#006242"}

# Validity and Representativeness






# Why is modeling the mean of a normal distribution with a linear function of predictors a good model? {background-color="#006242"}

# Linearity, Independence, and Identifiability



## {background-color="#006242"}

### Exercise 14 {.lato-font}

1. Clean up the soft launch data
2. Create two interesting visualizations that help you understand the data and its limitations
3. Walk through the validity, representativeness, and additivity/linearity assumptions and justify whether or not they are satisfied for your cleaned data
4. Submit your code, output, and explanations as a single PDF on Canvas

## {background-color="#288DC2"}

### Wrap Up {.permanent-marker-font}

#### Summary

- Interpreted logistic regression coefficients
- Reviewed assumption diagnostics and remedies
- Expanded diagnostics and remedies to GLMs

#### Next Time

- Extend model fitting beyond ordinary least squares
- Continue comparing frequentist and Bayesian inference

