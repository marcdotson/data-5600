---
title: "Logistic Regression"
subtitle: "DATA 5600 Introduction to Regression and Machine Learning for Analytics"
author: Marc Dotson
title-slide-attributes:
  data-background-color: "#0F2439"
format: 
  revealjs:
    theme: ../slides.scss  # Modified slides theme
    code-copy: true        # Show code blocks copy button
    slide-number: c/t      # Numbered slides current/total
    embed-resources: true  # Render to a single HTML file
execute:
  eval: false
  echo: true
jupyter: python3
---

## {background-color="#288DC2"}

### Get Started {.permanent-marker-font}

#### Last Time

- Reviewed linear models
- Generalized linear models beyond regression

#### Preview

- Interpret logistic regression coefficients
- Review assumption diagnostics and remedies
- Expand diagnostics and remedies to GLMs

## {background-color="#D1CBBD"}

::: {.columns .v-center}
![](../../figures/workflow_explore-reconcile){fig-align="center"}
:::

# Interpreting Parameter Estimates

## 

:::: {.v-center}
$$
\Large{y_i \sim \text{Binomial}(1, p_i)} \\
\Large{p_i = {\exp\left(\beta_0 + \beta_1 x_{1,i} + \cdots + \beta_p x_{p,i}\right) \over 1 + \exp\left(\beta_0 + \beta_1 x_{1,i} + \cdots + \beta_p x_{p,i}\right)}}
$$
::::

## {visibility="uncounted"}

:::: {.v-center}
$$
\Large{y_i \sim \text{Binomial}(1, p_i)} \\
\Large{\color{grey}{p_i = {\exp\left(\beta_0 + \beta_1 x_{1,i} + \cdots + \beta_p x_{p,i}\right) \over 1 + \exp\left(\beta_0 + \beta_1 x_{1,i} + \cdots + \beta_p x_{p,i}\right)}}}
$$
::::

## {visibility="uncounted"}

:::: {.v-center}
$$
\Large{\color{grey}{y_i \sim \text{Binomial}(1, p_i)}} \\
\Large{p_i = {\exp\left(\beta_0 + \beta_1 x_{1,i} + \cdots + \beta_p x_{p,i}\right) \over 1 + \exp\left(\beta_0 + \beta_1 x_{1,i} + \cdots + \beta_p x_{p,i}\right)}}
$$
::::

## {visibility="uncounted"}

:::: {.v-center}
$$
\Large{\color{grey}{y_i \sim \text{Binomial}(1, p_i)}} \\
\Large{\log\left({p_i \over 1 - p_i}\right) = \beta_0 + \beta_1 x_{1,i} + \cdots + \beta_p x_{p,i}}
$$
::::

## Likelihoods and link functions

Using a **link function** so we can still have a linear model with a non-normal likelihood is a **generalized linear model**

::: {.incremental}
- There are lots of generalized linear models (GLMs), but **logistic regression** is the most common
- Each GLM gets its name from the **likelihood** (PMF or PDF), link function, or **inverse link function**
- Logistic regression is also known as **binomial regression**, **Bernoulli regression**, and the **logit model**
- Using a link function makes interpreting parameter estimates tricky
:::

## Multiple *linear* regression parameters

$$
\large{y_i \sim \text{Normal}(\mu_i, \sigma^2)} \\
\large{\mu_i = \beta_0 + \beta_1 x_{1,i} + \cdots + \beta_p x_{p,i}}
$$

- $\beta_0$ is the average of $y$ when $x_1 \cdots x_p = 0$
- $\beta_1$ is the average change in $y$ for every one unit increase in $x_1$, **holding all other predictors constant** (and **relative to the reference level** if discrete)

## Multiple *logistic* regression parameters

$$
\Large{y_i \sim \text{Binomial}(1, p_i)} \\
\Large{\log\left({p_i \over 1 - p_i}\right) = \beta_0 + \beta_1 x_{1,i} + \cdots + \beta_p x_{p,i}}
$$

Each $\beta$ represents the **log-odds** of $y = 1$ (i.e., "success") where $\beta > 0$ increases and $\beta < 0$ descrease the log-odds of $y = 1$

::: {.incremental}
- $\beta_0$ is the **log-odds** of $y = 1$ when $x_1 \cdots x_p = 0$
- $\beta_1$ is the **log-odds** of $y = 1$ for every one unit increase in $x_1$, **holding all other predictors constant** (and **relative to the reference level** if discrete)
:::

## Odds ratios and probabilities

It's not just you, **log-odds** are not intuitive

:::: {.columns}

::: {.fragment .column width="50%"}
**Odds Ratios**

::: {.incremental}
- Exponentiating the coefficients gives us **odds ratios** where $\exp(\beta) > 1$ increases and $\exp(\beta) < 1$ decreases the odds of $y = 1$ by $\exp(\beta)$ *times*
- For every one unit increase in $x_1$, the odds of $y = 1$ are *multiplied* by $\exp(\beta_1)$ or increase by $100 \times (\exp(\beta_1) - 1)\%$, **holding all other predictors constant** (and **relative to the reference level** if discrete)
:::
:::

::: {.fragment .column width="50%"}
**Probabilities**

::: {.incremental}
- Applying the logistic (i.e., **sigmoid**) function to the coefficients gives us **probabilities** where values closer to 1 increase and values closer to 0 decrease the probability of $y = 1$
- For every one unit increase in $x_1$, the probability of $y = 1$ changes by $\text{logistic}(\beta_1)$, **holding all other predictors constant** (and **relative to the reference level** if discrete)
:::
:::

::::

## Logistic regression parameters as *log-odds*

```{python}
#| eval: true
#| echo: false

import numpy as np
import polars as pl
import statsmodels.api as sm
import statsmodels.formula.api as smf

# Set randomization seed
rng = np.random.default_rng(42)

# Specify a function to simulate data
def sim_data(n, beta_0, beta_days, beta_large, beta_trade, beta_online):
    # Simulate days since first contact using a normal distribution
    days = rng.normal(20, 5, size=n)
    # Simulate large company size using a binomial distribution
    large = rng.binomial(1, 0.3, size=n)
    # Simulate trade show contact using a binomial distribution
    trade = rng.binomial(1, 0.2, size=n)
    # Simulate online contact using a binomial distribution
    online = rng.binomial(1, 0.7, size=n)
    
    # Simulate the probability of a lead qualifying
    prob_y = (
      np.exp(beta_0 + beta_days * days + beta_large * large + beta_trade * trade + beta_online * online) / 
      (1 + np.exp(beta_0 + beta_days * days + beta_large * large + beta_trade * trade + beta_online * online))
    )
    # Use prob_y to simulate the qualifed outcome variable
    qualified = rng.binomial(1, prob_y, size=n)

    # Return the output
    return qualified, days, large, trade, online

# Call the function and save as an array
data_arr = sim_data(n = 500, beta_0 = 0.10, beta_days = -0.07, beta_large = 1, beta_trade = 0.75, beta_online = 0.25)

# Convert to a dataframe
data_df = pl.DataFrame(data_arr, schema = ['qualified', 'days', 'large', 'trade', 'online'])

# Fit a frequentist logistic regression
fr_fit = smf.glm(
    'qualified ~ days + large + trade', 
    data = data_df.to_pandas(), 
    family = sm.families.Binomial()
).fit()
```

Each $\beta$ represents the **log-odds** of $y = 1$ (i.e., "success") where $\beta > 0$ increases and $\beta < 0$ decreases the log-odds of $y = 1$

```{python}
#| eval: true

# Confidence intervals
fr_fit.conf_int()
```

::: {.fragment}
Holding all other predictors constant, we are 95% confident that for every additional day since first contact, the log-odds of a lead qualifying decreases between -0.097 and -0.017
:::

## Logistic regression parameters as *odds ratios*

Exponentiating the coefficients gives us **odds ratios** where $\exp(\beta) > 1$ increases and $\exp(\beta) < 1$ decreases the odds of $y = 1$ by $\exp(\beta)$ *times*

```{python}
#| eval: true

# Confidence intervals
np.exp(fr_fit.conf_int())
```

::: {.fragment}
Holding all other predictors constant, we are 95% confident that for a large company (relative to a small company), the odds of a lead qualifying increases between 1.61 and 3.57 times
:::

::: {.fragment .fade-up}
For *negative* log-odds, it can be helpful to think about the **reciprocal** of the odds ratio (i.e., $1 / \exp(\beta)$) to understand how many times more likely $y = 0$ is to occur
:::

## Logistic regression parameters as *odds ratios*

Exponentiating the coefficients gives us **odds ratios** where $\exp(\beta) > 1$ increases and $\exp(\beta) < 1$ decreases the odds of $y = 1$ by $\exp(\beta)$ *times*

```{python}
#| eval: true

# Confidence intervals
100 * np.exp(fr_fit.conf_int() - 1)
```

::: {.fragment}
Holding all other predictors constant, we are 95% confident that for a large company (relative to a small company), the odds of a lead qualifying increases between 59.23% and 131.50%
:::

::: {.fragment .fade-up}
Again, for *negative* log-odds, it can be helpful to think about the **reciprocal** of the odds ratio percentage (i.e., $100 \times (1 / (\exp(\beta_1) - 1))\%$) for the odds of $y = 0$
:::

## Logistic regression parameters as *probabilities*

Applying the logistic (i.e., **sigmoid**) function to the coefficients gives us **probabilities** where values closer to 1 increase and values closer to 0 decrease the probability of $y = 1$

```{python}
#| eval: true

from scipy.special import expit

# Confidence intervals
expit(fr_fit.conf_int())
```

::: {.fragment}
Holding all other predictors constant, we are 95% confident that a lead generated at a trade show (relative to it not being generated at a trade show), the probability of a lead qualifying increases between 0.592 and 0.779
:::

## Ensuring valid inference

We need to use **diagnostics** to confirm the model assumptions are met, and **remedy** any issues, so that our interpretation (i.e., statistical inference) is valid

::: {.fragment}
1. **Validity**: Data is relevant to the objective and no predictors are missing
2. **Representativeness**: Data is representative of the data generating process or population
3. **Linearity**: The mapping function from the predictors to the outcome is additive and a linear function of the parameters
4. **Independence**: Observations are independent of each other
5. **Constant Variance**: Homoscedasticity or constant variance of errors
6. **Normality**: Variation or error is normally distributed
7. **Identifiability**: Data allows for parameters to be estimated, including no strong multicollinearity
:::

## Ensuring valid inference {visibility="uncounted"}

We need to use **diagnostics** to confirm the model assumptions are met, and **remedy** any issues, so that our interpretation (i.e., statistical inference) is valid

1. **Validity**: Data is relevant to the objective and no predictors are missing
2. **Representativeness**: Data is representative of the data generating process or population
3. **Linearity**: The mapping function from the predictors to the outcome is additive and a linear function of the parameters
4. **Independence**: Observations are independent of each other
5. ~~**Constant Variance**: Homoscedasticity or constant variance of errors~~
6. ~~**Normality**: Variation or error is normally distributed~~
7. **Identifiability**: Data allows for parameters to be estimated, including no strong multicollinearity

# We want to keep our model as simple as possible, so why would we use multiple logistic regression? {background-color="#006242"}

# Validity and Representativeness

## Validity

Data is relevant to the objective and no predictors are missing

::: {.fragment}
Why should we care about **omitted variables**? [What about including variables that don't matter?]{.fragment}
:::

:::: {.columns}

::: {.fragment .column width="50%"}
**Diagnostics**

::: {.incremental}
- Is the data relevant to the objective?
- How well does the data match the ideal data?
- How do the simulated and real outcomes compare?
:::
:::

::: {.fragment .column width="50%"}
**Fixes**

::: {.incremental}
- Get different data
- Find additional data
- Justify **proxy variables**
:::
:::

::::

## Representativeness

Data is representative of the data generating process or population

::: {.fragment}
Why should we care about the data being representative?
:::

:::: {.columns}

::: {.fragment .column width="50%"}
**Diagnostics**

::: {.incremental}
- How was the data collected?
- Is it representiave of the process or population we care about?
- Is data **missing at random**?
- Check for **influential points**
:::
:::

::: {.fragment .column width="50%"}
**Fixes**

::: {.incremental}
- Get different data
- **Drop or impute** missing data
- Investigate influential points and **consider** removing
:::
:::

::::

# What data are you considering for your second project? Discuss validity and representativeness as a group. {background-color="#006242"}

# Linearity, Independence, and Identifiability

## Functional form of the deterministic component

The mapping function from the predictors to the outcome is additive and a linear function of the parameters

::: {.fragment}
If the process or population isn't approximated by an additive and linear function, what happens to our inferences?
:::

:::: {.columns}

::: {.fragment .column width="50%"}
**Diagnostics**

::: {.incremental}
- Scatterplot of **log-odds** and predictor(s)
- Partial regression plots
:::
:::

::: {.fragment .column width="50%"}
**Fixes**

::: {.incremental}
- Transform $X$
- Transform $y$
- Use a nonlinear model
:::
:::

::::

## Scatterplot of *log-odds* and predictor(s)

The relationship between the **log-odds** and predictor(s) should be roughly linear and **monotone** (i.e., always increasing or always decreasing) in probability

![](../../figures/temp_linearity.png){fig-align="center"}

## Independence and exchangeability

This assumption is often expressed as observations being **independent and identically distributed** (i.e., iid) or, if not identical, at least **exchangeable** (i.e., there is no order to the observations)

::: {.fragment}
In what ways could this assumption be violated (and often is)?
:::

:::: {.columns}

::: {.fragment .column width="50%"}
**Diagnostics**

::: {.incremental}
- Is the data a random sample?
- Is each observation a different unit or are there **repeat measures**?
- Are the observations **clustered** (e.g., a hierarchy or spatially corellated)?
- Is there a **sequence** to the observations (e.g., a time series)?
:::
:::

::: {.fragment .column width="50%"}
**Fixes**

::: {.incremental}
- Include a predictor to control for the dependence
- Use a more advanced model (i.e., multilevel, spatial, or time series model)
:::
:::

::::

## Multicollinearity and identifiability

**Multicollinearity** is when two or more predictors are highly correlated

::: {.fragment}
Why would this be a problem for identifiability (think about how we interpret the slope parameters)?
:::

:::: {.columns}

::: {.fragment .column width="50%"}
**Diagnostics**

::: {.incremental}
- Scatterplot matrix
- Correlation matrix
- Variance inflation factors (VIF)
:::
:::

::: {.fragment .column width="50%"}
**Fixes**

::: {.incremental}
- Drop some predictors
- Use penalized regression (e.g., ridge, lasso, elastic net)
- Combine the correlated predictors (e.g., PCR)
:::
:::

::::

## Compute variance inflation factors

VIF is based on $R^2$, but we don’t have a way to calculate $R^2$ in logistic regression (i.e., the loss function used to compute the point estimates isn't quadratic, so no sums of squares)

We can still run an OLS model to compute VIF for a model with a binary outcome since our its based on the relationship among predictors and the way the model is estimated isn't relevate to estimating collinearity

## {background-color="#006242"}

### Exercise 14 {.lato-font}

1. Clean up the leads data
2. Create two interesting visualizations that help you understand the data and its limitations, including plotting a sigmoid curve
3. Walk through the logistic regression model assumptions, use diagnostics, and justify whether or not they are satisfied for your cleaned data
4. Submit your code, output, and explanations as a single PDF on Canvas

## {background-color="#288DC2"}

### Wrap Up {.permanent-marker-font}

#### Summary

- Interpreted logistic regression coefficients
- Reviewed assumption diagnostics and remedies
- Expanded diagnostics and remedies to GLMs

#### Next Time

- Extend model fitting beyond ordinary least squares
- Continue comparing frequentist and Bayesian inference

