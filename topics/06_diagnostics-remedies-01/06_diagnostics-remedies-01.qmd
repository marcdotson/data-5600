---
title: "Diagnostics and Remedies"
subtitle: "DATA 5600 Introduction to Regression and Machine Learning for Analytics"
author: Marc Dotson
title-slide-attributes:
  data-background-color: "#0F2439"
format: 
  revealjs:
    theme: ../slides.scss  # Modified slides theme
    code-copy: true        # Show code blocks copy button
    slide-number: c/t      # Numbered slides current/total
    embed-resources: true  # Render to a single HTML file
execute:
  eval: false
  echo: true
jupyter: python3
---

## {background-color="#288DC2"}

### Get Started {.permanent-marker-font}

#### Last Time

- Formally introduced linear regression

#### Preview

- Start working with real data
- Conduct exploratory data analysis
- Begin reconciling the data and the model

## {background-color="#D1CBBD"}

::: {.columns .v-center}
![](../../figures/workflow_explore-reconcile.png){fig-align="center"}
:::

## Assumptions all the way down

We need to use **diagnostics** to confirm the model assumptions are met, and **remedy** any issues, so that our interpretation (i.e., statistical inference) is valid

:::: {.columns}

::: {.column width="50%"}
::: {.fragment}
![](../../figures/models_all-02.png){fig-align="center"}
:::
:::

::: {.column width="50%"}
::: {.incremental}
- Flexible, black-box models aren't interpretable
- Inferences from interpretable models require assumptions
- Causal effects require **additional assumptions** to be met
:::
:::

::::

## Linear regression assumptions

1. **Validity**: Data is relevant to the objective and no predictors are missing
2. **Representativeness**: Data is representative of the data generating process or population
3. **Additivity and Linearity**: The mapping function from the predictors to the outcome is additive and a linear function of the parameters
4. **Independence**: Observations are independent of each other
5. **Constant Variance**: Homoscedasticity or constant variance of errors
6. **Normality**: Variation or error is normally distributed
7. **Identifiability**: Data allows for parameters to be estimated, including no strong multicollinearity

# Exploratory Data Analysis

## Understand the data and its limitations

Remember that it is your job as a data analyst to understand the data so you can communicate about the data and address its limitations

::: {.incremental}
- Visualize data and relationships
- Compute numeric summaries
- Check for missing data, outliers, and errors
- Consider proxy variables
- Ask questions
:::

::: {.fragment}
Conducting EDA **with the model assumptions in mind** will help you identify potential issues and needed remedies early
:::

## Parquet, *pour quoi*?

Apache Parquet is an open source, **column-oriented data file format** designed for efficient data storage and retrieval

```{python}
#| eval: true
#| echo: false

import os
import polars as pl
import seaborn as sns
import seaborn.objects as so

# Import data
# sl_data = pl.read_parquet(os.path.join('..', '..', 'data', 'soft_launch.parquet'))
sl_data = (pl.read_parquet(os.path.join('..', '..', 'data', 'original_df.parquet'))
  .select('customer_id', 'units', 'sales', 'brand', 'promo', 'loyal', 'texture', 'size', 'price')
  .cast({'size': pl.String})
)
```

```{python}
import os
import polars as pl
import seaborn as sns
import seaborn.objects as so

# Import data
sl_data = pl.read_parquet(os.path.join('data', 'soft_launch.parquet'))
```

## {.scrollable}

Let's look at some already cleaned data to review the **data dictionary** and use Positron's **data explorer**

```{python}
#| eval: true

sl_data
```

## Variable types and relationships

:::: {.columns}

::: {.fragment .column width="33%"}
**Discrete**

"Individually separate and distinct" (i.e., **categorical** or qualitative)

::: {.incremental}
- Counts
- Column/bar plots
- Facets
:::
:::

::: {.fragment .column width="33%"}
**Continuous**

"Forming an unbroken whole; without interruption" (i.e., **numeric** or quantitative)

::: {.incremental}
- Means
- Histograms
- Correlations
- Scatterplots
:::
:::

::: {.fragment .column width="33%"}
**Discrete and Continuous**

Comparing continuous values by discrete levels

::: {.incremental}
- Grouped summaries
- Boxplots
- Density plots
- Facets
:::
:::

::::

## Counts and column plots

I recommend visualization libraries that follow the [grammar of graphics](https://link.springer.com/book/10.1007/0-387-28695-0) where each plot is composed of **data**, a **mapping** from data to visual elements, the **specific graphic** (e.g., Marks and Stats)

```{python}
#| eval: true
#| output-location: column

pb_data = sl_data.filter(pl.col('units') > 0)

texture_count = (pb_data
  .group_by(pl.col('texture'))
  .agg(n = pl.len())
)

(so.Plot(texture_count, x = 'texture', y = 'n')
  .add(so.Bar())
)
```

## Counts and column plots

```{python}
#| eval: true
#| output-location: column

(so.Plot(pb_data, x = 'texture')
  .add(so.Bar(), so.Hist())
)
```

::: {.fragment}
While not an issue with an assumption, how will our model handle **discrete** predictors? Can we include all of the levels of a given discrete predictor?
:::

## Counts and column plots

```{python}
#| eval: true
#| output-location: column

texture_count = (pb_data
  .group_by(pl.col(['texture', 'size']))
  .agg(n = pl.len())
)

(so.Plot(texture_count, x = 'texture', y = 'n', color = 'size')
  .add(so.Bar(), so.Dodge())
)
```

## Counts, column plots, and facets

```{python}
#| eval: true
#| output-location: column

texture_count = (pb_data
  .group_by(pl.col(['texture', 'size', 'brand']))
  .agg(n = pl.len())
  .with_columns(
    (pl.col('n') / pl.col('n').sum().over(['texture', 'size'])).alias('prop')
  ).sort('brand')
)

(so.Plot(texture_count, x = 'texture', y = 'prop', color = 'brand')
  .facet('size')
  .add(so.Bar(), so.Stack())
)
```

## Means and histograms

```{python}
#| eval: true
#| output-location: column

pb_data['units'].drop_nans().mean()
```

```{python}
#| eval: true
#| output-location: column

(so.Plot(pb_data, x = 'units')
  .add(so.Bars(), so.Hist(bins = 10))
)
```

::: {.fragment}
Why is **right-skewed continuous data** common? Why might this be a problem with our assumptions?
:::

## Correlations

```{python}
#| eval: true
#| output-location: column

(pb_data
  .select(['price', 'units'])
  .drop_nans()
  .corr()
)
```

## Scatterplots

```{python}
#| eval: true
#| output-location: column

(so.Plot(pb_data, x = 'price', y = 'units')
    .add(
      so.Dot(alpha = 0.25), 
      so.Jitter(x = 0.25, y = 0.25)
    )
    .add(so.Line(), so.PolyFit(order = 1))
)
```

## Correlation matrices

```{python}
#| eval: true
#| output-location: column

(pb_data
  .select(['price', 'units', 'sales'])
  .drop_nans()
  .corr()
)
```

## Scatterplots matrices

```{python}
#| eval: true
#| output-location: column

sns.pairplot(
  pb_data.select(['price', 'units', 'sales'])
  .to_pandas()
)
```

## Grouped summaries

```{python}
#| eval: true
#| output-location: column

(pb_data
  .group_by(pl.col(['texture', 'size']))
  .agg(
    n = pl.len(), 
    avg_units = pl.col('units').drop_nans().mean(), 
    avg_sales = pl.col('sales').drop_nans().mean()
  )
  .sort(pl.col('avg_units'), descending=True)
)
```

## Boxplots

```{python}
#| eval: true
#| output-location: column

sns.catplot(
  pb_data, x = 'units', y = 'brand', kind = 'box'
)
```

## Density plots

```{python}
#| eval: true
#| output-location: column

(so.Plot(pb_data, x = 'units', color = 'brand')
  .add(so.Area(), so.Hist(bins=10))
)
```

## Density plots and facets

```{python}
#| eval: true
#| output-location: column

(so.Plot(pb_data, x = 'units', color = 'brand')
  .facet(col = 'size', row = 'texture')
  .add(so.Area(), so.Hist(bins=10))
)
```

# Copy some visualization code and apply it to a different variable or set of variables. What do you discover? {background-color="#006242"}

# Validity and Representativeness

## Validity

Data is relevant to the objective and no predictors are missing

::: {.fragment}
Why should we care about **omitted variables**? What about including variables that don't matter?
:::

:::: {.columns}

::: {.fragment .column width="50%"}
**Diagnostics**

::: {.incremental}
- Is the data relevant to the objective?
- How well does the data match the ideal data?
- How do the simulated and real outcomes compare?
:::
:::

::: {.fragment .column width="50%"}
**Fixes**

::: {.incremental}
- Get different data
- Find additional data
- Justify proxy variables
:::
:::

::::

## Representativeness

Data is representative of the data generating process or population

::: {.fragment}
Why should we care about the data being representative? What's the difference between **underfitting** and **overfitting**?
:::

:::: {.columns}

::: {.fragment .column width="50%"}
**Diagnostics**

::: {.incremental}
- How was the data collected?
- Is it representiave of the process or population we care about?
- Is data missing at random?
- Check for **influential points**
:::
:::

::: {.fragment .column width="50%"}
**Fixes**

::: {.incremental}
- Get different data
- Impute missing data
- Investigate influential points and **consider** removing
:::
:::

::::

## Outliers vs. leverage points

**Outliers** and **leverage points** are both *potential* influential points, data that unduly influences the linear regression

::: {.incremental}
- An outlier is a point whose response does not follow the general trend of the rest of the data (i.e., extreme $y$ value)
- A leverage point is a point with a predictor value that is outside the norm (extreme $X$ value)
:::

::: {.fragment}
![](../../figures/assumption_representative-01.png){fig-align="center"}
:::

## Outliers vs. leverage points {visibility="uncounted"}

**Outliers** and **leverage points** are both *potential* influential points, data that unduly influences the linear regression

- An outlier is a point whose response does not follow the general trend of the rest of the data (i.e., extreme $y$ value)
- A leverage point is a point with a predictor value that is outside the norm (extreme $X$ value)

![](../../figures/assumption_representative-02.png){fig-align="center"}

## Check for influential points

To diagnose influential points, we can use visualizations or we can fit a model and use either **DFBETAS** or **DFFITS** to determine how "df" or "different" estimates or the overall model fit would be if a given observation were removed, respectively

```{python}
#| eval: true
#| echo: false

pb_data = (pb_data
  .drop_nans()
  .to_dummies(
      columns = ['brand', 'promo', 'texture', 'size'], drop_first = True
  ).with_columns(
      (pl.col('units') + 1).log().alias('log_units'),
      (pl.col('price') + 1).log().alias('log_price')
  ).to_pandas()
)
```

```{python}
#| eval: true

import numpy as np
import matplotlib.pyplot as plt
import statsmodels.formula.api as smf

# Fit a model
fit_01 = smf.ols(
  'log_units ~ log_price + brand_Jif + brand_Skippy + brand_PeterPan + promo_true + texture_Smooth + size_12', 
  data = pb_data
).fit()

# Save the dffits values
pb_data['dffits'] = fit_01.get_influence().dffits[0]
# pb_data['residuals'] = fit_01.resid
# pb_data['fittedvalues'] = fit_01.fittedvalues
```

## DFFITS

How different would the fitted value $\hat{y}_i$ be if observation $i$ wasn't used?

- $|\text{DFFITS}_i| \gt 1$ for $n \leq 30$
- $|\text{DFFITS}_i| \gt 2\sqrt{p/n}$ for $n \gt 30$

where $p$ represents the number of $\beta$s in the model (including the intercept)

```{python}
#| eval: true
#| output-location: column

fig = plt.figure(figsize = (4, 4))
plt.ylabel("DFFITS (Absolute Values)")
plt.xlabel("Observation Number")
plt.scatter(
  pb_data.index, 
  np.abs(pb_data['dffits']), 
  s = 3
)
plt.axhline(
  y = 2 * np.sqrt(len(fit_01.params) / len(pb_data)), 
  color = 'r', 
  linestyle = 'dashed'
)
plt.show()
```

## Investigate influential points

Just because a point may be influential **doesn't mean it should be removed**, it means we should investigate it and decide if its representative

```{python}
#| eval: true

outliers_dffits = pb_data[np.abs(pb_data['dffits']) > 2 * np.sqrt(len(fit_01.params) / len(pb_data))]
outliers_dffits_sorted = outliers_dffits.sort_values(by = 'dffits', ascending = False)
outliers_dffits_sorted.head()
```

# Why are validity and representativeness the most important of the linear regression assumptions? {background-color="#006242"}

# Additivity and Linearity

## Additivity and Linearity


```{python}
#| eval: true

# Save the residulas and fitted values
pb_data['residuals'] = fit_01.resid
pb_data['fittedvalues'] = fit_01.fittedvalues
```

:::: {.columns .v-center}

::: {.column width="60%"}
$$
y_i = \beta_0 + \beta_1 x_{1,i} + \cdots + \beta_p x_{p,i} + \epsilon_i \\
\text{where } \epsilon_i \sim \text{Normal}(0, \sigma^2)
$$

$$
\mu_i = \beta_0 + \beta_1 x_{1,i} + \cdots + \beta_p x_{p,i} \\
y_i \sim \text{Normal}(\mu_i, \sigma^2)
$$
:::

::: {.column width="40%"}
The mapping function from the predictors to the outcome is additive and a linear function of the parameters
:::

::::

## {background-color="#006242"}

### Exercise 06 {.lato-font}

Clean up the soft launch data
Create two interesting visualizations that help you understand the data and its limitations

1. Review the materials from the course thus far
2. Identify lingering questions you have about the concepts covered
3. Use an AI tool to ask your questions and reflect on the responses you receive
4. Identify at least two questions you feel have been answered and share your prompts, the responses you received, and your reflections on the responses
5. Submit your prompts, responses, and reflections as a single PDF on Canvas

## {background-color="#288DC2"}

### Wrap Up {.permanent-marker-font}

#### Summary

- Started working with real data
- Conducted exploratory data analysis
- Began reconciling the data and the model

#### Next Time

- 

