---
title: "Diagnostics and Remedies"
subtitle: "DATA 5600 Introduction to Regression and Machine Learning for Analytics"
author: Marc Dotson
title-slide-attributes:
  data-background-color: "#0F2439"
format: 
  revealjs:
    theme: ../slides.scss  # Modified slides theme
    code-copy: true        # Show code blocks copy button
    slide-number: c/t      # Numbered slides current/total
    embed-resources: true  # Render to a single HTML file
execute:
  eval: false
  echo: true
  cache: false
jupyter: python3
---

## {background-color="#288DC2"}

### Get Started {.permanent-marker-font}

<!-- TODO: 

- Limit EDA and start with feature engineering before (sprinkle throughout?) model assumptions, including log-transformations, dummy-coding, and interaction terms (see modeling workflow redux).
- Add log-transform, spider-man multicollinearity memes
- Divide assumptions:
  - Validity, Representativeness, and Linearity (correct from Additivity and Linearity)
  - Independence, Constant Variance, Normality, and Identifiability
- Figure out a way to simplify and streamline assumption evaluation. Provide functions â€“ something!
- Compare more before and after plots when the assumptions are violated and when they are satisfied using the same tools provided. -->

#### Last Time

- Formally introduced linear regression

#### Preview

- Start working with real data
- Conduct exploratory data analysis
- Begin reconciling the data and the model

## {background-color="#D1CBBD"}

::: {.columns .v-center}
![](../../figures/workflow_explore-reconcile.png){fig-align="center"}
:::

## Assumptions all the way down

We need to use **diagnostics** to confirm the model assumptions are met, and **remedy** any issues, so that our interpretation (i.e., statistical inference) is valid

:::: {.columns}

::: {.column width="50%"}
::: {.fragment}
![](../../figures/models_all.png){fig-align="center"}
:::
:::

::: {.column width="50%"}
::: {.incremental}
- Flexible, black-box models aren't interpretable
- Inferences from interpretable models require assumptions
- Causal effects require **additional assumptions** to be met
:::
:::

::::

## Linear regression assumptions

1. **Validity**: Data is relevant to the objective and no predictors are missing
2. **Representativeness**: Data is representative of the data generating process or population
3. **Additivity and Linearity**: The mapping function from the predictors to the outcome is additive and a linear function of the parameters
4. **Independence**: Observations are independent of each other
5. **Constant Variance**: Homoscedasticity or constant variance of errors
6. **Normality**: Variation or error is normally distributed
7. **Identifiability**: Data allows for parameters to be estimated, including no strong multicollinearity

# Exploratory Data Analysis

## Understand the data and its limitations

Remember that it is your job as a data analyst to understand the data so you can communicate about the data and address its limitations

::: {.incremental}
- Visualize data and relationships
- Compute numeric summaries
- Check for missing data, outliers, and errors
- Consider proxy variables
- Ask questions
:::

::: {.fragment}
Conducting EDA **with the model assumptions in mind** will help you identify potential issues and needed remedies early
:::

## Parquet, *pour quoi*?

Apache Parquet is an open source, **column-oriented data file format** designed for efficient data storage and retrieval

```{python}
#| eval: true
#| echo: false

import os
import polars as pl
import seaborn.objects as so
import seaborn as sns
import matplotlib.pyplot as plt

# Import data
sl_data = (pl.read_parquet(os.path.join('..', '..', 'data', 'original_df.parquet'))
  .select('customer_id', 'units', 'sales', 'brand', 'promo', 'loyal', 'texture', 'size', 'price')
  .cast({'size': pl.String})
)
```

```{python}
import os
import polars as pl
import seaborn.objects as so
import seaborn as sns
import matplotlib.pyplot as plt

# Import data
sl_data = pl.read_parquet(os.path.join('soft_launch.parquet'))
```

::: {.fragment}
The working directory may be different depending on how you run the code

- The **root** when running code line-by-line
- The **current folder** when rendering the document to PDF
:::

## {.scrollable}

Let's look at some already cleaned data to review the **data dictionary** and use Positron's **data explorer**

```{python}
#| eval: true

sl_data
```

## Variable types and relationships

:::: {.columns}

::: {.fragment .column width="33%"}
**Discrete**

"Individually separate and distinct" (i.e., **categorical** or qualitative)

::: {.incremental}
- Counts
- Column/bar plots
- Facets
:::
:::

::: {.fragment .column width="33%"}
**Continuous**

"Forming an unbroken whole; without interruption" (i.e., **numeric** or quantitative)

::: {.incremental}
- Means
- Histograms
- Correlations
- Scatterplots
:::
:::

::: {.fragment .column width="33%"}
**Discrete and Continuous**

Comparing continuous values by discrete levels

::: {.incremental}
- Grouped summaries
- Boxplots
- Density plots
- Facets
:::
:::

::::

## Counts and column plots

I recommend visualization libraries that follow the [grammar of graphics](https://link.springer.com/book/10.1007/0-387-28695-0) where each plot is composed of **data**, a **mapping** from data to visual elements, the **specific graphic** (e.g., Marks and Stats)

```{python}
#| eval: true
#| output-location: column

pb_data = sl_data.filter(pl.col('units') > 0)

texture_count = (pb_data
  .group_by(pl.col('texture'))
  .agg(n = pl.len())
)

(so.Plot(texture_count, x = 'texture', y = 'n')
  .add(so.Bar())
)
```

## Counts and column plots

```{python}
#| eval: true
#| output-location: column

(so.Plot(pb_data, x = 'texture')
  .add(so.Bar(), so.Hist())
)
```

::: {.fragment}
While not an issue with an assumption, how will our model handle **discrete** predictors? Can we include all of the levels of a given discrete predictor?
:::

## Counts and column plots

```{python}
#| eval: true
#| output-location: column

texture_count = (pb_data
  .group_by(pl.col(['texture', 'size']))
  .agg(n = pl.len())
)

(so.Plot(
    texture_count, 
    x = 'texture', 
    y = 'n', 
    color = 'size'
  ).add(so.Bar(), so.Dodge())
)
```

## Counts, column plots, and facets

```{python}
#| eval: true
#| output-location: column

texture_count = (pb_data
  .group_by(pl.col(['texture', 'size', 'brand']))
  .agg(n = pl.len())
  .with_columns(
    (pl.col('n') / pl.col('n')
      .sum().over(['texture', 'size'])
    ).alias('prop')
  ).sort('brand')
)

(so.Plot(
    texture_count, 
    x = 'texture', 
    y = 'prop', 
    color = 'brand'
  ).facet('size')
  .add(so.Bar(), so.Stack())
)
```

## Means and histograms

```{python}
#| eval: true
#| output-location: column

pb_data['units'].drop_nans().mean()
```

```{python}
#| eval: true
#| output-location: column

(so.Plot(pb_data, x = 'units')
  .add(so.Bars(), so.Hist(bins = 10))
)
```

::: {.fragment}
Why is **right-skewed continuous data** common? Why might this be a problem with our assumptions?
:::

## Correlations

```{python}
#| eval: true
#| output-location: column

(pb_data
  .select(['price', 'units'])
  .drop_nans()
  .corr()
)
```

## Scatterplots

```{python}
#| eval: true
#| output-location: column

(so.Plot(pb_data, x = 'price', y = 'units')
    .add(
      so.Dot(alpha = 0.25), 
      so.Jitter(x = 0.25, y = 0.25)
    )
    .add(so.Line(), so.PolyFit(order = 1))
)
```

## Correlation matrices

```{python}
#| eval: true

(pb_data
  .select(['price', 'units', 'sales'])
  .drop_nans()
  .corr()
)
```

## Scatterplots matrices

```{python}
#| eval: true
#| output-location: column

sns.pairplot(
  pb_data.select(['price', 'units', 'sales'])
  .to_pandas()
)
```

## Grouped summaries

```{python}
#| eval: true

(pb_data
  .group_by(pl.col(['texture', 'size']))
  .agg(
    n = pl.len(), 
    avg_units = pl.col('units').drop_nans().mean(), 
    avg_sales = pl.col('sales').drop_nans().mean()
  )
  .sort(pl.col('avg_units'), descending=True)
)
```

## Boxplots

```{python}
#| eval: true
#| output-location: column

sns.catplot(
  pb_data, 
  x = 'units', 
  y = 'brand', 
  kind = 'box'
)
```

## Density plots

```{python}
#| eval: true
#| output-location: column

(so.Plot(pb_data, x = 'units', color = 'brand')
  .add(so.Area(), so.Hist(bins=10))
)
```

## Density plots and facets

```{python}
#| eval: true
#| output-location: column

(so.Plot(pb_data, x = 'units', color = 'brand')
  .facet(col = 'size', row = 'texture')
  .add(so.Area(), so.Hist(bins=10))
)
```

# Visualize a different variable or set of variables, using some of this example code or your own. What do you discover? {background-color="#006242"}

# Validity and Representativeness

## Validity

Data is relevant to the objective and no predictors are missing

::: {.fragment}
Why should we care about **omitted variables**? What about including variables that don't matter?
:::

:::: {.columns}

::: {.fragment .column width="50%"}
**Diagnostics**

::: {.incremental}
- Is the data relevant to the objective?
- How well does the data match the ideal data?
- How do the simulated and real outcomes compare?
:::
:::

::: {.fragment .column width="50%"}
**Fixes**

::: {.incremental}
- Get different data
- Find additional data
- Justify proxy variables
:::
:::

::::

## Representativeness

Data is representative of the data generating process or population

::: {.fragment}
Why should we care about the data being representative? What's the difference between **underfitting** and **overfitting**?
:::

:::: {.columns}

::: {.fragment .column width="50%"}
**Diagnostics**

::: {.incremental}
- How was the data collected?
- Is it representiave of the process or population we care about?
- Is data **missing at random**?
- Check for **influential points**
:::
:::

::: {.fragment .column width="50%"}
**Fixes**

::: {.incremental}
- Get different data
- **Drop or impute** missing data
- Investigate influential points and **consider** removing
:::
:::

::::

## Outliers vs. leverage points

**Outliers** and **leverage points** are both *potential* influential points, data that unduly influences the linear regression

::: {.incremental}
- An outlier is a point whose response does not follow the general trend of the rest of the data (i.e., extreme $y$ value)
- A leverage point is a point with a predictor value that is outside the norm (extreme $X$ value)
:::

::: {.fragment}
![](../../figures/assumption_representative-01.png){fig-align="center"}
:::

## Outliers vs. leverage points {visibility="uncounted"}

**Outliers** and **leverage points** are both *potential* influential points, data that unduly influences the linear regression

- An outlier is a point whose response does not follow the general trend of the rest of the data (i.e., extreme $y$ value)
- A leverage point is a point with a predictor value that is outside the norm (extreme $X$ value)

![](../../figures/assumption_representative-02.png){fig-align="center"}

## Check for influential points

To diagnose influential points, we can use visualizations or we can fit a model and use either **DFBETAS** or **DFFITS** to determine how "df" or "different" estimates or the overall model fit would be if a given observation were removed, respectively

```{python}
#| eval: true
#| echo: false

pb_train = (pb_data
  .drop_nans()
  .to_dummies(
      columns = ['brand', 'promo', 'texture', 'size'], drop_first = True
  ).with_columns(
      (pl.col('units') + 1).log().alias('log_units'),
      (pl.col('price') + 1).log().alias('log_price')
  ).drop(['units', 'sales', 'price', 'loyal'])
  .to_pandas()
)
```

```{python}
#| eval: true

import numpy as np
import statsmodels.api as sm
import statsmodels.formula.api as smf

# Fit a model
fit_01 = smf.ols(
  'log_units ~ log_price + brand_Jif + brand_Skippy + brand_PeterPan + promo_true + texture_Smooth + size_12', 
  data = pb_train
).fit()

# Save the dffits values
pb_train['dffits'] = fit_01.get_influence().dffits[0]
```

## DFFITS

How different would the fitted value $\hat{y}_i$ be if observation $i$ wasn't used?

- $|\text{DFFITS}_i| \gt 1$ for $n \leq 30$
- $|\text{DFFITS}_i| \gt 2\sqrt{p/n}$ for $n \gt 30$

where $p$ represents the number of $\beta$s in the model (including the intercept)

```{python}
#| eval: true
#| output-location: column

fig = plt.figure(figsize = (4, 4))
plt.ylabel("DFFITS (Absolute Values)")
plt.xlabel("Observation Number")
plt.scatter(
  pb_train.index, 
  np.abs(pb_train['dffits']), 
  s = 3
)
plt.axhline(
  y = 2 * np.sqrt(len(fit_01.params) / len(pb_train)), 
  color = 'r', 
  linestyle = 'dashed'
)
plt.show()
```

## Investigate influential points

Just because a point may be influential **doesn't mean it should be removed**, it means we should investigate it and decide if its representative

```{python}
#| eval: true

outliers_dffits = pb_train[np.abs(pb_train['dffits']) > 2 * np.sqrt(len(fit_01.params) / len(pb_train))]
outliers_dffits_sorted = outliers_dffits.sort_values(by = 'dffits', ascending = False)
outliers_dffits_sorted.head()
```

# Why are both validity and representativeness the most essential of the linear regression assumptions? {background-color="#006242"}

# Additivity and Linearity

## 

:::: {.columns .v-center}

::: {.column width="60%"}
$$
y_i = \beta_0 + \beta_1 x_{1,i} + \cdots + \beta_p x_{p,i} + \epsilon_i \\
\text{where } \epsilon_i \sim \text{Normal}(0, \sigma^2)
$$

$$
y_i \sim \text{Normal}(\mu_i, \sigma^2) \\
\mu_i = \beta_0 + \beta_1 x_{1,i} + \cdots + \beta_p x_{p,i}
$$
:::

::: {.column width="40%"}
The mapping function from the predictors to the outcome is additive and a linear function **of the parameters**
:::

::::

## Functional form of the deterministic component

The mapping function from the predictors to the outcome is additive and a linear function of the parameters

::: {.fragment}
If the process or population isn't approximated by an additive and linear function, what happens to our inferences?
:::

:::: {.columns}

::: {.fragment .column width="50%"}
**Diagnostics**

::: {.incremental}
- Scatterplot of outcome and predictor(s)
- Scatterplot of residuals and predictor(s)
- Scatterplot of residuals and fitted values
- Partial regression plots
:::
:::

::: {.fragment .column width="50%"}
**Fixes**

::: {.incremental}
- Transform $X$
- Transform $y$
- Use a nonlinear model
:::
:::

::::

## Scatterplot of outcome and predictor(s)

The relationship between the outcome and predictor(s) should be roughly linear or cloud-like (i.e., no obvious non-linear patterns)

```{python}
#| eval: true
#| output-location: column

# Save the residuals and fitted values
pb_train['residuals'] = fit_01.resid
pb_train['fittedvalues'] = fit_01.fittedvalues

(so.Plot(pb_data, x = 'price', y = 'units')
    .add(so.Dot(alpha = 0.25))
    .add(so.Line(), so.PolyFit())
)
```

## Scatterplot of outcome and predictor(s)

The relationship between the outcome and predictor(s) should be roughly linear or cloud-like (i.e., no obvious non-linear patterns)

```{python}
#| eval: true
#| output-location: column

(so.Plot(pb_train, x = 'log_price', y = 'log_units')
    .add(so.Dot(alpha = 0.25))
    .add(so.Line(), so.PolyFit())
)
```

## Scatterplot of residuals and predictor(s)

The relationship between the residuals and predictor(s) should also be roughly linear or cloud-like (i.e., no obvious non-linear patterns)

```{python}
#| eval: true
#| output-location: column

(so.Plot(pb_train, x = 'residuals', y = 'log_units')
    .add(so.Dot(alpha = 0.25))
    .add(so.Line(), so.PolyFit())
)
```

## Scatterplot of residuals and fitted values

There shouldn't be a relationship between the residuals and fitted values, where a fitted line would be roughly horizontal

```{python}
#| eval: true
#| output-location: column

(so.Plot(pb_train, x = 'residuals', y = 'fittedvalues')
    .add(so.Dot(alpha = 0.25))
    .add(so.Line(), so.PolyFit())
)
```

## Partial regression plots

A partial regression plot for a specific predictor has a slope that is the same as the multiple regression coefficient for that predictor

::: {.fragment}
How is that different from a scatterplot of the outcome and the same predictor?
:::

::: {.incremental}
- Partial regression plots attempt to show the effect of adding an additional variable to the model (given that one or more predictors are already in the model)
- For a specific predictor, we take whatever is leftover after the fit of the other variables and look to see if what remains is correlated with the part of the predictor that is not already accounted for by the other variables
:::

## Partial regression plots

The relationship in the partial regression plots should be roughly linear or cloud-like (i.e., no obvious non-linear patterns)

```{python}
#| eval: true
#| output-location: column

predictors = [
  'log_price', 'brand_Jif', 'brand_Skippy', 
  'brand_PeterPan', 'promo_true', 'texture_Smooth', 
  'size_12'
]

fig = plt.figure(figsize = (7,7))
sm.graphics.plot_partregress_grid(
  fit_01, 
  exog_idx = list(predictors), 
  grid = (4, 2), 
  fig = fig
)
fig.tight_layout()
plt.show()
```

## {background-color="#006242"}

### Exercise 06 {.lato-font}

1. Clean up the soft launch data
2. Create two interesting visualizations that help you understand the data and its limitations
3. Walk through the validity, representativeness, and additivity/linearity assumptions and justify whether or not they are satisfied for your cleaned data
4. Submit your code, output, and explanations as a single PDF on Canvas

## {background-color="#288DC2"}

### Wrap Up {.permanent-marker-font}

#### Summary

- Started working with real data
- Conducted exploratory data analysis
- Began reconciling the data and the model

#### Next Time

- Finish assumption diagnostics and remedies

