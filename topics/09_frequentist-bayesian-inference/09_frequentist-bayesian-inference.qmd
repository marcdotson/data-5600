---
title: "Frequentist and <br>Bayesian Inference"
subtitle: "DATA 5600 Introduction to Regression and Machine Learning for Analytics"
author: Marc Dotson
title-slide-attributes:
  data-background-color: "#0F2439"
format: 
  revealjs:
    theme: ../slides.scss  # Modified slides theme
    code-copy: true        # Show code blocks copy button
    slide-number: c/t      # Numbered slides current/total
    embed-resources: true  # Render to a single HTML file
execute:
  eval: false
  echo: true
jupyter: python3
---

## {background-color="#288DC2"}

### Get Started {.permanent-marker-font}

#### Last Time

- Completed feature engineering
- Began detailing model estimation

#### Preview

- Compare frequentist and Bayesian inference

## {background-color="#D1CBBD"}

::: {.columns .v-center}
![](../../figures/workflow_fit-evaluate.png){fig-align="center"}
:::

# Estimating Model Parameters

## Data cleaning and feature engineering {.scrollable}

Before we return to fitting models, let's start with data cleaning and feature engineering completed

```{python}
#| eval: true
#| echo: false

import os
import polars as pl
from sklearn.model_selection import train_test_split
import statsmodels.formula.api as smf
import bambi as bmb
import arviz as az

sl_data = pl.read_parquet(os.path.join('..', '..', 'data', 'soft_launch.parquet'))

# Standardize brand names
brand_names = {
    # Jif
    'JIF': 'Jif', 'Jiff': 'Jif',
    # Skippy
    'SKIPPY': 'Skippy', 'Skipy': 'Skippy', 'Skipp': 'Skippy',
    # PeterPan
    'Peter Pan': 'PeterPan', 'Peter Pa': 'PeterPan',
    # Harmons
    "Harmon's": 'Harmons',
}

pb_data = (sl_data
    # Fix brand naming variants
    .with_columns([
        pl.col('brand').cast(pl.Utf8).replace(brand_names).alias('brand')
    ])
    # Drop rows with missing values
    .drop_nans(['units', 'sales'])
    # Drop rows with "None" values
    .remove(
        (pl.col('brand') == "None") | (pl.col('loyal') == "None") | 
        (pl.col('texture') == "None") | (pl.col('size') == "None")
    )
    # Make price numeric
    .with_columns([
        pl.col('price').cast(pl.Float64).alias('price'),
        pl.col('loyal').cast(pl.Int64).alias('loyal'),
        pl.col('promo').cast(pl.Int64).alias('promo')
    ])
    # Drop duplicate
    .unique()
    # Filter for only peanut butter purchases
    .filter(pl.col('units') > 0)
    # Select relevant columns
    .select('units', 'brand', 'coupon', 'ad', 'texture', 'size', 'price')
)

# Specify the design matrix and outcome
X = pb_data.select(pl.exclude('units'))
y = pb_data.select('units')

# Split into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)

# Transform units
y_train = (y_train
    .with_columns((pl.col('units') + 1).log().alias('log_units'))
    .select('log_units')
)

# Transform price and dummy code discrete predictors
X_train = (X_train
    .with_columns((pl.col('price') + 1).log().alias('log_price'))
    .to_dummies(columns = ['brand', 'texture', 'size'], drop_first = False)
    .select(pl.exclude('price','brand_Jif', 'texture_Smooth', 'size_16'))
)

# Add constant for intercept
pb_train = (pl.concat([y_train, X_train], how = 'horizontal')
    .to_pandas()
)
```

```{python}
import os
import polars as pl
from sklearn.model_selection import train_test_split
import statsmodels.formula.api as smf
import bambi as bmb
import arviz as az

sl_data = pl.read_parquet(os.path.join('data', 'soft_launch.parquet'))

# Standardize brand names
brand_names = {
    # Jif
    'JIF': 'Jif', 'Jiff': 'Jif',
    # Skippy
    'SKIPPY': 'Skippy', 'Skipy': 'Skippy', 'Skipp': 'Skippy',
    # PeterPan
    'Peter Pan': 'PeterPan', 'Peter Pa': 'PeterPan',
    # Harmons
    "Harmon's": 'Harmons',
}

pb_data = (sl_data
    # Fix brand naming variants
    .with_columns([
        pl.col('brand').cast(pl.Utf8).replace(brand_names).alias('brand')
    ])
    # Drop rows with missing values
    .drop_nans(['units', 'sales'])
    # Drop rows with "None" values
    .remove(
        (pl.col('brand') == "None") | (pl.col('loyal') == "None") | 
        (pl.col('texture') == "None") | (pl.col('size') == "None")
    )
    # Make price numeric
    .with_columns([
        pl.col('price').cast(pl.Float64).alias('price'),
        pl.col('loyal').cast(pl.Int64).alias('loyal'),
        pl.col('promo').cast(pl.Int64).alias('promo')
    ])
    # Drop duplicate
    .unique()
    # Filter for only peanut butter purchases
    .filter(pl.col('units') > 0)
    # Select relevant columns
    .select('units', 'brand', 'coupon', 'ad', 'texture', 'size', 'price')
)

# Specify the design matrix and outcome
X = pb_data.select(pl.exclude('units'))
y = pb_data.select('units')

# Split into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)

# Transform units
y_train = (y_train
    .with_columns((pl.col('units') + 1).log().alias('log_units'))
    .select('log_units')
)

# Transform price and dummy code discrete predictors
X_train = (X_train
    .with_columns((pl.col('price') + 1).log().alias('log_price'))
    .to_dummies(columns = ['brand', 'texture', 'size'], drop_first = False)
    .select(pl.exclude('price','brand_Jif', 'texture_Smooth', 'size_16'))
)

# Add constant for intercept
pb_train = (pl.concat([y_train, X_train], how = 'horizontal')
    .to_pandas()
)
```

## Ordinary least squares {auto-animate=true}

![](../../figures/lm_fit_04.png){fig-align="center"}

## Ordinary least squares {auto-animate=true visibility="uncounted"}

:::: {.columns}

::: {.column width="40%"}
![](../../figures/lm_fit_04.png){fig-align="center"}
:::

::: {.column width="60%"}
::: {.incremental}
- We find the fitted line (and estimate parameters) $\hat{y}_i = \hat{\beta}_0 + \hat{\beta}_1 x_{1,i} + \cdots + \hat{\beta}_p x_{p,i}$ by minimizing the **sum of squared residuals**
- Ordinary least squares (OLS) is a special case of **maximum likelihood estimation** (MLE)
- In other words, the parameter estimates that **minimize** the sum of squared residuals are the same as those that **maximize** the likelihood function
- OLS and MLE are frequentist methods
:::
:::

::::

## What about $\sigma^2$?

The variance parameter $\sigma^2$ can also be estimated using the residuals, specifically the **average squared variance of the residuals** around the fitted line

$$
\LARGE{\hat{\sigma^2} = \frac{\sum_{i=1}^n (\hat{y}_i - y_i)^2}{n - p}}
$$

where $n$ is the number of observations and $p$ is the number of slope parameters in the model; this estimate is often referred to as $s^2$ or **mean squared error**

## Minimizing quadratic loss

Choosing $\hat{\beta}_0, \hat{\beta}_1 \cdots \hat{\beta}_p, \text{ and } \hat{\sigma^2}$ to minimize the sum of squared residuals is equivalent to minimizing **quadratic loss** or **squared error loss** (a.k.a., $\ell_2$ loss)

$$
\LARGE{\ell(a, \theta) = \sum_{i=1}^n (a_i - \theta_i)^2}
$$

Fitting models and estimating parameters is a decision theory problem where the actions are the **possible values of the parameter estimates** in the fitted model $\hat{y}_i = \hat{\beta}_0 + \hat{\beta}_1 x_{1,i} + \cdots + \hat{\beta}_p x_{p,i}$ and $\hat{\sigma^2}$

## Minimizing quadratic loss {visibility="uncounted"}

Choosing $\hat{\beta}_0, \hat{\beta}_1 \cdots \hat{\beta}_p, \text{ and } \hat{\sigma^2}$ to minimize the sum of squared residuals is equivalent to minimizing **quadratic loss** or **squared error loss** (a.k.a., $\ell_2$ loss)

$$
\LARGE{\ell(\hat{y}, y) = \sum_{i=1}^n (\hat{y}_i - y_i)^2}
$$

Fitting models and estimating parameters is a decision theory problem where the actions are the **possible values of the parameter estimates** in the fitted model $\hat{y}_i = \hat{\beta}_0 + \hat{\beta}_1 x_{1,i} + \cdots + \hat{\beta}_p x_{p,i}$ and $\hat{\sigma^2}$

# Why do we care about model parameters? What is missing from our estimates so far? {background-color="#006242"}

# OLS $\approx$ Uniform Priors

## Bayesian models

Both frequentist and Bayesian models use **likelihood functions**, so how does a Bayesian model differ?

:::: {.columns}

::: {.column width="50%"}
$$
\mu_i = \beta_0 + \beta_1 x_{1,i} + \cdots + \beta_p x_{p,i} \\
y_i \sim \text{Normal}(\mu_i, \sigma^2)
$$
:::

::: {.column width="50%"}
::: {.incremental}
- Parameters (and **any unknown** variables) are **random variables**
- The likelihood is combined with a **prior** $p(\theta)$ to produce a **posterior** $p(\theta | X)$
- We get draws from the posterior using **Markov chain Monte Carlo** (MCMC)
:::
:::

::::

## Bayesian models {visibility="uncounted"}

Both frequentist and Bayesian models use **likelihood functions**, so how does a Bayesian model differ?

:::: {.columns}

::: {.column width="50%"}
$$
\mu_i = \beta_0 + \beta_1 x_{1,i} + \cdots + \beta_p x_{p,i} \\
y_i \sim \text{Normal}(\mu_i, \sigma^2) \\
\beta \sim \text{Uniform}(-\infty, \infty) \\
\sigma^2 \sim \text{Exponential}(1)
$$
:::

::: {.column width="50%"}
- Parameters (and **any unknown** variables) are **random variables**
- The likelihood is combined with a **prior** $p(\theta)$ to produce a **posterior** $p(\theta | X)$
- We get draws from the posterior using **Markov chain Monte Carlo** (MCMC)
:::

::::

## Priors are part of the model

If we don't specify priors, most **probabilistic programming languages** (PPLs) will use default priors

```{python}
#| eval: true
#| output-location: slide
#| code-line-numbers: "|1-5|7-11|9|13"

# Specify predictors
predictors = [
    'brand_Harmons', 'brand_PeterPan', 'brand_Skippy',
    'coupon', 'ad', 'texture_Chunky', 'size_12', 'log_price'
]

# Specify a Bayesian linear regression
ba_model = bmb.Model(
    'log_units ~ ' + ' + '.join(predictors), 
    data = pb_train
)

ba_model
```

## Priors include information beyond the data

What information could we have about the parameters outside of what's included in the data?

```{python}
#| eval: true
#| output-location: slide

# Specify priors
priors_dict = {
    'Intercept': bmb.Prior("Uniform", lower=-100, upper=100),
    **{term: bmb.Prior("Uniform", lower=-100, upper=100) for term in predictors},
    'sigma': bmb.Prior("Exponential", lam=1)
}

ba_model.set_priors(priors = priors_dict)
ba_model
```

::: {.fragment}
What do uniform priors imply?
:::

## Frequentist models as an approximation

OLS can be seen as a simple **point estimate approximation** to a Bayesian posterior with uniform priors

```{python}
#| eval: true

# Fit a frequentist linear regression
fr_fit = smf.ols(
    'log_units ~ ' + ' + '.join(predictors), 
    data = pb_train
).fit()

# Fit a Bayesian linear regression
ba_fit = ba_model.fit()
```

## Frequentist models as an approximation {.scrollable}

```{python}
#| eval: true

fr_fit.summary()
```

## Frequentist models as an approximation {.scrollable}

```{python}
#| eval: true

az.summary(ba_fit)
```

## Bayesian updating is *learning*

Fitting a Bayesian model is often referred to as **Bayesian updating**, why?

$$
\LARGE{p(\theta | X) \propto p(X | \theta) \ p(\theta)}
$$

::: {.incremental}
1. Start with a prior distribution over the possible parameter values
2. **Update that prior distribution** using data and the likelihod function
3. Which produces a posterior distribution over the possible parameter values
:::

## Bayesian updating is *learning*

The posterior distribution is a weighted combination of the likelihood and prior distribution: the **more informative** the likelihood or prior, the **more influence** it has on the posterior distribution

![](../../figures/plot_bayesian-updating.png){fig-align="center"}

::: {.fragment}
The influence of the prior on the resulting posterior distribution is called **shrinkage** or **regularization** and is the basis for **penalized regression**
:::

# When might prior information be especially important in an analysis? {background-color="#006242"}

# Parameter Estimates

## Points, intervals, and distributions

There are three types of parameters estimates:

::: {.incremental}
1. **Point estimates**: A **single number** that represents our best guess of the parameter
2. **Interval estimates**: A **range of numbers** that represent our best guess of the parameter
3. **Distributions**: An entire **probability distribution** representing our best guess over the support of all parameter values simultaneously in a **joint distribution** or of a given parameter individually in a **marginal distribution**
:::

## Points, intervals, and distributions {visibility="uncounted"}

![](../../figures/plot_log-price-estimate.png){fig-align="center"}

## Posterior vs. sampling distributions

Where does the variation come from in our parameter estimates?

:::: {.columns}

::: {.fragment .column width="50%"}
**Bayesian**

::: {.fragment}
$$
X = \text{data we observe} \\
\theta \sim \text{a probability distribution}
$$
:::

::: {.incremental}
- Start with what we know about the parameters and use the data we observe to **update what we know** about the parameters
- **Directly use probability** to describe uncertainty when working with incomplete knowledge
:::
:::

::: {.fragment .column width="50%"}
**Frequentist**

::: {.fragment}
$$
X \sim \text{a probability distribution} \\
\theta = \text{a value we don't observe}
$$
:::

::: {.incremental}
- Consider what point estimates we would expect to get if we had many hypothetical datasets so that our interval estimate is based on the **asymptotic properties of the procedure** used to produce our point estimate
- **Indirectly use probability** to describe uncertainty as a repeatable event under identical conditions
:::
:::

::::

## Credible intervals vs. confidence intervals 

Even when frequentist and Bayesian models produce similar results, the **interpretation** of the results is different

::: {.fragment}
$$
\LARGE{(10.23, 15.67)}
$$

This is the 95% interval estimate for Harmon's (with Jif as the reference level) for a multiple regression with sales as the outcome
:::

:::: {.columns}

::: {.fragment .column width="50%"}
**Bayesian Credible Intervals**

With **probability of 0.95**, Harmon's peanut butter increases sales between $10.23 and $15.67, compared to Jif, holding all other variables fixed
:::

::: {.fragment .column width="50%"}
**Frequentist Confidence Intervals**

Based on OLS/MLE interval estimates, **95% of such intervals** will contain the true value of the parameter such that we are **95% confident** Harmon's peanut butter increases sales between $10.23 and $15.67, compared to Jif, holding all other variables fixed
:::

::::

## Credible intervals vs. confidence intervals{visibility="uncounted"}

Even when frequentist and Bayesian models produce similar results, the **interpretation** of the results is different

$$
\LARGE{(10.23, 15.67)}
$$

This is the 95% interval estimate for Harmon's (with Jif as the reference level) for a multiple regression with sales as the outcome

:::: {.columns}

::: {.column width="50%"}
**Bayesian Credible Intervals**

With **probability of 0.95**, Harmon's peanut butter increases sales between $10.23 and $15.67, compared to Jif, holding all other variables fixed
:::

::: {.column width="50%"}
**Frequentist Confidence Intervals**

~~Based on OLS/MLE interval estimates, **95% of such intervals** will contain the true value of the parameter such that~~ we are **95% confident** Harmon's peanut butter increases sales between $10.23 and $15.67, compared to Jif, holding all other variables fixed
:::

::::

## Significance, confidence intervals, and p-values

If the interval estimate **doesn't include zero**, we can say that the parameter estimate is **statistically significant** (a.k.a., significant or significantly different from zero)

::: {.incremental}
- Just because a parameter estimate isn't statistically significant **doesn't mean it shouldn't be included** in the model
- If a parameter estimate is statistically significance **doesn't mean that it's practically significant**
:::

::: {.fragment}
A $p$-value is the **probability of observing something as or more extreme**, assuming the null hypothesis is true, such that a small $p$-value (usually smaller than $0.05$) indicates evidence against the null hypothesis (i.e., that the parameter is zero or has no effect)
:::

## 

::: {.v-center}
![](../../figures/meme_pvalues.png){fig-align="center"}
:::

## {background-color="#006242"}

### Exercise 09 {.lato-font}

1. Return to your data from the previous exercise (or start with the exercise solution)
2. Fit the model again using OLS and a Bayesian model with uniform priors (if your computer can)
3. Interpret the interval estimates, being careful to track transformed scales, reference levels, the presence of multiple predictors in the model, and the differences between credible and confidence intervals
4. Submit your code, output, and interpretations as a single PDF on Canvas

## {background-color="#288DC2"}

### Wrap Up {.permanent-marker-font}

#### Summary

- Compared frequentist and Bayesian inference

#### Next Time

- Detail overall model fit and prediction

