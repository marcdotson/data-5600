---
title: "Multilevel Models"
subtitle: "DATA 5600 Introduction to Regression and Machine Learning for Analytics"
author: Marc Dotson
title-slide-attributes:
  data-background-color: "#0F2439"
format: 
  revealjs:
    theme: ../slides.scss  # Modified slides theme
    code-copy: true        # Show code blocks copy button
    slide-number: c/t      # Numbered slides current/total
    embed-resources: true  # Render to a single HTML file
execute:
  eval: false
  echo: true
jupyter: python3
---

## {background-color="#288DC2"}

### Get Started {.permanent-marker-font}

#### Last Time

- Introduced two-way and higher-order interactions
- Started motivating multilevel models using interactions

#### Preview

- Finish introducing multilevel models
- Summarize decision-making under uncertainty

## {background-color="#D1CBBD"}

::: {.columns .v-center}
![](../../figures/workflow_build.png){fig-align="center"}
:::

# Beyond Interactions

## Conditioning in the model

Interactions allow **predictors to be conditioned on each other** so we can keep our linear model while no longer requiring independent predictors[, but where else have we been relying on **independence instead of conditioning**?]{.fragment}

::: {.fragment}
$$
\large{y_i \sim \text{Distribution}(\theta_i)} \\
\large{f(\theta_i) = \beta_0 + \beta_1 x_{1,i} + \beta_2 x_{2,i} + \beta_3 x_{1,i} x_{2,i}}
$$
:::

::: {.fragment}
We assume that observations are independent of each other, but are they really? [How might we include **conditioning for our parameters**?]{.fragment}
:::

## Multilevel logistic regression

$$
\large{y_i \sim \text{Binomial}(1, p_i)} \\
\large{\log\left({p_i \over 1 - p_i}\right) = \beta_0 + \beta_1 x_{1,i} + \cdots + \beta_p x_{p,i}}
$$

## Multilevel logistic regression {visibility="uncounted"}

$$
\large{y_i \sim \text{Binomial}(1, p_i)} \\
\large{\log\left({p_i \over 1 - p_i}\right) = \beta_{0,h} + \beta_{1,h} x_{1,i} + \cdots + \beta_{p,h} x_{p,i}}
$$

::: {.incremental}
- The $h$ index references the **group** that observation $i$ belongs to
- We can condition the intercept $\beta_{0,h}$, slopes $\beta_{j,h}$, or both by group
- This allows us to capture **heterogeneous effects** across groups
- Estimating Bayesian multilevel models not only includes regularization, it includes **adaptive regularization** via partial pooling
:::

## 

::: {.v-center}
![](../../figures/meme_multilevel-models.png){fig-align="center"}
:::

# What two-way interactions might be meaningful to include for your project? {background-color="#006242"}


# Does including interactions make regularization, variable selection, and dimensionality reduction more or less relevant? {background-color="#006242"}



## {background-color="#006242"}

### Exercise 23 {.lato-font}

1. Choose a concept from the course to summarize
2. [Create a meme](https://imgflip.com/memegenerator) to illustrate the concept
3. Submit your meme as a PNG on Canvas

## {background-color="#288DC2"}

### Wrap Up {.permanent-marker-font}

#### Summary

- Finished introducing multilevel models
- Summarized decision-making under uncertainty

#### Next Time

- Project presentations

