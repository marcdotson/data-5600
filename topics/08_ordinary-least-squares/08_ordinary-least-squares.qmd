---
title: "Ordinary Least Squares"
subtitle: "DATA 5600 Introduction to Regression and Machine Learning for Analytics"
author: Marc Dotson
title-slide-attributes:
  data-background-color: "#0F2439"
format: 
  revealjs:
    theme: ../slides.scss  # Modified slides theme
    code-copy: true        # Show code blocks copy button
    slide-number: c/t      # Numbered slides current/total
    embed-resources: true  # Render to a single HTML file
execute:
  eval: false
  echo: true
  cache: false
jupyter: python3
---

## {background-color="#288DC2"}

### Get Started {.permanent-marker-font}

#### Last Time

- Finished assumption diagnostics and remedies

#### Preview

- Complete feature engineering
- Begin detailing model estimation

## {background-color="#D1CBBD"}

::: {.columns .v-center}
![](../../figures/workflow_reconcile-fit.png){fig-align="center"}
:::

## 

::: {.columns .v-center}
![](../../figures/underfit-overfit.png){fig-align="center"}
:::

## Extract information about the *regular* features

The information we want to extract is about the data generating process or population while the data we observe is **just one realization** or sample of that process or population

::: {.incremental}
- If we learn too little from the data, we are **underfitting**
- If we learn too much from the data, we are **overfitting**
- We want to find a **balance** between underfitting and overfitting so we can **generalize** what we interpret from the model
:::

::: {.fragment}
All models as they increase in complexity will **naturally tend toward overfitting**, so what can we do?
:::

# Feature Engineering

## Caring for and feeding models

Feature engineering (a.k.a., **preprocessing data**) is data cleaning or wrangling for the benefit of the model and is critical to navigating between underfitting and overfitting

::: {.incremental}
- Splitting the data into training and testing sets
- Transformations to address assumption violations
- Dummy coding discrete predictors
:::

::: {.fragment}
Neural networks are a black-box of **feature engineering automation** to improve predictive fit
:::

## Training and testing split {.scrollable}

Before we do any feature engineering, we need to split the data into **training** data to fit the model and **testing** data to evaluate the model's predictive fit

```{python}
#| eval: true

import os
import polars as pl
import seaborn.objects as so
import statsmodels.api as sm
from sklearn.model_selection import train_test_split

sl_data = pl.read_parquet(os.path.join('data', 'soft_launch.parquet'))

# Standardize brand names
brand_names = {
    # Jif
    'JIF': 'Jif', 'Jiff': 'Jif',
    # Skippy
    'SKIPPY': 'Skippy', 'Skipy': 'Skippy', 'Skipp': 'Skippy',
    # PeterPan
    'Peter Pan': 'PeterPan', 'Peter Pa': 'PeterPan',
    # Harmons
    "Harmon's": 'Harmons',
}

# Clean the data
pb_data = (sl_data
    # Fix brand naming variants
    .with_columns([
        pl.col('brand').cast(pl.Utf8).replace(brand_names).alias('brand')
    ])
    # Drop rows with missing values
    .drop_nans(['units'])
    .remove(
        (pl.col('brand') == "None") | 
        (pl.col('texture') == "None") | 
        (pl.col('size') == "None")
    )
    # Make price numeric
    .with_columns(pl.col('price').cast(pl.Float64).alias('price'))
    # Drop duplicate rows
    .unique()
    # Filter for only peanut butter purchases
    .filter(pl.col('units') > 0)
    # Select relevant columns
    .select('units', 'brand', 'coupon', 'ad', 'texture', 'size', 'price')
)
```

## Training and testing split

How might **using testing data to evaluate model fit** help us avoid overfitting?

```{python}
#| eval: true

# Specify the design matrix and outcome
X = pb_data.select(pl.exclude('units'))
y = pb_data.select('units')

# Split into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)
```

## Transforming continuous $y$ and $X$

Having **approximately normal continuous data** will help satisfy some of the linear regression assumptions: linearity, constant variance, and normality

:::: {.columns}

::: {.column width="50%" .fragment}
**Right-Skewed Continuous Data**

::: {.incremental}
- Log transformation
- Square root transformation
:::
:::

::: {.column width="50%" .fragment}
**Left-Skewed Continuous Data**

::: {.incremental}
- Square transformation
:::
:::

::::

::: {.fragment}
Feature engineering for interpretable models isn't just about satisfying assumptions, it's also about making sure the model is **still easy to interpret**
:::

## Transforming continuous $y$ and $X$

When performing a log transformation, remember to include an **offset** to avoid taking the log of zero

```{python}
#| eval: true
#| output-location: column

y_train = (y_train
  .with_columns(
    (pl.col('units') + 1).log().alias('log_units')
  )
  .select('log_units')
  .to_pandas()
)

(so.Plot(y_train, x = 'log_units')
  .add(so.Bars(), so.Hist(bins = 10))
)
```

## Different scales, same model

Transforming $y$ and $X$ shows that linear models aren't necessarily **lines** and we can always **back-transform** using the **inverse function** to get back to the original scale

![](../../figures/transformations.png){fig-align="center"}

## Interpreting models with transformed $y$ and $X$

Interpreting models with transformed variables is all about **keeping track of the scales**, so how does the log transformation change how we interpret parameters estimates?

::: {.incremental}
- $y$ and $x_1$: A one unit increase in $x_1$ leads to a $\beta_1$ unit change in $y$, holding all other variables fixed
- $log(y)$ and $x_1$: A one unit increase in $x_1$ leads to a $(exp(\beta_1) - 1) \times 100\%$ change in $y$, holding all other variables fixed
- $y$ and $log(x_1)$: A 1% increase in $x_1$ leads to a $\beta_1/100$ unit change in $y$, holding all other variables fixed
- $log(y)$ and $log(x_1)$: A 1% increase in $x_1$ leads to a $\beta_1\%$ change in $y$, holding all other variables fixed
:::

## Dummy coding {.scrollable}

One of the most common feature engineering steps is **dummy coding** (a.k.a., indicator coding or one-hot encoding) where the **levels** of a discrete predictor are turned into multiple binary predictors

```{python}
#| eval: true

X_train = (X_train
    .to_dummies(columns = ['brand', 'texture', 'size'], drop_first = False)
    .select(pl.exclude('brand_Jif', 'texture_Smooth', 'size_16'))
    .to_pandas()
)
```

::: {.fragment}
Why can't we include all of the **levels** of a discrete predictor in a linear regression (i.e., the **dummy variable trap**)?
:::

## Interpreting models with discrete $X$

Interpreting models with discrete predictors is all about **keeping track of the baseline/reference level**

::: {.incremental}
- If $x_1$ is discrete, the associated slope parameter $\beta_1$ represents the expected amount by which $y$ will change, **relative to the baseline/reference level** of $x_1$, holding all other variables fixed
:::

::: {.fragment}
Why would you want some control over which level is the baseline/reference level?
:::

# Why do we need to reconcile the differences between the model and the data? {background-color="#006242"}

# Ordinary Least Squares

## Estimating model parameters

$$
\large{y_i = \beta_0 + \beta_1 x_{1,i} + \cdots + \beta_p x_{p,i} + \epsilon_i} \\
\large{\text{where } \epsilon_i \sim \text{Normal}(0, \sigma^2)}
$$

$$
\large{y_i \sim \text{Normal}(\mu_i, \sigma^2)} \\
\large{\mu_i = \beta_0 + \beta_1 x_{1,i} + \cdots + \beta_p x_{p,i}}
$$

Fitting (a.k.a., estimating, training, calibrating) a parametric model will give us **estimates** (i.e., guesses) of the model parameters, so what?

## Estimating $\beta_0$ and $\beta_1$

![](../../figures/lm_scatter.png){fig-align="center"}

## Finding the best line

![](../../figures/lm_fit_01.png){fig-align="center"}

## Finding the best line {visibility="uncounted"}

![](../../figures/lm_fit_02.png){fig-align="center"}

## Finding the best line {visibility="uncounted"}

![](../../figures/lm_fit_03.png){fig-align="center"}

## Finding the best line {visibility="uncounted"}

![](../../figures/lm_SSE_01.png){fig-align="center"}

## Minimizing the sum of squared residuals

The best line should be the one that makes **the sum** of the **residuals** (i.e., vertical bars) as **small as possible** such that our model

$$
\large{y_i = \beta_0 + \beta_1 x_{1,i} + \cdots + \beta_p x_{p,i}}
$$

yields the fitted or expected value $\hat{y}$

$$
\large{\hat{y}_i = \hat{\beta}_0 + \hat{\beta}_1 x_{1,i} + \cdots + \hat{\beta}_p x_{p,i}}
$$

where $\hat{\beta}_0, \hat{\beta}_1 \cdots \hat{\beta}_p$ are the parameter estimates

## Minimizing the sum of squared residuals

![](../../figures/lm_SSE_02.png){fig-align="center"}

## Minimizing the sum of squared residuals {visibility="uncounted"}

![](../../figures/lm_fit_04.png){fig-align="center"}

## Minimizing quadratic loss

Choosing $\hat{\beta}_0, \hat{\beta}_1 \cdots \hat{\beta}_p$ to minimize the sum of squared residuals is equivalent to minimizing **quadratic loss** or **squared error loss** (a.k.a., $\ell_2$ loss)

$$
\LARGE{\ell(a, \theta) = \sum_{i=1}^n (a_i - \theta_i)^2}
$$

## Minimizing quadratic loss {visibility="uncounted"}

Choosing $\hat{\beta}_0, \hat{\beta}_1 \cdots \hat{\beta}_p$ to minimize the sum of squared residuals is equivalent to minimizing **quadratic loss** or **squared error loss** (a.k.a., $\ell_2$ loss)

$$
\LARGE{\ell(\hat{y}, y) = \sum_{i=1}^n (\hat{y}_i - y_i)^2}
$$

::: {.fragment}
Fitting models to get point estimates is a decision theory problem where the actions are the **possible point estimates** in the fitted model $\hat{y}_i = \hat{\beta}_0 + \hat{\beta}_1 x_{1,i} + \cdots + \hat{\beta}_p x_{p,i}$ and we are uncertain about the possible values of the parameter estimates
:::

## Fitting models with ordinary least squares {.scrollable}

Choosing point estimates by minimizing the sum of squared residuals is a frequentist method called **ordinary least squares** (OLS) and is a special case of **maximum likelihood estimation** (MLE)

```{python}
#| eval: true
#| output-location: slide

# Add constant for intercept
X_train = sm.add_constant(X_train)

# Fit the model
fit_04 = sm.OLS(y_train, X_train).fit()
fit_04.summary()
```

# What are the pros and cons of fitting a model using ordinary least squares? {background-color="#006242"}

## Using the likelihood to estimate parameters {auto-animate=true}

![](../../figures/plot_likelihood.png){fig-align="center"}

## Using the likelihood to estimate parameters {auto-animate=true visibility="uncounted"}

:::: {.columns}

::: {.column width="40%"}
![](../../figures/plot_likelihood.png){fig-align="center"}
:::

::: {.column width="60%"}
::: {.fragment}
**Frequentist**

- Find the single value of $\theta$ that maximizes the likelihood function
- This **maximum likelihood estimate** is a point estimate of the fixed but unknown value $\theta$
:::

::: {.fragment}
**Bayesian**

- Use Bayes' theorem to combine the likelihood with a **prior distribution** $p(\theta)$
- This produces a **posterior distribution** $p(\theta | X)$ of the random variable $\theta$
- Instead of computing this distribution analytically, we draw samples from it using **Markov chain Monte Carlo** (MCMC)
:::
:::

::::

## {background-color="#006242"}

### Exercise 08 {.lato-font}

1. Return to your data from the previous exercise (or start with the exercise solution)
2. Split the data into training and testing data, confirm any other feature engineering, and fit a model using OLS
3. Interpret the parameter estimates, being careful to track transformed scales, reference levels, and the presence of multiple predictors in the model
4. Submit your code, output, and interpretations as a single PDF on Canvas

## {background-color="#288DC2"}

### Wrap Up {.permanent-marker-font}

#### Summary

- Completed feature engineering
- Began detailing model estimation

#### Next Time

- Compare frequentist and Bayesian inference

