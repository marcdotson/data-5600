---
title: "Maximum Likelihood Estimation"
subtitle: "DATA 5600 Introduction to Regression and Machine Learning for Analytics"
author: Marc Dotson
title-slide-attributes:
  data-background-color: "#0F2439"
format: 
  revealjs:
    theme: ../slides.scss  # Modified slides theme
    code-copy: true        # Show code blocks copy button
    slide-number: c/t      # Numbered slides current/total
    embed-resources: true  # Render to a single HTML file
execute:
  eval: false
  echo: true
  cache: false
jupyter: python3
---

## {background-color="#288DC2"}

### Get Started {.permanent-marker-font}

#### Last Time

- Interpreted logistic regression coefficients
- Reviewed assumption diagnostics and remedies
- Expanded diagnostics and remedies to GLMs

#### Preview

- Extend model fitting beyond ordinary least squares
- Continue comparing frequentist and Bayesian inference

## {background-color="#D1CBBD"}

::: {.columns .v-center}
![](../../figures/workflow_fit.png){fig-align="center"}
:::

# Ordinary Least Squares

## Using geometry to estimate model parameters {auto-animate=true}

![](../../figures/lm_fit_04.png){fig-align="center"}

## Using geometry to estimate model parameters {auto-animate=true visibility="uncounted"}

:::: {.columns}

::: {.column width="40%"}
![](../../figures/lm_fit_04.png){fig-align="center"}
:::

::: {.column width="60%"}
::: {.incremental}
- We find the fitted line (and estimate parameters) $\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 x_{1} + \cdots + \hat{\beta}_p x_{p}$ by **minimizing the sum of squared residuals**
- **Ordinary least squares** (OLS) has a closed-form solution $\hat{\beta} = (X'X)^{-1}X'y$
- Note that the "line" has **as many dimensions as there are slope parameters** in the model
- OLS is a special case of **maximum likelihood estimation** (MLE), such that the $\hat{\beta}$ that **minimize** the sum of squared residuals are the same that **maximize** the likelihood function
- OLS is a frequentist method used to find **point estimates**
:::
:::

::::

## Minimizing quadratic loss

Choosing the $\hat{\beta}$s to minimize the sum of squared residuals is equivalent to minimizing **quadratic loss** or **squared error loss** (a.k.a., $\ell_2$ loss)

$$
\LARGE{\ell(\hat{y}, y) = \sum_{i=1}^n (\hat{y}_i - y_i)^2}
$$

::: {.fragment}
Point estimation is a type of decision problem

::: {.incremental}
- the actions are the **parameter estimates** in the fitted model $\hat{y}$
- the state we are uncertain about is the **parameters** in the model $y$
:::
:::

# What are the pros and cons of fitting a model using ordinary least squares? {background-color="#006242"}

# Likelihood-Based Methods

## A model is a likelihood function

Our **parametric model** of the data $f(X, Y | \theta)$ is used to determine *likely* values of $\theta$ and is also called a **likelihood function** $\mathcal{L}(\theta | X, Y)$

$$
\LARGE{f(X, Y | \theta) = \mathcal{L}(\theta | X, Y)}
$$

::: {.incremental}
- If you give a likelihood function parameters $\theta$ and $X$, it can produce plausible observations $Y$
- If you give a likelihood function observations $Y$ and $X$, it can determine plausible parameters $\theta$
:::

## Using the likelihood to estimate parameters {auto-animate=true}

![](../../figures/plot_likelihood.png){fig-align="center"}

## Using the likelihood to estimate parameters {auto-animate=true visibility="uncounted"}

:::: {.columns}

::: {.column width="40%"}
![](../../figures/plot_likelihood.png){fig-align="center"}
:::

::: {.column width="60%"}
::: {.incremental}
- We find parameter estimates by **maximizing the likelihood**
- Searching for this estimate involves **optimization** methods like **gradient ascent/descent** to explore and find the maximum/minimum
- Note that the likelihood has **as many dimensions as there are parameters** in the model
- MLE is a frequentist method used to find **point estimates**
:::
:::

::::

## Minimizing the negative log-likelihood

Choosing the $\hat{\theta}$s that maximize the likelihood is equivalent to choosing the $\hat{\theta}$s that **minimize the negative log-likelihood**

$$
\LARGE{\ell(\hat{\theta}, \theta) = \underset{\theta}{\text{argmin}} - \sum_{i=1}^n \log \mathcal{L}(\theta | X_i, Y_i)}
$$

::: {.incremental}
- Most optimization algorithms are designed to **minimize** loss functions
- Log-transforming the likelihood avoids **precision** and **overflow** issues
:::

## Using the likelihood to estimate parameters {auto-animate=true}

![](../../figures/plot_likelihood.png){fig-align="center"}

## Using the likelihood to estimate parameters {auto-animate=true visibility="uncounted"}

:::: {.columns}

::: {.column width="40%"}
![](../../figures/plot_likelihood.png){fig-align="center"}
:::

::: {.column width="60%"}
::: {.incremental}
- Use Bayes' theorem to combine the likelihood $p(X, Y | \theta)$ with a **prior distribution** $p(\theta)$ to produce a **posterior distribution** $p(\theta | X, Y)$
- Computing this estimate involves **sampling** methods like **Markov chain Monte Carlo** (MCMC) to explore and approximate the posterior
- Note that the posterior, like the likelihood, has **as many dimensions as there are parameters** in the model
- The posterior distribution is the Bayesian estimate, but there are different methods for approximating the posterior
:::
:::

::::

# What is the relationship between a maximum likelihood point estimate and a posterior distribution? {background-color="#006242"}

# Quantifying Uncertainty

## Points, intervals, and distributions

Remember that there are three types of parameters estimates:

::: {.incremental}
1. **Point estimates**: A **single number** that represents our best guess of the parameter
2. **Interval estimates**: A **range of numbers** that represent our best guess of the parameter
3. **Distributions**: An entire **probability distribution** representing our best guess over the support of all parameter values simultaneously in a **joint distribution** or of a given parameter individually in a **marginal distribution**
:::

::: {.fragment}
Frequentist inference **starts with a point** and constructs an interval estimate, Bayesian inference **starts with a distribution** that can be summarized as an interval or point estimate
:::

## Points, intervals, and distributions {visibility="uncounted"}

![](../../figures/plot_log-price-estimate.png){fig-align="center"}

## Posterior vs. sampling distributions

Where does the variation come from in our parameter estimates?

:::: {.columns}

::: {.fragment .column width="50%"}
**Bayesian**

::: {.fragment}
$$
X = \text{data we observe} \\
\theta \sim \text{a probability distribution}
$$
:::

::: {.incremental}
- Start with what we know about the parameters and use the data we observe to **update what we know** about the parameters
- **Directly use probability** to describe uncertainty when working with incomplete knowledge
:::
:::

::: {.fragment .column width="50%"}
**Frequentist**

::: {.fragment}
$$
X \sim \text{a probability distribution} \\
\theta = \text{a value we don't observe}
$$
:::

::: {.incremental}
- Consider what point estimates we would expect to get if we had many hypothetical datasets so that our interval estimate is based on the **asymptotic properties of the estimator**, the procedure used to get our point estimate
- **Indirectly use probability** to describe uncertainty as a repeatable event under identical conditions
:::
:::

::::

## 

::: {.v-center}
![](../../figures/meme_estimand-estimator-estimate.png){fig-align="center"}
:::

## Using the likelihood to quantify uncertainty {auto-animate=true}

![](../../figures/plot_likelihood.png){fig-align="center"}

## Using the likelihood to quantify uncertainty {auto-animate=true visibility="uncounted"}

:::: {.columns}

::: {.column width="40%"}
![](../../figures/plot_likelihood.png){fig-align="center"}
:::

::: {.column width="60%"}
::: {.fragment}
**Frequentist**

::: {.incremental}
- Find the MLE point estimate $\hat{\theta}$
- Rely on theoretical properties of the MLE to construct an interval estimate or use a **bootstrap** procedure to approximate the sampling distribution of the MLE $p(\hat{\theta} | X, Y)$
- A different estimator has a different sampling distribution
:::
:::

::: {.fragment}
**Bayesian**

::: {.incremental}
- Use Bayes' theorem to combine the likelihood and prior to produce a posterior
- Sample from the posterior $p(\theta | X, Y)$
:::
:::
:::

::::

## Summarizing frequentist and Bayesian inference

::: {.center}
|                    | **Frequentist**    | **Bayesian**        |
|--------------------|--------------------|---------------------|
| *Probability*      | Long-run frequency | Information/belief  |
| *Random Variables* | Data               | Parameters          |
| *Distributions*    | Sampling           | Posterior           |
| *Estimation*       | Point              | Posterior           |
| *Intervals*        | Confidence         | Credible            |
| *Algorithms*       | Optimization       | Sampling            |
| *Speed*            | Faster             | Slower              |
:::

## {background-color="#006242"}

### Exercise 15 {.lato-font}

1. Return to your data from the previous exercise (or start with the exercise solution)
2. Split the data into training and testing data using the `stratify` parameter, confirm any other feature engineering, and fit a model using MLE or a Bayesian model
3. Interpret the parameter estimates, being careful to track transformed scales, reference levels, and the presence of multiple predictors in the model
4. Submit your code, output, and interpretations as a single PDF on Canvas

## {background-color="#288DC2"}

### Wrap Up {.permanent-marker-font}

#### Summary

- Extended model fitting beyond ordinary least squares
- Continued comparing frequentist and Bayesian inference

#### Next Time

- Introduce hyperparameters
- Use prior predictive checks to set priors
- Discuss using model selection to tune hyperparameters

