---
title: "DATA 5600 Course Notes"
format: gfm
---

# What I Did

- Organized the course better, including using the decision theoretic objectives and probabilistic modeling workflow as a unifying framework.
- Replaced homework assignments with exercises leading up to two instead of three projects. Exercises provide more consistent practice and include opportunities for reflection and preparation for upcoming class topics.
- Detailed two business cases, one for each unit, to reduce the cognitive burden of switching between many datasets and allowing for greater depth for each application to better inform student project work.
- Clarified the expectations for each project by ensuring the rubrics make the purpose, task, skills, and criteria clear and by having a single initial submission deadline so students can get feedback before presenting and so students who present later in the week don’t have an advantage over others.
- Revised the course content to make it my own, including slides to preview and summarize each lecture.
- Included more live coding in class instruction (as well as discussing error messages) and by starting class with a student called on at random to share their solution to the exercise due for that class.
- Provided more opportunities for student participation beyond live coding by including question slides as prompts for activities, including discussing with a neighbor, raising hands, speed dialogue, and whiteboarding concepts.

# Assessment

- Exercises (20%): At the beginning of each class, a student will be called on at random to walk through and share their solution. Additionally, for each exercise every student will be randomly assigned to review one other student’s exercise. Students won’t get credit if they don’t submit their solution on time, aren’t present and prepared to share when called on at random, or don’t complete their randomly assigned review.
- Interviews (30%): During the first two weeks of class, during the two weeks in the middle of the semester, and during the final two weeks of class each student will have a required 10-minute interview with me where we discuss the course and their efforts and evaluate their understanding in the form of an oral exam.
- Projects (50%): Students will complete two group projects, one focused on multiple regression and the second focused on multiple regression or classification. The groups will both present and submit a technical report developed on a GitHub repository. A week before the presentations, they will submit a draft of their slides to get feedback and have time for revision. The other students in the class, as well as the group members themselves, will help evaluate each of the presentations.

----- CASE ONE -----

# Week 01 (Aug 25, 27; Jan 5, 7)

## Regression and Machine Learning

- Introductions: What business applications fascinate you? What's something you really enjoy doing?
- Stories -- modeling as storytelling
- Analytics as using data to inform decisions in the presence of uncertainty
- Business applications for analytics
- What is regression and ML? What is the relationship and overlap?
- Regression and ML: History lesson about related fields: Let me tell you a story: Mathematics, Statistics, Economics; Computer Science, AI, Machine Learning; and Data Analytics.
- Supervised vs. unsupervised vs. reinforcement learning -- functional mapping (see PML)
- All models predict, some are interpretable, fewere still are causal; is the model consistent with your objective?
- illustrate decision theory with a pros and cons list? then a payoff/loss matrix?
- Reviewing essentials from syllabus
- Discuss the Goldilocks zone for AI use, reference Copilot
- Forming groups, finding data -- happy to help!
- Reference data-stack as recommended tools for the course
- CV entry (as part of the syllabus)

## Workflow

Decision theory is a principled, unifying framework for informing decision making in the presence of uncertainty. While we can and should provide a variety of business “cases” to illustrate applications, we can’t cover every possible application. Decision theory facilitates the generalization of a principled approach to any kind of application. It also helps embed data analysts in the details of the business problem so they can better communicate the implications of their analysis to the decision makers. Decision theory is clearly not used in every discipline, but it can be used in every discipline. This framework can both help differentiate our students and improve the practice of data analytics.

- High-level overview of the workflow
- descriptive statistics, inferential statistics, and decision theory
- Decision theory and decision science
- actions
- states of the world
- objective/payoff/loss functions

### Module 0 (Slides 1-18, 19-30)

Supervised and unsupervised learning in ML
Prediction vs. interpretability/causality
DATA 3100 stats terminology review
Modeling; everything is regression

# Week 02 (Sept 3; Jan 12, 14)

## Probability

See https://allendowney.github.io/ElementsOfDataScience/ 
See https://allendowney.github.io/ThinkStats/
See https://sta210-s22.github.io/website/

- Uncertainty
- RVs, distributions
- set theory
- expectations, conditional expectation
- Bayes theory

### Module 1 (Slides 1-17)

Correlation coefficient
Outcome vs. explanatory variables
Definition of SLR
Deterministic vs. probabilistic
Parameter definitions
Error is a function of unknown parameters.
A residual is an "estimate" of this error.

# Week 03 (Sept 8, 10; Jan 21)

## Statistics

A probabilistic approach to machine learning enhances the principled, unifying framework provided by decision theory. This includes Bayesian statistics, where we treat all unknowns as random variables and can directly use probability distributions to quantify uncertainty in our estimates, and frequentist statistics, where we treat data as random and indirectly use probability distributions to quantify uncertainty in our estimates. The direct, Bayesian approach is arguably more intuitive, especially for applied students, while the indirect, frequentist approach is less computationally intensive. The hope is that introducing them in contrast and as complements will enhance student understanding of modeling generally and combat the problem of student overfitting to a set list of procedures.

- Probability as uncertainty: directly and indirectly, Bayesian and frequentist
- Try using the introduction from my teaching demo?
- Points, intervals, and distributions
- MLE, Bootstrap
- Bayes: Grid Approximation to ABC?
- regression and classification
- Interpreting parameter estimates
- Confidence intervals and bootstrapping

## Linear Models

- SLR, MLR
- Simulating data and recovering parameters

### Module 1 (Slides 18-38)

Normal vs. uniform distributions
Interpreting the slope parameter
Residuals
OLS intuition
Danger with extrapolating
MSE as an estimate of variance
Code: Scatterplots, correlation, OLS, and MSE by hand

### Module 2 (Slides 1-8)

Diagnostics for assumptions before inference
How is the iid assumption formally described?
Does iid persist in the case of grouped observations?
Using graphical diagnostics of assumptions

# Week 04 (Sept 15, 17; Jan 26, 28) POSIT CONF

## Prepare Data

- Feature engineering, transformations, and dummy coding
- Be sure to include +1 for log transforms

## Prior Predictions

- Prior predictive checks?
- Frequentist equivalance?

### Module 2 (Slides 9-34, 35-54)

Are all these assumptions really required to use regression?
Why does the Q-Q plot say "standardized residuals"?
Where did DFBETA and DFFITS come from?
Using numerical diagnostics of assumptions
How to remediate invalidated assumptions
Interpreting estimates with transformations
Code: All graphical and numerical diagnostics

# Week 05 (Sept 22, 24; Feb 2, 4)

## Assumptions and Diagnostics

- Validity: Data is relevant to the objective, no missing variables.
- Representativeness: Data is representative of the population or process.
- Additivity and Linearity: The relationship between the outcome and predictors is additive and linear in parameters.
- Independence: Observations are independent of each other.
- Equal Variance of Errors: Homoscedasticity or constant variance of errors across all levels of predictors.
- Normality of Errors: Errors are normally distributed?

### Module 3 (Slides 1-6, 7-28)

Using the CLT, assuming null is true, to test hypotheses
Computing and using the standard error for hypothesis testing
Manually computing a test statistic and looking up a p-value
Probability of observing something as or more extreme, assuming the null is true
Manually computing a confidence interval using the t-distribution and margin of error
Equivalence of p-values and confidence intervals
Confidence intervals are about uncertainty in parameter estimates (i.e., parameters are fixed)

# Week 06 (Sept 29, Oct 1; Feb 9, 11)

## Model Evaluation

- In-sample vs. out-of-sample vs. decision theoretic evaluation

## Posterior Predictions

### Module 3 (Slides 29-36)

Why do we compute a confidence band for the average of y?
How do we get the confidence band out of a fixed confidence interval?
Prediction intervals are about uncertainty in new data (i.e., new data is random)
Why even produce a confidence interval around the mean if it isn't for a confidence band?
What's the extra term in the standard error for the prediction interval? SD used twice.
Are prediction intervals different in Bayesian statistics since data is fixed?
Properties of MSE, RMSE, MAE, R-squared, adjusted R-squared, and F statistic
Code: Confidence intervals, test statistics, p-value, prediction intervals, confidence and prediction bands, and model fit statistics

### Module 4 (Slides 1-16)

Interpreting multiple slopes
Adding multicollinearity to the list of assumptions

# Week 07 (Oct 6, 8; Feb 18)

Presentations






----- CASE TWO -----

# Week 08 (Oct 13, 15; Feb 23, 25)

### Module 4 (Slides 17-39, 40-55)

Interpreting the F-test and model vs. coefficient p-values
What is multiplicity or the multiple comparisons problem?
Using partial regression to have multiple regression diagnostics
Underfitting and overfitting
Code: Create a scatterplot matrix, correlation matrix (plus a heat map), fitting multiple linear regression, diagnostics (including partial regression plots)

# Week 09 (Oct 20, 22; Mar 2, 4)

## Regularization

## Hyperparameter Tuning

## Module 5 (Slides 1-12, 13-38)

Stepwise regression for model selection when focusing on prediction
Discussed the possibility of p > n
Working from a full model and using backward or forward-selection
No discussion on overfitting needing predictive fit?
Is stepwise regression used in ML practice?
Cross-validation without one static test dataset? What about leakage?
Are variable selection and shrinkage methods primarily used for multicollinearity?
AIC and BIC

# Week 10 (Oct 27, 29; Mar 16, 18)

## Feature Engineering

## Dummy Coding

## Module 5 (Slides 39-59)

Ridge regression, LASSO, elastic net, best subsets, sequential replacement
Bias-variance tradeoff and shrinkage methods
Accuracy and precision is just bias and variance
Code: Clunky, implementing stepwise regression and shrinkage methods with manual hyperparameter tuning

## Module 6 (Slides 1-14)

Dummy variables and the "dummy variable trap"
Interpretation, synonyms, etc.

# Week 11 (Nov 3, 5; Mar 23, 25)

## PCR

## Module 6 (Slides 15-27, 28-58)

Interactions (including higher-order interactions), including interactions
Why do we just interpret interaction effects and not main effects?
Does this all have to be done with the statsmodels API? What about https://www.scikit-yb.org/en/latest/index.html?
What about models built to bring assumption verification, etc. to scikit-learn?
Code: EDA, interactions plots, dummy coding, creating interactions

# Week 12 (Nov 10, 12; Mar 30, Apr 1)

## Multilevel Models

## Module 7 (Slides 1-21, 22-35)

Logistic regression basics, including maximum likelihood estimation
Why not introduce training/testing split earlier without cross-validation?
Probability vs. (Log) Odds

# Week 13 (Nov 17, 19; Apr 6, 8)

## GLMs

## Module 7 (Slides 36-54, 55-61)

Logistic regression assumptions and diagnostics
If the assumptions we used previously break down, why use them at all?
Interpreting a confusion matrix
Code: Splitting the data into train and test, maximum likelihood, ROC/AUC

# Week 14 (Nov 24; Apr 13, 15)

## Logistic Regression

## Module 8 (Slides 1-15, 16-36)

Dimension reduction as opposed to variable selection and regularization
Should this all be in a "feature engineering" module?
Shouldn't we have decision trees and random forests as well/instead?
Are there students who *only* take the introduction to ML?
PCA on its own and then PCA as part of PCR
Code: Using PCA and PCR for continuous and discrete outcomes

# Week 15 (Dec 1, 3; Apr 20, 22)

Presentations

