---
title: "DATA 5600 Course Notes"
format: gfm
---

Provided more opportunities for student participation beyond live coding by including question slides as prompts for activities, including discussing with a neighbor, raising hands, speed dialogue, and whiteboarding concepts.

- Slow down and illustrate -- especially with case, code, questions, and activities
- Use minimal scales for grades (e.g., "Excellent," "Good," "Needs Improvement")
- Create rubrics as a set of guiding questions rather than a checklist
- Learn names so you can call on students to share
- Create QR code for adding me on LinkedIn
- Produce a meme to summarize a concept
- Speed dating to explain concepts

## Week 01 (Aug 25, 27; Jan 5, 7; May 4, 6)

- Regression and Machine Learning (Aug 25; Jan 5; May 4)
- Modeling Workflow (Aug 27; Jan 7; May 6)

## Week 02 (Sept 3; Jan 12, 14; May 11, 13)

- Decisions and Data (Sept 3; Jan 12; May 11)

## Week 03 (Sept 8, 10; Jan 21; May 18, 20)

- Probability and Statistics (Sept 8; Jan 14; May 13)
- Linear Models (Sept 10; Jan 21; May 18)

# Week 04 (Sept 15, 17; Jan 26, 28; May 27)

## Assumptions

TO DO: assumption diagnostics and remediation CODE from Brinley's material

- Preprocessing, feature engineering, transformations (NN are about automating feature engineering)

1. **Validity**: Data is relevant to the objective and no predictors are missing

- Relevant to the objective -- outcome of units sold should include zero or no?
- Omitted/included variable bias
- Comparison to simulated data
- Prior predictive check

Why Bayes? Carefully and directly model uncertainty
This prior predictive distribution is the expected distribution of our data, given how we've specified our likelihood and priors. Does this look reasonable? No one has a negative height, for a start. At this point we can iterate on how we've specified our likelihood and priors, produce another prior predictive distribution and evaluate again, etc.

> "Prior predictive simulation is very useful for assigning sensible priors, because it can be quite hard to anticipate how priors influence the observable variables."

Does Bambi have an easy way to do prior predictive checks? Or just expand our Monte Carlo simulation?
Facet in seaborn.objects to compare the distribution of the outcome vs. the prior predictive check

2. **Representativeness**: Data is representative of the data generating process or population

- y is representative of the population or process we care about, conditioned on the predictors included
- Handling missing data (p. 27 of PML)
- No influential points

3. **Additivity and Linearity**: The mapping function from the predictors to the outcome is additive and a linear function of the parameters

- Determinsitic component is additive and lienar in the parameters
- Scatterplot of X vs. Y -- need to log transform
- Residuals vs. X (cloud-like)
- 𝑦 ̂ is the average/predicted/fitted/expected value of 𝑌 computed from the linear regression model. (𝑦 ̂_𝑖=𝛽 ̂_0+𝛽 ̂_1 𝑥_𝑖 )
- Residuals vs. fitted values (Y-hat) cloud-like
- Partial regression plots, Using partial regression to have multiple regression diagnostics

Linear models are additive, but not necessarily *lines*. Allowing for curvature might be helpful. The form of the model (especially the likelihood) depends on the data. However, as we add complexity to the model, issues begin to emerge with overfitting and interpretation (glimpse of the black-box nature of predictive models).

- Be sure to include +1 for log transforms
- Dummy/one-hot and index coding
- Dummy variables and the "dummy variable trap"

4. **Independence**: Observations are independent of each other

- Dependence in the data that we need a more complex model to accounts for (repeated measures, clustered/spatial data, temporal data)
- random or represenative sample

5. **Constant Variance**: Homoscedasticity or constant variance of errors

- Check for heteroscedasticity, residuals vs. fitted values
- the variability around the line, also called the error variance or 𝜎^2. 
- Note: 𝜎^2 represents the variability of the residual values (𝜖_𝑖).

6. **Normality**: Variation or error is normally distributed

- Q-Q plots
- Why does the Q-Q plot say "standardized residuals"?
- Boxplot/histogram of residuals

7. **Identifiability**: Data allows for parameters to be estimated, including no strong multicollinearity

- Scatterplot matrix, correlation matrix, VIF

Code: Create a scatterplot matrix, correlation matrix (plus a heat map)





# Week 05 (Sept 22, 24; Feb 2, 4; Jun 1, 3)

## Fitting and Interpreting Models

- Training/testing split
- Discussion of the objective of learning about the data generation process so we can generalize to new data
- Reference overfitting/underfitting, bias/variance tradeoff?

- Reference MLE and OLS (p. 8-9 of PML, p. 105 of PML)
- Correlation does not imply causation, and the lack of correlation does not imply lack of causation
- Matrix multiplication -- notation works in both statsmodels and Bambi?

### OLS intuition?

Our goal is to use the model to **estimate** the unobserved parameters from the data (i.e., make our best guess).

To revise our original definition of a model, we use an inferential model to extract **parameter estimates** from the data to inform our managerial decision.

Estimating $\beta_0$ and $\beta_1$ (using lines from)

The best line should be the one that makes **the sum** of the vertical bars as **small as possible**.

The vertical bars are called **residuals**, and represent the distance between the data $y$ and a particular line.

Residuals can be positive and negative, so we make the sum of the **squared residuals** as small as possible.

- Using OLS to estimate the parameters in a linear regression is equivalent to using Bayesian inference with a ______ prior. (Show with the two equation forms.)

- Show that OLS and MLE (and Bayes?) are all decision problems with loss functions to minimize and select the best estimator (p. 143 of PML?)
- Parameter estimates
- Sampling distributions vs. posterior distributions
- Points, intervals, bootstrap, and distributions (estimate, estimator, estimand meme) -- danger of summarizing/summary statistics
- Sampling distributions vs. posterior distributions, bootstrap as poor man's posterior
- Illustrate how a posterior is an updated version of the prior taking into the account the likelihood
- That this is an illustration of shrinkage that happens automatically (p. 89 of PML)
- Significance, confidence intervals, and p-values
- Create a chart showing the differences between Bayesian and frequentist statistics?

Probability of observing something as or more extreme, assuming the null is true
Manually computing a confidence interval using the t-distribution and margin of error
Equivalence of p-values and confidence intervals (meme)
Confidence intervals are about uncertainty in parameter estimates (i.e., parameters are fixed)

# Week 06 (Sept 29, Oct 1; Feb 9, 11; Jun 17)

## Model Evaluation and Prediction

- No free lunch theorem: which model is best depends on the application?

Why do we compute a confidence band for the average of y?
How do we get the confidence band out of a fixed confidence interval?
Prediction intervals are about uncertainty in new data (i.e., new data is random)
Why even produce a confidence interval around the mean if it isn't for a confidence band?
What's the extra term in the standard error for the prediction interval? SD used twice.
Are prediction intervals different in Bayesian statistics since data is fixed?

Properties of MSE, RMSE, MAE, R-squared, adjusted R-squared, and F statistic
Code: Confidence intervals, test statistics, p-value, prediction intervals, confidence and prediction bands, and model fit statistics

- Statistical models capture association, not causation
- In-sample vs. out-of-sample vs. decision theoretic evaluation
- Overfitting and underfitting, variance vs. bias tradeoff
- Use theory-model-evidence.png

## Communicating Results

- Demonstrate presenting on a project as part of the communication session

# Week 07 (Oct 6, 8; Feb 18; Jun 22, 24)

Presentations






----- CASE TWO -----

- Update the syllabus to reflect the changed schedule topics
- Student evaluations
- Student panel invitation
- Post-quiz for research project

# Week 08 (Oct 13, 15; Feb 23, 25; Jun 29, Jul 1)

- Week 09 notes from pre-PhD seminar

### Module 4 (Slides 17-39, 40-55)

Interpreting the F-test and model vs. coefficient p-values
Underfitting and overfitting, the complexity vs. error plot (bias variance tradeoff?) to navigate between the two

# Week 09 (Oct 20, 22; Mar 2, 4; Jul 6, 8)

## Regularization

- Penalize the regression to learn "regular" (i.e., generalizable) features
- Shrinkage toward the MLE

## Hyperparameter Tuning

## Module 5 (Slides 1-12, 13-38)

Stepwise regression for model selection when focusing on prediction
Discussed the possibility of p > n
Working from a full model and using backward or forward-selection
No discussion on overfitting needing predictive fit?
Is stepwise regression used in ML practice?
Cross-validation without one static test dataset? What about leakage?
Cross-validation is needed for hyperparameter tuning, and model selection is a form of "hyperparameter tuning"
Cross-validation for model selection as well in order to keep a larger training dataset (p. 123 of PML)
Are variable selection and shrinkage methods primarily used for multicollinearity?
AIC and BIC

# Week 10 (Oct 27, 29; Mar 16, 18; Jul 13, 15)

## Feature Engineering

## Dummy Coding

## Module 5 (Slides 39-59)

Ridge regression, LASSO, elastic net, best subsets, sequential replacement
Bias-variance tradeoff and shrinkage methods
Accuracy and precision is just bias and variance
Code: Clunky, implementing stepwise regression and shrinkage methods with manual hyperparameter tuning

# Week 11 (Nov 3, 5; Mar 23, 25; Jul 20, 22)

## PCR

## Module 6 (Slides 15-27, 28-58)

Interactions (including higher-order interactions), including interactions
Why do we just interpret interaction effects and not main effects?
Does this all have to be done with the statsmodels API? What about https://www.scikit-yb.org/en/latest/index.html?
What about models built to bring assumption verification, etc. to scikit-learn?
Code: EDA, interactions plots, dummy coding, creating interactions

# Week 12 (Nov 10, 12; Mar 30, Apr 1; Jul 27, 29)

## Multilevel Models

- Motivate with Simpson's paradox (PML p. 80)

## Module 7 (Slides 1-21, 22-35; Aug 3, 5)

Logistic regression basics, including maximum likelihood estimation
Why not introduce training/testing split earlier without cross-validation?
Probability vs. (Log) Odds

# Week 13 (Nov 17, 19; Apr 6, 8)

## GLMs

## Module 7 (Slides 36-54, 55-61)

Logistic regression assumptions and diagnostics
If the assumptions we used previously break down, why use them at all?
Interpreting a confusion matrix
Code: Splitting the data into train and test, maximum likelihood, ROC/AUC

# Week 14 (Nov 24; Apr 13, 15)

## Logistic Regression

## Module 8 (Slides 1-15, 16-36)

Dimension reduction as opposed to variable selection and regularization
PCA on its own and then PCA as part of PCR
Code: Using PCA and PCR for continuous and discrete outcomes

# Week 15 (Dec 1, 3; Apr 20)

Presentations

