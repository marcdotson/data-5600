---
title: "DATA 5600 Course Notes"
format: gfm
---

Provided more opportunities for student participation beyond live coding by including question slides as prompts for activities, including discussing with a neighbor, raising hands, speed dialogue, and whiteboarding concepts.

- Slow down and illustrate -- especially with case, code, questions, and activities
- Use minimal scales for grades (e.g., "Excellent," "Good," "Needs Improvement")
- Create rubrics as a set of guiding questions rather than a checklist
- Learn names so you can call on students to share
- Create QR code for adding me on LinkedIn
- Produce a meme to summarize a concept
- Speed dating to explain concepts

## Week 01 (Aug 25, 27; Jan 5, 7; May 4, 6)

- Regression and Machine Learning (Aug 25; Jan 5; May 4)
- Modeling Workflow (Aug 27; Jan 7; May 6)

## Week 02 (Sept 3; Jan 12, 14; May 11, 13)

- Decisions and Data (Sept 3; Jan 12; May 11)

## Week 03 (Sept 8, 10; Jan 21; May 18, 20)

- Probability and Statistics (Sept 8; Jan 14; May 13)
- Linear Models (Sept 10; Jan 21; May 18)

# Week 04 (Sept 15, 17; Jan 26, 28; May 27)

- Diagnostics and Remedies (Part 01)
- Diagnostics and Remedies (Part 02)

Linear models are additive, but not necessarily *lines*. Allowing for curvature might be helpful. The form of the model (especially the likelihood) depends on the data. However, as we add complexity to the model, issues begin to emerge with overfitting and interpretation (glimpse of the black-box nature of predictive models).

- Be sure to include +1 for log transforms
- Dummy/one-hot and index coding
- Dummy variables and the "dummy variable trap"

4. **Independence**: Observations are independent of each other

## Independence

:::: {.columns .v-center}

::: {.column width="60%"}
$$
y_i = \beta_0 + \beta_1 x_{1,i} + \cdots + \beta_p x_{p,i} + \epsilon_i \\
\text{where } \epsilon_i \sim \text{Normal}(0, \sigma^2)
$$

$$
\mu_i = \beta_0 + \beta_1 x_{1,i} + \cdots + \beta_p x_{p,i} \\
y_i \sim \text{Normal}(\mu_i, \sigma^2)
$$
:::

::: {.column width="40%"}
Observations are independent of each other
:::

::::

Common ways this assumption is violated:
Repeated measures
Observations are collected from the same individual/unit so that each individual/unit is represented multiple times in the data set
Ex: Data of pre- and post-test scores for a class of students
Clustered data
Observations are grouped into clusters such that data within clusters are more similar than data across clusters
Ex: Data of elementary students from three different schools
Temporal correlation
Observations are collected in regular time intervals such that sequential data points vary in similar ways
Ex: daily wastewater measurements
Spatial correlation
Observations are collected across space/geography such that data points that are next to each other are similar
Ex: Data from oil drilling sites in Texas

- Dependence in the data that we need a more complex model to accounts for (repeated measures, clustered/spatial data, temporal data)
- Sequence plot (if observations are in some natural order) with no trend in the mean or variance
- random sampling, any reason to believe observations are dependent on each other?
- Can artificially narrow interval estimates, so include predictors that account for dependence (a team indicator?) or use a more complex model (multilevel/hierarchical model, time series model, spatial model)

```{python}
# Sequence Plot (NOT APPROPRIATE HERE!)
x_values = range(1, len(cars) + 1)
plt.plot(x_values, cars['residuals'], linestyle='-')
plt.xlabel("Order in Data Set")
plt.show()
```

5. **Constant Variance**: Homoscedasticity or constant variance of errors

## Constant Variance

:::: {.columns .v-center}

::: {.column width="60%"}
$$
y_i = \beta_0 + \beta_1 x_{1,i} + \cdots + \beta_p x_{p,i} + \epsilon_i \\
\text{where } \epsilon_i \sim \text{Normal}(0, \sigma^2)
$$

$$
\mu_i = \beta_0 + \beta_1 x_{1,i} + \cdots + \beta_p x_{p,i} \\
y_i \sim \text{Normal}(\mu_i, \sigma^2)
$$
:::

::: {.column width="40%"}
Homoscedasticity or constant variance of errors
:::

::::


```{python}
# Residuals vs. Fitted Values Plot
fig = plt.figure(figsize = (4, 4))
sns.residplot(x = cars['fittedvalues'], 
              y = cars['residuals'],
              lowess = True,
              scatter_kws = {'s': 3},
              line_kws = {'color': 'red', 'lw': 1})
plt.title("Residuals vs. Fitted")
plt.ylabel("Residuals")
plt.xlabel("Fitted Values")
plt.show()
```

- Check for heteroscedasticity, residuals vs. fitted values
- No funnel shape or megaphone shape
- the variability around the line, also called the error variance or ùúé^2. 
- Note: ùúé^2 represents the variability of the residual values (ùúñ_ùëñ).
- Transform Y and then X.

6. **Normality**: Variation or error is normally distributed

## Normality

:::: {.columns .v-center}

::: {.column width="60%"}
$$
y_i = \beta_0 + \beta_1 x_{1,i} + \cdots + \beta_p x_{p,i} + \epsilon_i \\
\text{where } \epsilon_i \sim \text{Normal}(0, \sigma^2)
$$

$$
\mu_i = \beta_0 + \beta_1 x_{1,i} + \cdots + \beta_p x_{p,i} \\
y_i \sim \text{Normal}(\mu_i, \sigma^2)
$$
:::

::: {.column width="40%"}
Variation or error is normally distributed
:::

::::

```{python}
# Boxplot
fig = plt.figure(figsize = (4, 4))
plt.boxplot(cars['residuals'])
plt.ylabel("Residuals")
plt.show()

# Histogram
fig = plt.figure(figsize = (4, 4))

# plot histogram (density = True so that it's on the same scale as the normal distribution)
plt.hist(cars['residuals'], 
         density = True, 
         bins = 11)
plt.xlabel("Residuals")
plt.ylabel("Density")

# calculate mean and standard deviation
mean = np.mean(cars['residuals'])
sd = np.std(cars['residuals'])

# generate x values to plot
xmin, xmax = plt.xlim()
x = np.linspace(xmin, xmax, 100)

# plot normal distribution curve
plt.plot(x,
         stats.norm.pdf(x, mean, sd), 
         color = 'r',
         lw = 3)
plt.show()

# Q-Q Plot (Normal Probability Plot)
sm.qqplot(cars['residuals'], 
          line = '45', 
          fit = True)
plt.title("Normal Q-Q")
plt.show()

# Shapiro-Wilk Test
stats.shapiro(cars['residuals'])
```

- Consideration of residuals, non-normal continuous variables
- Q-Q plots
- Why does the Q-Q plot say "standardized residuals"?
- Boxplot/histogram of residuals
- Fix: Transform Y and then X

7. **Identifiability**: Data allows for parameters to be estimated, including no strong multicollinearity

## Identifiability

:::: {.columns .v-center}

::: {.column width="60%"}
$$
y_i = \beta_0 + \beta_1 x_{1,i} + \cdots + \beta_p x_{p,i} + \epsilon_i \\
\text{where } \epsilon_i \sim \text{Normal}(0, \sigma^2)
$$

$$
\mu_i = \beta_0 + \beta_1 x_{1,i} + \cdots + \beta_p x_{p,i} \\
y_i \sim \text{Normal}(\mu_i, \sigma^2)
$$
:::

::: {.column width="40%"}
Data allows for parameters to be estimated, including no strong multicollinearity
:::

::::

```{python}
from statsmodels.stats.outliers_influence import variance_inflation_factor as vif

# Correlation matrix
super.corr()

# Heatmap
sns.heatmap(super.corr(), 
            cmap = 'coolwarm',  # color pallete
            annot = True,  # adds the correlation values in the boxes
            vmin = -1,  # start color legend at -1
            vmax = 1,  # stop color legend at 1
            mask = np.triu(super.corr()))  # remove the upper half of the plot
plt.show()

# Variance Inflation Factors (VIF)
super_vifs = pd.DataFrame()
super_vifs['Feature'] = X.columns[1:]
super_vifs['VIF'] = [vif(X, i) for i in range(1, len(X.columns))]

print("Max = ", max(super_vifs['VIF']))
print("Mean = ", np.mean(super_vifs['VIF']))
super_vifs
```

- Scatterplot matrix, correlation matrix, VIF

Code: Create a scatterplot matrix, correlation matrix (plus a heat map)





# Week 05 (Sept 22, 24; Feb 2, 4; Jun 1, 3)

## Dummy coding

It isn't a model assumption, but one of the most common **feature engineering** (a.k.a., preprocessing data) steps is **dummy coding** (a.k.a., indicator coding or one hot encoding) where a discrete predictor is turned into multiple binary predictors

```{python}
#| eval: true
#| output-location: column

pb_model = pb_data.to_dummies(
    columns = ['brand', ], drop_first = True
)
```

::: {.fragment}
Why can't we include all of the **levels** of a discrete predictor in a linear regression?
:::

## Fitting and Interpreting Models

- Preprocessing, feature engineering, transformations (NN are about automating feature engineering)

- Training/testing split
- Discussion of the objective of learning about the data generation process so we can generalize to new data
- Reference overfitting/underfitting, bias/variance tradeoff?

- Reference MLE and OLS (p. 8-9 of PML, p. 105 of PML)
- Correlation does not imply causation, and the lack of correlation does not imply lack of causation
- Matrix multiplication -- notation works in both statsmodels and Bambi?

### OLS intuition?

Our goal is to use the model to **estimate** the unobserved parameters from the data (i.e., make our best guess).

To revise our original definition of a model, we use an inferential model to extract **parameter estimates** from the data to inform our managerial decision.

Estimating $\beta_0$ and $\beta_1$ (using lines from)

The best line should be the one that makes **the sum** of the vertical bars as **small as possible**.

The vertical bars are called **residuals**, and represent the distance between the data $y$ and a particular line.

Residuals can be positive and negative, so we make the sum of the **squared residuals** as small as possible.

- Using OLS to estimate the parameters in a linear regression is equivalent to using Bayesian inference with a ______ prior. (Show with the two equation forms.)

- Show that OLS and MLE (and Bayes?) are all decision problems with loss functions to minimize and select the best estimator (p. 143 of PML?)
- Parameter estimates
- Sampling distributions vs. posterior distributions
- Points, intervals, bootstrap, and distributions (estimate, estimator, estimand meme) -- danger of summarizing/summary statistics
- Sampling distributions vs. posterior distributions, bootstrap as poor man's posterior
- Illustrate how a posterior is an updated version of the prior taking into the account the likelihood
- That this is an illustration of shrinkage that happens automatically (p. 89 of PML)
- Significance, confidence intervals, and p-values
- Create a chart showing the differences between Bayesian and frequentist statistics?

Probability of observing something as or more extreme, assuming the null is true
Manually computing a confidence interval using the t-distribution and margin of error
Equivalence of p-values and confidence intervals (meme)
Confidence intervals are about uncertainty in parameter estimates (i.e., parameters are fixed)

# Week 06 (Sept 29, Oct 1; Feb 9, 11; Jun 17)

## Model Evaluation and Prediction

- No free lunch theorem: which model is best depends on the application?

Why do we compute a confidence band for the average of y?
How do we get the confidence band out of a fixed confidence interval?
Prediction intervals are about uncertainty in new data (i.e., new data is random)
Why even produce a confidence interval around the mean if it isn't for a confidence band?
What's the extra term in the standard error for the prediction interval? SD used twice.
Are prediction intervals different in Bayesian statistics since data is fixed?

Properties of MSE, RMSE, MAE, R-squared, adjusted R-squared, and F statistic
Code: Confidence intervals, test statistics, p-value, prediction intervals, confidence and prediction bands, and model fit statistics

- Statistical models capture association, not causation
- In-sample vs. out-of-sample vs. decision theoretic evaluation
- Overfitting and underfitting, variance vs. bias tradeoff
- Use theory-model-evidence.png

## Communicating Results

- Demonstrate presenting on a project as part of the communication session

# Week 07 (Oct 6, 8; Feb 18; Jun 22, 24)

Presentations






----- CASE TWO -----

- Update the syllabus to reflect the changed schedule topics
- Student evaluations
- Student panel invitation
- Post-quiz for research project

# Week 08 (Oct 13, 15; Feb 23, 25; Jun 29, Jul 1)

- Week 09 notes from pre-PhD seminar

### Module 4 (Slides 17-39, 40-55)

Interpreting the F-test and model vs. coefficient p-values
Underfitting and overfitting, the complexity vs. error plot (bias variance tradeoff?) to navigate between the two

# Week 09 (Oct 20, 22; Mar 2, 4; Jul 6, 8)

## Regularization

- Penalize the regression to learn "regular" (i.e., generalizable) features
- Shrinkage toward the MLE

Why Bayes? Carefully and directly model uncertainty
This prior predictive distribution is the expected distribution of our data, given how we've specified our likelihood and priors. Does this look reasonable? No one has a negative height, for a start. At this point we can iterate on how we've specified our likelihood and priors, produce another prior predictive distribution and evaluate again, etc.

> "Prior predictive simulation is very useful for assigning sensible priors, because it can be quite hard to anticipate how priors influence the observable variables."

Does Bambi have an easy way to do prior predictive checks? Or just expand our Monte Carlo simulation?
Facet in seaborn.objects to compare the distribution of the outcome vs. the prior predictive check

## Hyperparameter Tuning

## Module 5 (Slides 1-12, 13-38)

Stepwise regression for model selection when focusing on prediction
Discussed the possibility of p > n
Working from a full model and using backward or forward-selection
No discussion on overfitting needing predictive fit?
Is stepwise regression used in ML practice?
Cross-validation without one static test dataset? What about leakage?
Cross-validation is needed for hyperparameter tuning, and model selection is a form of "hyperparameter tuning"
Cross-validation for model selection as well in order to keep a larger training dataset (p. 123 of PML)
Are variable selection and shrinkage methods primarily used for multicollinearity?
AIC and BIC

# Week 10 (Oct 27, 29; Mar 16, 18; Jul 13, 15)

## Feature Engineering

## Dummy Coding

## Module 5 (Slides 39-59)

Ridge regression, LASSO, elastic net, best subsets, sequential replacement
Bias-variance tradeoff and shrinkage methods
Accuracy and precision is just bias and variance
Code: Clunky, implementing stepwise regression and shrinkage methods with manual hyperparameter tuning

# Week 11 (Nov 3, 5; Mar 23, 25; Jul 20, 22)

## PCR

## Module 6 (Slides 15-27, 28-58)

Interactions (including higher-order interactions), including interactions
Why do we just interpret interaction effects and not main effects?
Does this all have to be done with the statsmodels API? What about https://www.scikit-yb.org/en/latest/index.html?
What about models built to bring assumption verification, etc. to scikit-learn?
Code: EDA, interactions plots, dummy coding, creating interactions

# Week 12 (Nov 10, 12; Mar 30, Apr 1; Jul 27, 29)

## Multilevel Models

- Motivate with Simpson's paradox (PML p. 80)

## Module 7 (Slides 1-21, 22-35; Aug 3, 5)

Logistic regression basics, including maximum likelihood estimation
Why not introduce training/testing split earlier without cross-validation?
Probability vs. (Log) Odds

# Week 13 (Nov 17, 19; Apr 6, 8)

## GLMs

## Module 7 (Slides 36-54, 55-61)

Logistic regression assumptions and diagnostics
If the assumptions we used previously break down, why use them at all?
Interpreting a confusion matrix
Code: Splitting the data into train and test, maximum likelihood, ROC/AUC

# Week 14 (Nov 24; Apr 13, 15)

## Logistic Regression

## Module 8 (Slides 1-15, 16-36)

Dimension reduction as opposed to variable selection and regularization
PCA on its own and then PCA as part of PCR
Code: Using PCA and PCR for continuous and discrete outcomes

# Week 15 (Dec 1, 3; Apr 20)

Presentations

