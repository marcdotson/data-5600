---
title: "DATA 5600 Course Notes"
format: gfm
---

- Include slide/code animations, code highlighting, slide/fragment transitions, use icons, use memes

- Provided more opportunities for student participation beyond live coding by including question slides as prompts for activities, including discussing with a neighbor, raising hands, speed dialogue, and whiteboarding concepts.

- Activities and exercises to have an AI produce code and then have students evaluate
- Activities and exercises to provide an AI prompt to ask students questions to help them evalute their understanding, a practice interview
- Demonstrate how to use AI in class, how to present to a non-technical audience
- Demonstrate presenting on a project as part of the communication session

- Use minimal scales for grades (e.g., "Excellent," "Good," "Needs Improvement")
- Create project/interview rubrics as a set of guiding questions rather than a checklist
- Learn names so you can call on students to share

- QR code for adding me on LinkedIn

# Week 01 (Aug 25, 27; Jan 5, 7; May 4, 6)

## Regression and Machine Learning (Aug 25; Jan 5; May 4)

## Modeling Workflow (Aug 27; Jan 7; May 6)

### Before Data

- High-level overview of the (interpretable) modeling workflow (with the case as application?)

- Plan: objective/loss function, data generation process/ideal dataset
- Stories -- implicitly we weight pros and cons when making a decision -- narrate this objective
- Stories -- modeling as storytelling, starts as a narrative and then gets mathematized
- Build: Translate story into mathematical models -- a functional mapping of your objective (loss function) inputs to output (likelihood)
- Activity: Discuss with your groups what kind of loss function, data generating process makes sense for your problem








## Visualize | Summarize the Data

Summarizing data is initially about discovery. It includes computing **statistics** (i.e., numerical summaries) and **data visualization** (i.e., graphical summaries).

- Summarizing data is closely tied with data wrangling.
- Summarizing data is often not an end in itself.

## {background-color="#D1CBBD"}

::: {.columns .v-center}
![](../../figures/workflow_reconcile.png){fig-align="center"}
:::

## Model | Inference and Prediction

Models *extract information* from the data to inform our managerial decision.

- In order to inform the marketing mix, the models we use are often inferential.
- Some managerial decisions only rely on prediction.

## Inference

Remember that models *extract information* from the data to inform our managerial decision. While data wrangling and visualization can *suggest* patterns of interest, a model is often needed.

How we model the data depends on if we care about **inference** or only about *prediction*.

- Inference means "reaching a conclusion based on evidence."
- Inferential modeling may also be referred to as **statistical modeling** given its ties to statistics.
- We use inferential models to understand a process we don't observe.

Note that models used for inference can also be used to predict, but they are focused primarily on understanding unobserved processes.

## Preprocess | Preprocess Data for Modeling

When working with real data, we often need to *preprocess the data* in order to model it or to make it easier for the model to use. Think of this as **data wrangling for models**.

One common preprocessing problem is dealing with discrete data.

```{r echo=FALSE, message=FALSE}
# Demonstrate dummy coding.
library(tidyverse)

sub_category <- read_csv(here::here("Data", "soup_data.csv")) |> 
  transmute(sub_category = Sub_Category)

one_dummy <- sub_category |> 
  filter(sub_category %in% c("CONDENSED SOUP", "RAMEN"))

one_dummy |> unique()
```

---

For the computer to understand the different *levels* of a discrete variable used in a model, we need to recode into *binary variables*.

```{r echo=FALSE, message=FALSE}
# One dummy.
one_dummy |> 
  mutate(fastDummies::dummy_cols(sub_category)) |> 
  rename(
    condensed_soup = '.data_CONDENSED SOUP',
    ramen = '.data_RAMEN'
  ) |> 
  select(sub_category, condensed_soup, ramen)
```

---

The more levels in a discrete variable, the more binary variables we need.

```{r echo=FALSE, message=FALSE}
# Many dummies.
sub_category |> 
  mutate(fastDummies::dummy_cols(sub_category)) |> 
  rename(
    condensed_soup = '.data_CONDENSED SOUP',
    ramen = '.data_RAMEN',
    dry_soup = '.data_DRY SOUP'
  ) |> 
  select(sub_category, condensed_soup, ramen, dry_soup)
```

---

One more thing: For statistical reasons, the model can't use *all* of the binary variables created from a single discrete variable. One of them needs to be dropped as a **reference level** or **baseline level**. If you had more than one discrete variable, *each* discrete variable would have its own baseline level.

All of this discrete variable preprocessing is called **dummy coding** (a.k.a., indicator coding).

```{r echo=FALSE, message=FALSE}
# Baseline level.
sub_category |> 
  mutate(fastDummies::dummy_cols(sub_category)) |> 
  rename(
    condensed_soup = '.data_CONDENSED SOUP',
    ramen = '.data_RAMEN',
    dry_soup = '.data_DRY SOUP'
  ) |> 
  select(sub_category, ramen, dry_soup)
```

## {background-color="#D1CBBD"}

::: {.columns .v-center}
![](../../figures/workflow_fit.png){fig-align="center"}
:::

## Fit | Fit the Model

When we **fit** the model (a.k.a., training, calibrating, or estimating the model) we are getting parameter estimates.


## {background-color="#D1CBBD"}

::: {.columns .v-center}
![](../../figures/workflow_evaluate.png){fig-align="center"}
:::

## Evaluate | Parameter Estimates, Significance, and Predictive Fit

Our goal is to use the model to *estimate* the parameters from the data. In other words, parameter estimates are the information we are extracting from the data to inform our managerial decision.


## {background-color="#D1CBBD"}

::: {.columns .v-center}
![](../../figures/workflow_predict.png){fig-align="center"}
:::

## Predict | Counterfactual Predictions

Once we have a best-fitting model, we want to predict what will happen if we intervene in the process in a certain way. This is called a **counterfactual**.

For example, once we have parameter estimates for $\beta_0$ and $\beta_1$ from fitting the model, we can play the counterfactual "what if" game.

Specifically, what would happen if we allocated certain amounts of `promotion_spend`? By combining the parameter estimates with possible budget allotments, we can predict `sales`.

## {background-color="#D1CBBD"}

::: {.columns .v-center}
![](../../figures/workflow_communicate.png){fig-align="center"}
:::

## Communicate | Report and Create Data Products

Effectively communicating marketing insights brings us full circle and highlights the necessity of domain expertise.

The analyst needs to **interpret results in a way that clearly informs the managerial decision**. You may hear this referred to as "storytelling."








### Using Data

- Explore: Summarize and visualize the data, data dictionary; X as design matrix, N and P
- Reconcile: Data and Model. Preprocess the data (what transformations?), compare the data to what you've simulated to see if you're missing something from your model. Check assumptions, specific to linear models or other kinds of models (or is that just part of prepare?). Careful with overfitting. Sensitivity analysis.
- Fit: Fit/train/calibrate the model on training data (lots of different models, libraries we can use -- needs to be consistent with our objective)
- Evaluate: Parameter estimates (including uncertainty), considering significance, and overall (predictive) model fit on testing data
- Predict: Use the model to make predictions on new data, including uncertainty in those predictions
- Activity: Speed dialogue to discuss favorite libraries you use for working with data

### Communicating Results

- Communicate: Report and present the results in a way that is understood by a mixed audience
- Everything you've worked on is to inform the managerial decision, so if you don't it's like giving up at the end of the race
- Discuss good and bad examples of communicating modeling results
- Exercise: Summarize the case's objective and the ideal data
- Wrap up

# Week 02 (Sept 3; Jan 12, 14; May 11, 13)

## Decisions and Data / Loss and Likelihood Functions / Payoffs, Losses, and Likelihoods (Sept 3; Jan 12; May 11)

### Decisions

- Get started (solution, schedule w/o dates?, and workflow)
- Use the case to start illustrating in more detail the modeling workflow
- Decision theory is a principled, unifying framework for informing decision making in the presence of uncertainty

- Decision theory provides a principled, *unifying* framework for data analytics
- All decisions can be characterized by minimizing a loss function $\ell (\theta, a)$
- Business decisions, statistical estimators, model selection, etc.
- Complemented by a probabilistic approach to machine learning
- Analytics is using data to inform decision-making and decision theory shows *how*


As a data analyst communicates with domain experts to understand the problem and how to inform the decision, a decision theoretic approach requires specifying an objective function that is consistent with the problem and captures the risk aversion of the decision maker. For this problem, a simple customer-specific profit function suffices:

$$
\pi(p) = Y(p) \times (p - mc)
$$

where p is the personalized price for the given customer, Y(p) is the predicted demand for the given customer, and mc is the marginal cost. In decision theory, the objective function is typically a loss function. We can identify an optimal decision, conditioned on the specification of the objective function, by minimizing the loss function or equivalently, as we have here, maximizing a utility or profit function.

SLR-Like

Actions: Go or no-go
State of the World: Consumer demand (units sold)
Objective Function: Profit function

MLR-Like

Actions: Go or no-go and pricing decision
State of the World: Consumer demand (units sold as a function of price)
Objective Function: Profit function


- Need to communicate with domain experts to understand the problem and how to inform the decision
- Illustrate decision theory with a pros and cons list? then a payoff/loss matrix? maximize utility/minimize loss
- Maximize profit, maximize market share, minimize churn, etc.
- Equivalence of utility and loss functions: u(a, s) = - l(a, s) (or l(a, s) = - u(a, s))

- Actions, states of the world, objective function (payoff/loss) (starting on BID p. 239)
- Illustrate first with decision making under *certainty*
- Consider clearing up the difference between uncertainty and risk (BID p. 199)?
- Profit and revenue as a function of demand
- Understand or assume costs (consumer goods 20% markup)

- Example loss functions -- or just focus on a linear profit function?
- Risk neutral, risk averse, risk seeking loss functions (BID p. 227)
- What are the inputs into the function? Parameters -- the states of the world -- along with what actions to take?

- The fact that we don't know the state of the world is what motivates probability and statistics
- Activity: The actions are the prices. The unknown state of the world is demand. The objective function is profit.

### Data Generating Process

- Stories -- modeling as storytelling, starts as a narrative and then gets mathematized
- One way to consider this is what data would you need to inform your decision? Or what is the ideal dataset for your objective?
- Reference DAGs in DATA 5620

It's helpful to draw the possible data generating process as a **graph**. This is a special kind of graph known as a **directed acyclic graph** or **DAG**.

```{r echo=FALSE, message=FALSE, warning=FALSE}
# Create a DAG.
library(dagitty)
library(ggdag)

dag <- dagitty("dag {
  F -> P
  F -> S
  P -> S
}")

ggdag(dag, layout = "circle") +
  theme_dag()
```

- Using linear models to formalize this story and probability distributions to quantify uncertainty
- Models are simplifications of reality, but the goal is to capture the essence of the data generation process
- No free lunch theorem: which model is best depends on the application
- Example likelihoods (as models of the data generating process)

##

To make this a *statistical* model we need to express that our model is not going to explain everything about the data:

$$\text{sales} = \beta_0 + \beta_1 \times \text{promotion_spend} + \epsilon$$

Here $\epsilon$ represents **statistical error**. Think of this $\epsilon$ as including every variable beyond `promotion_spend` that effects `sales`.

## 

Every inferential model is a *simplification* of the *true* data generating process (which we don't observe). Even so, models can still help us learn about parameters, assuming the model is a good approximation of the true data generating process.

<!-- <center>
![](../../Figures/models_rockets.png){width=700px}
</center> -->

- Generative models, use to generate or simulate data to evaluate assumptions, prepare for an analysis, test code (recover parameters)
- Signal simulating data

## 

Since the data generating process is the unobserved process that generates the data, before we work with real data where we never know the data generating process, we can assume that our model *is* the data generating process, *choose* values for the parameters, and generate or **simulate** data using the model. 

For example, we can choose values for $\beta_0$ and $\beta_1$, simulate possible `promotion_spend` data and then simulate `sales` data.

Why? Two big reasons:

1. Prepare our data analysis before getting real data.
2. Prove our code is working by recovering parameter values.


- Activity: 

- Exercise: 
- Wrap up

# Week 03 (Sept 8, 10; Jan 21; May 18, 20)

## Probability and Statistics / Probabilistic Machine Learning (Sept 8; Jan 14; May 13)

A probabilistic approach to machine learning enhances the principled, unifying framework provided by decision theory. This includes Bayesian statistics, where we treat all unknowns as random variables and can directly use probability distributions to quantify uncertainty in our estimates, and frequentist statistics, where we treat data as random and indirectly use probability distributions to quantify uncertainty in our estimates. The direct, Bayesian approach is arguably more intuitive, especially for applied students, while the indirect, frequentist approach is less computationally intensive. The hope is that introducing them in contrast and as complements will enhance student understanding of modeling generally and combat the problem of student overfitting to a set list of procedures.

- Probability as uncertainty: directly and indirectly, Bayesian and frequentist (PML p. 33)
- Try using the introduction from my teaching demo?

- Why teach both? If all you have is a hammer, everything looks like a nail. And it's often easier to understand a thing in contrast.

### Uncertainty

- Get started (solution, schedule, and workflow)
- Probability as a unifying framework for machine learning (PML p. 1)
- Bayes vs. frequentist probability?
- Uncertainty from not knowing the true data generating process vs. naturally occuring uncertainty in the data (PML p. 7, 33-34)
- Axioms and basic definitions, set theory

### Random Variables and Distributions

- Use an experiment to illustrate the difference between random and fixed variables: Need something fixed to learn about the random variable
- RVs, discrete and continous, PMF and PDF
- Distributions, supports
- Normal vs. uniform distributions, etc.
- Joint and marginal distributions
- Expectations, conditional expectation; Means, variance, standard deviation, mode, limitations of summaries (PML p. 43)
- Bayes rule
- Why Normal? (PML p. 60)

### 

- CLT? Monte Carlo? (PML p. 71-72)
- Covariance and correlation?
- Correlation does not imply causation, and the lack of correlation does not imply lack of causation
- Data and parameters $f(X; \theta)$
- Wrap up

### Statistics

- Learning parameters from data, estimating parameters, model fitting, training, calibration
- Likelihood as our data generating process, the evidence for the data we see?
- MLE: Pick the parameters that assign the highest probability to the training data (PML p. 105)
- MLE as a point approximation of the posterior distribution with a uniform prior (PML p. 106)
- MLE to OLS

- Points, intervals, and distributions
- Bayes: Grid Approximation to ABC?
- Sampling distributions vs. posterior distributions, bootstrap as poor man's posterior

- Interpreting parameter estimates
- Confidence intervals and bootstrapping

See https://allendowney.github.io/ElementsOfDataScience/ 
See https://allendowney.github.io/ThinkStats/
See https://sta210-s22.github.io/website/

## Linear Regression (Sept 10; Jan 21; May 18)

- Regression is also referred to as linear regression or, more generally, a linear model
- Key property is the expected value (mu) of the output is assumed to be a linear function of the inputs (X)
- Line isn't great, but we can do non-linear transofmrations on the predictors while still keeping a linear model

- SLR, MLR
- Simulating data and recovering parameters

### Module 1 (Slides 1-17, Slides 18-38)

Correlation coefficient
Outcome vs. explanatory variables
Definition of SLR
Deterministic vs. probabilistic
Parameter definitions
Error is a function of unknown parameters.
A residual is an "estimate" of this error.
Interpreting the slope parameter
Residuals
OLS intuition
Danger with extrapolating
MSE as an estimate of variance
Code: Scatterplots, correlation, OLS, and MSE by hand

### Module 2 (Slides 1-8)

Diagnostics for assumptions before inference
How is the iid assumption formally described?
Does iid persist in the case of grouped observations?
Using graphical diagnostics of assumptions







# Week 04 (Sept 15, 17; Jan 26, 28; May 27) POSIT CONF

## Continuous Predictors

- Exploration and preparation
- Feature engineering, transformations
- Be sure to include +1 for log transforms

## Discrete Predictors

- Exploration and preparation
- Dummy/one-hot and index coding

### Module 2 (Slides 9-34, 35-54)

Are all these assumptions really required to use regression?
Why does the Q-Q plot say "standardized residuals"?
Where did DFBETA and DFFITS come from?
Using numerical diagnostics of assumptions
How to remediate invalidated assumptions
Interpreting estimates with transformations
Code: All graphical and numerical diagnostics

# Week 05 (Sept 22, 24; Feb 2, 4; Jun 1, 3)

## Assumptions and Diagnostics

- Validity: Data is relevant to the objective, no missing variables.
- Representativeness: Data is representative of the population or process.
- Additivity: The relationship between the outcome and predictors is additive.
- Linearity: The mean is a linear function of the predictors.
- Independence: Observations are independent of each other.
- Constant Variance: Homoscedasticity or constant variance of errors across all levels of predictors.
- Normality of Errors: Errors are normally distributed?

- Omitted and included variable bias
- Multicollinearity

## Fitting and Interpreting Models

- Parameter estimates
- Significance, confidence intervals, and p-values

### Module 3 (Slides 1-6, 7-28)

Using the CLT, assuming null is true, to test hypotheses
Computing and using the standard error for hypothesis testing
Manually computing a test statistic and looking up a p-value
Probability of observing something as or more extreme, assuming the null is true
Manually computing a confidence interval using the t-distribution and margin of error
Equivalence of p-values and confidence intervals
Confidence intervals are about uncertainty in parameter estimates (i.e., parameters are fixed)

# Week 06 (Sept 29, Oct 1; Feb 9, 11; Jun 17)

## Model Evaluation and Prediction

- In-sample vs. out-of-sample vs. decision theoretic evaluation
- Overfitting and underfitting

## Communicating Results

### Module 3 (Slides 29-36)

Why do we compute a confidence band for the average of y?
How do we get the confidence band out of a fixed confidence interval?
Prediction intervals are about uncertainty in new data (i.e., new data is random)
Why even produce a confidence interval around the mean if it isn't for a confidence band?
What's the extra term in the standard error for the prediction interval? SD used twice.
Are prediction intervals different in Bayesian statistics since data is fixed?
Properties of MSE, RMSE, MAE, R-squared, adjusted R-squared, and F statistic
Code: Confidence intervals, test statistics, p-value, prediction intervals, confidence and prediction bands, and model fit statistics

### Module 4 (Slides 1-16)

Interpreting multiple slopes
Adding multicollinearity to the list of assumptions

# Week 07 (Oct 6, 8; Feb 18; Jun 22, 24)

Presentations





----- CASE TWO -----

- Student evaluations
- Student panel invitation
- Post-quiz for research project

# Week 08 (Oct 13, 15; Feb 23, 25; Jun 29, Jul 1)

### Module 4 (Slides 17-39, 40-55)

Interpreting the F-test and model vs. coefficient p-values
What is multiplicity or the multiple comparisons problem?
Using partial regression to have multiple regression diagnostics
Underfitting and overfitting
Code: Create a scatterplot matrix, correlation matrix (plus a heat map), fitting multiple linear regression, diagnostics (including partial regression plots)

# Week 09 (Oct 20, 22; Mar 2, 4; Jul 6, 8)

## Regularization

- Penalize the regression to learn "regular" (i.e., generalizable) features
- Shrinkage toward the MLE

## Hyperparameter Tuning

## Module 5 (Slides 1-12, 13-38)

Stepwise regression for model selection when focusing on prediction
Discussed the possibility of p > n
Working from a full model and using backward or forward-selection
No discussion on overfitting needing predictive fit?
Is stepwise regression used in ML practice?
Cross-validation without one static test dataset? What about leakage?
Are variable selection and shrinkage methods primarily used for multicollinearity?
AIC and BIC

# Week 10 (Oct 27, 29; Mar 16, 18; Jul 13, 15)

## Feature Engineering

## Dummy Coding

## Module 5 (Slides 39-59)

Ridge regression, LASSO, elastic net, best subsets, sequential replacement
Bias-variance tradeoff and shrinkage methods
Accuracy and precision is just bias and variance
Code: Clunky, implementing stepwise regression and shrinkage methods with manual hyperparameter tuning

## Module 6 (Slides 1-14)

Dummy variables and the "dummy variable trap"
Interpretation, synonyms, etc.

# Week 11 (Nov 3, 5; Mar 23, 25; Jul 20, 22)

## PCR

## Module 6 (Slides 15-27, 28-58)

Interactions (including higher-order interactions), including interactions
Why do we just interpret interaction effects and not main effects?
Does this all have to be done with the statsmodels API? What about https://www.scikit-yb.org/en/latest/index.html?
What about models built to bring assumption verification, etc. to scikit-learn?
Code: EDA, interactions plots, dummy coding, creating interactions

# Week 12 (Nov 10, 12; Mar 30, Apr 1; Jul 27, 29)

## Multilevel Models

- Motivate with Simpson's paradox (PML p. 80)

## Module 7 (Slides 1-21, 22-35; Aug 3, 5)

Logistic regression basics, including maximum likelihood estimation
Why not introduce training/testing split earlier without cross-validation?
Probability vs. (Log) Odds

# Week 13 (Nov 17, 19; Apr 6, 8)

## GLMs

## Module 7 (Slides 36-54, 55-61)

Logistic regression assumptions and diagnostics
If the assumptions we used previously break down, why use them at all?
Interpreting a confusion matrix
Code: Splitting the data into train and test, maximum likelihood, ROC/AUC

# Week 14 (Nov 24; Apr 13, 15)

## Logistic Regression

## Module 8 (Slides 1-15, 16-36)

Dimension reduction as opposed to variable selection and regularization
Should this all be in a "feature engineering" module?
Shouldn't we have decision trees and random forests as well/instead?
Are there students who *only* take the introduction to ML?
PCA on its own and then PCA as part of PCR
Code: Using PCA and PCR for continuous and discrete outcomes

# Week 15 (Dec 1, 3; Apr 20)

Presentations

