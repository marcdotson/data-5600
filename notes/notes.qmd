---
title: "DATA 5600 Course Notes"
format: gfm
---

Provided more opportunities for student participation beyond live coding by including question slides as prompts for activities, including discussing with a neighbor, raising hands, speed dialogue, and whiteboarding concepts.

- Slow down and illustrate -- especially with case, code, questions, and activities
- Use minimal scales for grades (e.g., "Excellent," "Good," "Needs Improvement")
- Create rubrics as a set of guiding questions rather than a checklist
- Learn names so you can call on students to share
- Create QR code for adding me on LinkedIn
- Produce a meme to summarize a concept
- Speed dating to explain concepts

## Week 01 (Aug 25, 27; Jan 5, 7; May 4, 6)

- Regression and Machine Learning (Aug 25; Jan 5; May 4)
- Modeling Workflow (Aug 27; Jan 7; May 6)

## Week 02 (Sept 3; Jan 12, 14; May 11, 13)

- Decisions and Data (Sept 3; Jan 12; May 11)

## Week 03 (Sept 8, 10; Jan 21; May 18, 20)

- Probability and Statistics (Sept 8; Jan 14; May 13)
- Linear Models (Sept 10; Jan 21; May 18)

- Simulate data to work with (we know the truth, we're satisfying assumptions)
- Model uncertainty
- No free lunch theorem: which model is best depends on the application?

## Lines and Means

- Modeling means
- The normal distribution shows up a lot in nature and, like the linear model, is a good assumption when we don't know a lot about the data generating process. It says the effects of all the variables we don't include in the model add up to something that looks like normal error.
- Reference that mu can also be any real number (infer GLMs)
- This model has two parameters. The modeling components and this language generalize no matter the number of parameters.
- Since we have two parameters, the likelihood is the evaluation of the combination of all possible values of the parameters, thus the posterior will be two-dimensional.

- Why lines? Equation of a line.
- A linear model is simple but a good assumption when we don't know a lot about the data generating process. It says the outcome variable is the result of summing up the effects of the explanatory variable(s).

- Intercept-only model -- no story -- intercept or bias
- Correlation coefficient (interactive coding)

This kind of intercept-only model isn't especially interesting. We want to add covariates/independent variables/predictors -- other data that might help explain the observed heights.

> "The strategy is to make the parameter for the mean of a Gaussian distribution, mu, into a linear function of the predictor variable and other, new parameters that we invent. This strategy is often simply called the linear model."

- The `i` subscript now applies to `mu` as well.
- `mu` is no longer a parameter. `mu_i` is equal to (not distributed) the linear combination of our parameters and predictors.

## Regression

A linear model with normal error and a continuous outcome $y$ is known as a **regression**.
- Regression is also referred to as linear regression or, more generally, a linear model
- Key property is the expected value (mu) of the output is assumed to be a linear function of the inputs (X)

SLR

$$
y = \beta_0 + \beta_1 x + \epsilon
$$

Because $y = \beta_0 + \beta_1 x$ is a linear equation, adding $\epsilon$ makes this a **linear model**.

- $y$ is the **outcome** variable (a.k.a., response or dependent variable).
- $x$ is an **explanatory** variable (a.k.a., a predictor or independent variable, feature, or covariate).
- $\beta_0$ is the **intercept** parameter.
- $\beta_1$ is a **slope** parameter.
- $\epsilon$ is the *error* term.

MLR
- Matrix multiplication -- notation works in both statsmodels and Bambi?

As you start adding many explanatory variables, it may be easier to use matrix multiplication instead of listing each variable and parameter separately in the linear model.

Including more than one explanatory variable is almost always a better approximation of the *data generating process*.

$$
y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + ... + \beta_p x_p + \epsilon, \text{ where } \epsilon \sim Normal(0, 1)
$$

- $y$ is the *outcome* variable.
- $x_1$ is the 1st *explanatory* variable.
- $x_p$ is the $p$th *explanatory* variable.
- $\beta_0$ is the *intercept* parameter.
- $\beta_1$ is the 1st *slope* parameter.
- $\beta_p$ is the $p$th *slope* parameter.
- $\epsilon$ is the *error* term.

- Hypothesis tests meme

### OLS intuition?

Our goal is to use the model to **estimate** the unobserved parameters from the data (i.e., make our best guess).

To revise our original definition of a model, we use an inferential model to extract **parameter estimates** from the data to inform our managerial decision.

Estimating $\beta_0$ and $\beta_1$ (using lines from)

The best line should be the one that makes **the sum** of the vertical bars as **small as possible**.

The vertical bars are called **residuals**, and represent the distance between the data $y$ and a particular line.

Residuals can be positive and negative, so we make the sum of the **squared residuals** as small as possible.

- Using OLS to estimate the parameters in a linear regression is equivalent to using Bayesian inference with a ______ prior. (Show with the two equation forms.)

## Assumptions

Are all these assumptions really required to use regression?

- Validity: Data is relevant to the objective, no missing variables.
- Representativeness: Data is representative of the population or process.
- Additivity: The relationship between the outcome and predictors is additive.
- Linearity: The mean is a linear function of the predictors.
- Independence: Observations are independent of each other.
- Constant Variance: Homoscedasticity or constant variance of errors across all levels of predictors.
- Normality of Errors: Errors are normally distributed?

- Omitted and included variable bias
- Multicollinearity






# Week 04 (Sept 15, 17; Jan 26, 28; May 27) POSIT CONF

- EDA review? Broken down by continuous and discrete variables? By assumptions?
- Assumptions diagnostics and remediation

## Continuous Predictors

- Exploration and preparation
- Feature engineering, transformations
- Training/testing split

- Flexible ML is about automating feature engineering

- Be sure to include +1 for log transforms
- Rescaling/normalizing/standardizing predictors

Why does the Q-Q plot say "standardized residuals"?

## Discrete Predictors

- Exploration and preparation
- Dummy/one-hot and index coding
Dummy variables and the "dummy variable trap"

This prior predictive distribution is the expected distribution of our data, given how we've specified our likelihood and priors. Does this look reasonable? No one has a negative height, for a start. At this point we can iterate on how we've specified our likelihood and priors, produce another prior predictive distribution and evaluate again, etc.

> "Prior predictive simulation is very useful for assigning sensible priors, because it can be quite hard to anticipate how priors influence the observable variables."

Line isn't great, but we can do non-linear transofmrations on the predictors while still keeping a linear model

Linear models are additive, but not necessarily *lines*. Allowing for curvature might be helpful. The form of the model (especially the likelihood) depends on the data. However, as we add complexity to the model, issues begin to emerge with overfitting and interpretation (glimpse of the black-box nature of predictive models).

# Week 05 (Sept 22, 24; Feb 2, 4; Jun 1, 3)

## Fitting and Interpreting Models

- Correlation does not imply causation, and the lack of correlation does not imply lack of causation

- Parameter estimates
- Sampling distributions vs. posterior distributions
- Points, intervals, bootstrap, and distributions (estimate, estimator, estimand meme)
- Sampling distributions vs. posterior distributions, bootstrap as poor man's posterior
- Significance, confidence intervals, and p-values
- Create a chart showing the differences between Bayesian and frequentist statistics?

Probability of observing something as or more extreme, assuming the null is true
Manually computing a confidence interval using the t-distribution and margin of error
Equivalence of p-values and confidence intervals (meme)
Confidence intervals are about uncertainty in parameter estimates (i.e., parameters are fixed)

# Week 06 (Sept 29, Oct 1; Feb 9, 11; Jun 17)

## Model Evaluation and Prediction

Why do we compute a confidence band for the average of y?
How do we get the confidence band out of a fixed confidence interval?
Prediction intervals are about uncertainty in new data (i.e., new data is random)
Why even produce a confidence interval around the mean if it isn't for a confidence band?
What's the extra term in the standard error for the prediction interval? SD used twice.
Are prediction intervals different in Bayesian statistics since data is fixed?

Properties of MSE, RMSE, MAE, R-squared, adjusted R-squared, and F statistic
Code: Confidence intervals, test statistics, p-value, prediction intervals, confidence and prediction bands, and model fit statistics

- Statistical models capture association, not causation
- In-sample vs. out-of-sample vs. decision theoretic evaluation
- Overfitting and underfitting, variance vs. bias tradeoff
- Use theory-model-evidence.png

## Communicating Results

- Demonstrate presenting on a project as part of the communication session

# Week 07 (Oct 6, 8; Feb 18; Jun 22, 24)

Presentations






----- CASE TWO -----

- Update the syllabus to reflect the changed schedule topics
- Student evaluations
- Student panel invitation
- Post-quiz for research project

# Week 08 (Oct 13, 15; Feb 23, 25; Jun 29, Jul 1)

- Week 09 notes from pre-PhD seminar

### Module 4 (Slides 17-39, 40-55)

Interpreting the F-test and model vs. coefficient p-values
What is multiplicity or the multiple comparisons problem?
Using partial regression to have multiple regression diagnostics
Underfitting and overfitting
Code: Create a scatterplot matrix, correlation matrix (plus a heat map), fitting multiple linear regression, diagnostics (including partial regression plots)

# Week 09 (Oct 20, 22; Mar 2, 4; Jul 6, 8)

## Regularization

- Penalize the regression to learn "regular" (i.e., generalizable) features
- Shrinkage toward the MLE

## Hyperparameter Tuning

## Module 5 (Slides 1-12, 13-38)

Stepwise regression for model selection when focusing on prediction
Discussed the possibility of p > n
Working from a full model and using backward or forward-selection
No discussion on overfitting needing predictive fit?
Is stepwise regression used in ML practice?
Cross-validation without one static test dataset? What about leakage?
Cross-validation is needed for hyperparameter tuning, and model selection is a form of "hyperparameter tuning"
Are variable selection and shrinkage methods primarily used for multicollinearity?
AIC and BIC

# Week 10 (Oct 27, 29; Mar 16, 18; Jul 13, 15)

## Feature Engineering

## Dummy Coding

## Module 5 (Slides 39-59)

Ridge regression, LASSO, elastic net, best subsets, sequential replacement
Bias-variance tradeoff and shrinkage methods
Accuracy and precision is just bias and variance
Code: Clunky, implementing stepwise regression and shrinkage methods with manual hyperparameter tuning

# Week 11 (Nov 3, 5; Mar 23, 25; Jul 20, 22)

## PCR

## Module 6 (Slides 15-27, 28-58)

Interactions (including higher-order interactions), including interactions
Why do we just interpret interaction effects and not main effects?
Does this all have to be done with the statsmodels API? What about https://www.scikit-yb.org/en/latest/index.html?
What about models built to bring assumption verification, etc. to scikit-learn?
Code: EDA, interactions plots, dummy coding, creating interactions

# Week 12 (Nov 10, 12; Mar 30, Apr 1; Jul 27, 29)

## Multilevel Models

- Motivate with Simpson's paradox (PML p. 80)

## Module 7 (Slides 1-21, 22-35; Aug 3, 5)

Logistic regression basics, including maximum likelihood estimation
Why not introduce training/testing split earlier without cross-validation?
Probability vs. (Log) Odds

# Week 13 (Nov 17, 19; Apr 6, 8)

## GLMs

## Module 7 (Slides 36-54, 55-61)

Logistic regression assumptions and diagnostics
If the assumptions we used previously break down, why use them at all?
Interpreting a confusion matrix
Code: Splitting the data into train and test, maximum likelihood, ROC/AUC

# Week 14 (Nov 24; Apr 13, 15)

## Logistic Regression

## Module 8 (Slides 1-15, 16-36)

Dimension reduction as opposed to variable selection and regularization
PCA on its own and then PCA as part of PCR
Code: Using PCA and PCR for continuous and discrete outcomes

# Week 15 (Dec 1, 3; Apr 20)

Presentations

